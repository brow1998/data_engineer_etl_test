[2018-11-07 22:00:07,792] {jobs.py:385} INFO - Started process (PID=47607) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:00:12,797] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:00:12,801] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:00:12,802] {logging_mixin.py:95} INFO - [2018-11-07 22:00:12,801] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:00:12,808] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:00:12,841] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:00:12,851] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:12,876] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:12,895] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:12,909] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:12,922] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:12,937] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:12,950] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:12,963] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:12,979] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:12,997] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:13,016] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:13,029] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:13,046] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:13,062] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:13,078] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:13,095] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:00:13,217] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:00:13,224] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:00:13,228] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:00:13,232] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:00:13,236] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:00:13,239] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:00:13,243] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:00:13,246] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:00:13,250] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:00:13,259] {logging_mixin.py:95} INFO - [2018-11-07 22:00:13,258] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:00:13,260] {logging_mixin.py:95} INFO - [2018-11-07 22:00:13,259] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-07 23:55:13.259694+00:00

[2018-11-07 22:00:13,264] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.472 seconds
[2018-11-07 22:01:10,141] {jobs.py:385} INFO - Started process (PID=47651) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:01:15,149] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:01:15,156] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:01:15,158] {logging_mixin.py:95} INFO - [2018-11-07 22:01:15,158] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:01:15,168] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:01:15,194] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:01:15,201] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,216] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,227] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,238] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,252] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,263] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,273] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,283] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,293] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,303] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,313] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,324] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,334] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,345] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,356] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,366] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:01:15,479] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:01:15,487] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:01:15,491] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:01:15,495] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:01:15,498] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:01:15,501] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:01:15,505] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:01:15,509] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:01:15,512] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:01:15,520] {logging_mixin.py:95} INFO - [2018-11-07 22:01:15,520] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:01:15,521] {logging_mixin.py:95} INFO - [2018-11-07 22:01:15,521] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-07 23:56:15.521112+00:00

[2018-11-07 22:01:15,526] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.384 seconds
[2018-11-07 22:02:09,912] {jobs.py:385} INFO - Started process (PID=47691) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:02:14,918] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:02:14,925] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:02:14,927] {logging_mixin.py:95} INFO - [2018-11-07 22:02:14,926] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:02:14,935] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:02:14,966] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:02:14,973] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:14,993] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,005] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,017] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,027] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,038] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,048] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,059] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,069] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,080] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,091] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,102] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,115] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,126] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,135] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,146] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:02:15,257] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:02:15,264] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:02:15,268] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:02:15,273] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:02:15,276] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:02:15,280] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:02:15,283] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:02:15,287] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:02:15,290] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:02:15,299] {logging_mixin.py:95} INFO - [2018-11-07 22:02:15,299] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:02:15,301] {logging_mixin.py:95} INFO - [2018-11-07 22:02:15,300] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-07 23:57:15.300498+00:00

[2018-11-07 22:02:15,305] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.393 seconds
[2018-11-07 22:03:09,246] {jobs.py:385} INFO - Started process (PID=47729) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:03:14,254] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:03:14,257] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:03:14,259] {logging_mixin.py:95} INFO - [2018-11-07 22:03:14,258] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:03:14,271] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:03:14,301] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:03:14,308] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,325] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,336] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,347] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,358] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,369] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,379] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,390] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,401] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,412] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,423] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,434] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,444] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,455] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,466] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,478] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:03:14,598] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:03:14,606] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:03:14,610] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:03:14,613] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:03:14,617] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:03:14,620] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:03:14,623] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:03:14,627] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:03:14,630] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:03:14,639] {logging_mixin.py:95} INFO - [2018-11-07 22:03:14,638] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:03:14,640] {logging_mixin.py:95} INFO - [2018-11-07 22:03:14,639] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-07 23:58:14.639713+00:00

[2018-11-07 22:03:14,645] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.399 seconds
[2018-11-07 22:04:08,799] {jobs.py:385} INFO - Started process (PID=47769) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:04:13,805] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:04:13,812] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:04:13,814] {logging_mixin.py:95} INFO - [2018-11-07 22:04:13,813] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:04:13,826] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:04:13,856] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:04:13,864] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:13,881] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:13,893] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:13,904] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:13,919] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:13,931] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:13,941] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:13,952] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:13,966] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:13,976] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:13,986] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:13,998] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:14,013] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:14,025] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:14,040] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:14,054] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:04:14,169] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:04:14,177] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:04:14,181] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:04:14,184] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:04:14,188] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:04:14,193] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:04:14,196] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:04:14,199] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:04:14,203] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:04:14,215] {logging_mixin.py:95} INFO - [2018-11-07 22:04:14,215] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:04:14,216] {logging_mixin.py:95} INFO - [2018-11-07 22:04:14,215] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-07 23:59:14.215834+00:00

[2018-11-07 22:04:14,221] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.422 seconds
[2018-11-07 22:05:08,669] {jobs.py:385} INFO - Started process (PID=47808) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:05:13,686] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:05:13,688] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:05:13,690] {logging_mixin.py:95} INFO - [2018-11-07 22:05:13,689] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:05:13,698] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:05:13,726] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:05:13,732] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,747] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,759] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,771] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,784] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,797] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,807] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,818] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,828] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,838] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,849] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,860] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,873] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,883] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,894] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:13,906] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:05:14,021] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:05:14,029] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:05:14,033] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:05:14,037] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:05:14,040] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:05:14,044] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:05:14,048] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:05:14,055] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:05:14,065] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:05:14,086] {logging_mixin.py:95} INFO - [2018-11-07 22:05:14,085] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:05:14,088] {logging_mixin.py:95} INFO - [2018-11-07 22:05:14,087] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:00:14.087861+00:00

[2018-11-07 22:05:14,093] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.424 seconds
[2018-11-07 22:06:14,095] {jobs.py:385} INFO - Started process (PID=47853) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:06:19,104] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:06:19,112] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:06:19,113] {logging_mixin.py:95} INFO - [2018-11-07 22:06:19,113] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:06:19,120] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:06:19,148] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:06:19,155] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,170] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,181] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,195] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,211] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,224] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,235] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,245] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,256] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,267] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,278] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,288] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,298] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,308] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,319] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,329] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:06:19,451] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:06:19,458] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:06:19,463] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:06:19,467] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:06:19,470] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:06:19,473] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:06:19,477] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:06:19,480] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:06:19,484] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:06:19,495] {logging_mixin.py:95} INFO - [2018-11-07 22:06:19,494] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:06:19,497] {logging_mixin.py:95} INFO - [2018-11-07 22:06:19,496] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:01:19.495974+00:00

[2018-11-07 22:06:19,504] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.409 seconds
[2018-11-07 22:07:14,773] {jobs.py:385} INFO - Started process (PID=47896) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:07:19,780] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:07:19,787] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:07:19,788] {logging_mixin.py:95} INFO - [2018-11-07 22:07:19,787] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:07:19,793] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:07:19,819] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:07:19,827] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,843] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,854] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,864] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,875] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,886] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,899] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,919] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,932] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,943] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,953] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,964] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,975] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:19,988] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:20,000] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:20,012] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:07:20,128] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:07:20,136] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:07:20,140] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:07:20,143] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:07:20,147] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:07:20,150] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:07:20,153] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:07:20,157] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:07:20,162] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:07:20,171] {logging_mixin.py:95} INFO - [2018-11-07 22:07:20,170] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:07:20,173] {logging_mixin.py:95} INFO - [2018-11-07 22:07:20,172] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:02:20.172116+00:00

[2018-11-07 22:07:20,181] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.408 seconds
[2018-11-07 22:08:15,224] {jobs.py:385} INFO - Started process (PID=47934) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:08:20,231] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:08:20,234] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:08:20,236] {logging_mixin.py:95} INFO - [2018-11-07 22:08:20,235] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:08:20,247] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:08:20,276] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:08:20,283] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,297] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,310] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,324] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,336] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,349] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,365] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,379] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,396] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,410] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,421] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,432] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,443] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,455] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,466] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,478] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:08:20,596] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:08:20,604] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:08:20,607] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:08:20,611] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:08:20,615] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:08:20,618] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:08:20,622] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:08:20,625] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:08:20,629] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:08:20,638] {logging_mixin.py:95} INFO - [2018-11-07 22:08:20,637] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:08:20,639] {logging_mixin.py:95} INFO - [2018-11-07 22:08:20,638] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:03:20.638782+00:00

[2018-11-07 22:08:20,644] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.420 seconds
[2018-11-07 22:09:14,670] {jobs.py:385} INFO - Started process (PID=47973) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:09:19,677] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:09:19,680] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:09:19,682] {logging_mixin.py:95} INFO - [2018-11-07 22:09:19,681] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:09:19,691] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:09:19,719] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:09:19,728] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,744] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,754] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,765] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,776] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,786] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,796] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,807] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,818] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,830] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,840] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,850] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,860] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,872] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,882] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:19,893] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:09:20,005] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:09:20,013] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:09:20,017] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:09:20,021] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:09:20,024] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:09:20,028] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:09:20,031] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:09:20,034] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:09:20,038] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:09:20,046] {logging_mixin.py:95} INFO - [2018-11-07 22:09:20,046] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:09:20,047] {logging_mixin.py:95} INFO - [2018-11-07 22:09:20,046] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:04:20.046813+00:00

[2018-11-07 22:09:20,051] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.381 seconds
[2018-11-07 22:10:14,214] {jobs.py:385} INFO - Started process (PID=48012) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:10:19,220] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:10:19,223] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:10:19,225] {logging_mixin.py:95} INFO - [2018-11-07 22:10:19,225] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:10:19,235] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:10:19,265] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:10:19,273] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,288] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,298] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,309] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,320] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,331] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,344] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,357] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,368] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,379] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,390] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,401] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,410] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,421] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,432] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,442] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:10:19,557] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:10:19,564] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:10:19,569] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:10:19,573] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:10:19,576] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:10:19,579] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:10:19,583] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:10:19,587] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:10:19,591] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:10:19,599] {logging_mixin.py:95} INFO - [2018-11-07 22:10:19,599] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:10:19,600] {logging_mixin.py:95} INFO - [2018-11-07 22:10:19,599] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:05:19.599868+00:00

[2018-11-07 22:10:19,604] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.391 seconds
[2018-11-07 22:11:14,230] {jobs.py:385} INFO - Started process (PID=48051) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:11:19,240] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:11:19,243] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:11:19,245] {logging_mixin.py:95} INFO - [2018-11-07 22:11:19,245] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:11:19,255] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:11:19,281] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:11:19,289] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,302] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,312] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,323] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,334] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,345] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,357] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,367] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,377] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,387] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,397] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,406] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,417] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,428] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,439] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,449] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:11:19,562] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:11:19,570] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:11:19,573] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:11:19,577] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:11:19,580] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:11:19,583] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:11:19,587] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:11:19,590] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:11:19,594] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:11:19,603] {logging_mixin.py:95} INFO - [2018-11-07 22:11:19,603] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:11:19,605] {logging_mixin.py:95} INFO - [2018-11-07 22:11:19,604] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:06:19.604354+00:00

[2018-11-07 22:11:19,609] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.379 seconds
[2018-11-07 22:12:14,069] {jobs.py:385} INFO - Started process (PID=48090) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:12:19,077] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:12:19,081] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:12:19,083] {logging_mixin.py:95} INFO - [2018-11-07 22:12:19,082] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:12:19,091] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:12:19,118] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:12:19,126] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,142] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,153] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,164] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,174] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,186] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,197] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,207] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,219] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,229] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,241] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,253] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,263] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,274] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,287] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,297] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:12:19,409] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:12:19,416] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:12:19,421] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:12:19,424] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:12:19,427] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:12:19,431] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:12:19,434] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:12:19,438] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:12:19,441] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:12:19,450] {logging_mixin.py:95} INFO - [2018-11-07 22:12:19,450] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:12:19,451] {logging_mixin.py:95} INFO - [2018-11-07 22:12:19,450] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:07:19.450906+00:00

[2018-11-07 22:12:19,456] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.386 seconds
[2018-11-07 22:13:03,820] {jobs.py:385} INFO - Started process (PID=48126) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:13:08,829] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:13:08,833] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:13:08,835] {logging_mixin.py:95} INFO - [2018-11-07 22:13:08,834] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:13:08,843] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:13:08,871] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:13:08,879] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:08,893] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:08,903] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:08,916] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:08,926] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:08,936] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:08,946] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:08,956] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:08,967] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:08,976] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:08,986] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:08,997] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:09,007] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:09,017] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:09,027] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:09,038] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:13:09,147] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:13:09,154] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:13:09,158] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:13:09,162] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:13:09,166] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:13:09,169] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:13:09,172] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:13:09,176] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:13:09,180] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:13:09,188] {logging_mixin.py:95} INFO - [2018-11-07 22:13:09,188] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:13:09,190] {logging_mixin.py:95} INFO - [2018-11-07 22:13:09,189] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:08:09.189376+00:00

[2018-11-07 22:13:09,194] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.374 seconds
[2018-11-07 22:14:05,273] {jobs.py:385} INFO - Started process (PID=48164) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:14:10,293] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:14:10,298] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:14:10,299] {logging_mixin.py:95} INFO - [2018-11-07 22:14:10,299] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:14:10,306] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:14:10,340] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:14:10,353] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,373] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,393] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,407] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,419] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,430] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,442] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,453] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,463] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,473] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,486] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,496] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,506] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,517] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,534] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,547] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:14:10,663] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:14:10,671] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:14:10,677] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:14:10,681] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:14:10,684] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:14:10,687] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:14:10,691] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:14:10,695] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:14:10,698] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:14:10,707] {logging_mixin.py:95} INFO - [2018-11-07 22:14:10,707] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:14:10,708] {logging_mixin.py:95} INFO - [2018-11-07 22:14:10,708] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:09:10.708032+00:00

[2018-11-07 22:14:10,713] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.440 seconds
[2018-11-07 22:15:06,472] {jobs.py:385} INFO - Started process (PID=48203) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:15:11,483] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:15:11,491] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:15:11,494] {logging_mixin.py:95} INFO - [2018-11-07 22:15:11,493] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:15:11,502] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:15:11,532] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:15:11,539] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,553] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,565] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,575] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,586] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,597] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,607] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,618] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,628] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,639] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,649] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,659] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,670] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,680] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,691] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,702] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:15:11,811] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:15:11,819] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:15:11,822] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:15:11,826] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:15:11,829] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:15:11,833] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:15:11,836] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:15:11,840] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:15:11,843] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:15:11,851] {logging_mixin.py:95} INFO - [2018-11-07 22:15:11,851] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:15:11,852] {logging_mixin.py:95} INFO - [2018-11-07 22:15:11,852] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:10:11.851989+00:00

[2018-11-07 22:15:11,856] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.384 seconds
[2018-11-07 22:16:07,192] {jobs.py:385} INFO - Started process (PID=48243) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:16:12,208] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:16:12,215] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:16:12,217] {logging_mixin.py:95} INFO - [2018-11-07 22:16:12,216] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:16:12,224] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:16:12,249] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:16:12,257] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,273] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,284] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,295] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,307] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,317] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,327] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,340] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,351] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,361] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,372] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,382] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,393] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,409] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,420] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,436] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:16:12,550] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:16:12,559] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:16:12,563] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:16:12,566] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:16:12,570] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:16:12,574] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:16:12,577] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:16:12,581] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:16:12,584] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:16:12,594] {logging_mixin.py:95} INFO - [2018-11-07 22:16:12,594] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:16:12,595] {logging_mixin.py:95} INFO - [2018-11-07 22:16:12,595] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:11:12.595126+00:00

[2018-11-07 22:16:12,600] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.408 seconds
[2018-11-07 22:17:07,059] {jobs.py:385} INFO - Started process (PID=48282) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:17:12,066] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:17:12,071] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:17:12,073] {logging_mixin.py:95} INFO - [2018-11-07 22:17:12,072] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:17:12,080] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:17:12,111] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:17:12,119] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,133] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,144] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,156] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,168] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,178] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,189] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,200] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,211] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,222] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,232] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,243] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,255] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,265] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,276] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,288] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:17:12,400] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:17:12,407] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:17:12,411] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:17:12,415] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:17:12,419] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:17:12,422] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:17:12,425] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:17:12,429] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:17:12,432] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:17:12,441] {logging_mixin.py:95} INFO - [2018-11-07 22:17:12,440] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:17:12,442] {logging_mixin.py:95} INFO - [2018-11-07 22:17:12,441] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:12:12.441636+00:00

[2018-11-07 22:17:12,446] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.388 seconds
[2018-11-07 22:18:04,103] {jobs.py:385} INFO - Started process (PID=48321) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:18:09,117] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:18:09,120] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:18:09,123] {logging_mixin.py:95} INFO - [2018-11-07 22:18:09,121] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:18:09,133] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:18:09,165] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:18:09,172] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,187] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,197] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,207] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,220] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,230] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,240] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,250] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,260] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,270] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,281] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,291] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,301] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,311] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,321] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,332] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:18:09,440] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:18:09,449] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:18:09,454] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:18:09,458] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:18:09,462] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:18:09,466] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:18:09,470] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:18:09,475] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:18:09,479] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:18:09,487] {logging_mixin.py:95} INFO - [2018-11-07 22:18:09,487] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:18:09,489] {logging_mixin.py:95} INFO - [2018-11-07 22:18:09,488] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:13:09.488356+00:00

[2018-11-07 22:18:09,494] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.391 seconds
[2018-11-07 22:19:03,577] {jobs.py:385} INFO - Started process (PID=48356) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:19:08,586] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:19:08,593] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:19:08,595] {logging_mixin.py:95} INFO - [2018-11-07 22:19:08,594] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:19:08,601] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:19:08,632] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:19:08,640] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,660] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,673] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,685] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,696] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,712] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,725] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,738] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,751] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,764] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,776] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,788] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,808] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,826] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,842] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,860] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:19:08,990] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:19:08,998] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:19:09,002] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:19:09,009] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:19:09,013] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:19:09,017] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:19:09,021] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:19:09,025] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:19:09,030] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:19:09,039] {logging_mixin.py:95} INFO - [2018-11-07 22:19:09,038] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:19:09,040] {logging_mixin.py:95} INFO - [2018-11-07 22:19:09,039] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:14:09.039498+00:00

[2018-11-07 22:19:09,045] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.468 seconds
[2018-11-07 22:20:08,206] {jobs.py:385} INFO - Started process (PID=48400) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:20:13,212] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:20:13,216] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:20:13,219] {logging_mixin.py:95} INFO - [2018-11-07 22:20:13,218] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:20:13,228] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:20:13,256] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:20:13,263] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,281] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,294] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,305] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,320] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,337] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,349] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,360] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,371] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,382] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,394] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,404] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,416] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,427] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,438] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,449] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:20:13,566] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:20:13,574] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:20:13,578] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:20:13,581] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:20:13,585] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:20:13,590] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:20:13,593] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:20:13,597] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:20:13,600] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:20:13,609] {logging_mixin.py:95} INFO - [2018-11-07 22:20:13,609] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:20:13,611] {logging_mixin.py:95} INFO - [2018-11-07 22:20:13,610] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:15:13.610546+00:00

[2018-11-07 22:20:13,617] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.411 seconds
[2018-11-07 22:21:08,638] {jobs.py:385} INFO - Started process (PID=48439) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:21:13,648] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:21:13,656] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:21:13,658] {logging_mixin.py:95} INFO - [2018-11-07 22:21:13,657] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:21:13,667] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:21:13,720] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:21:13,735] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:13,769] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:13,801] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:13,825] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:13,857] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:13,889] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:13,916] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:13,938] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:13,961] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:13,989] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:14,044] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:14,087] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:14,117] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:14,145] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:14,172] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:14,197] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:21:14,529] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:21:14,538] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:21:14,546] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:21:14,551] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:21:14,555] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:21:14,563] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:21:14,570] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:21:14,579] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:21:14,585] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:21:14,598] {logging_mixin.py:95} INFO - [2018-11-07 22:21:14,598] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:21:14,602] {logging_mixin.py:95} INFO - [2018-11-07 22:21:14,600] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:16:14.600939+00:00

[2018-11-07 22:21:14,613] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.975 seconds
[2018-11-07 22:22:04,877] {jobs.py:385} INFO - Started process (PID=48479) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:22:09,892] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:22:09,893] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:22:09,894] {logging_mixin.py:95} INFO - [2018-11-07 22:22:09,894] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:22:09,901] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:22:09,929] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:22:09,937] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:09,964] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:09,979] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:09,993] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,010] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,022] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,035] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,047] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,060] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,075] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,087] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,100] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,112] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,126] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,139] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,151] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:22:10,283] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:22:10,291] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:22:10,297] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:22:10,303] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:22:10,307] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:22:10,310] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:22:10,314] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:22:10,318] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:22:10,322] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:22:10,331] {logging_mixin.py:95} INFO - [2018-11-07 22:22:10,331] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:22:10,334] {logging_mixin.py:95} INFO - [2018-11-07 22:22:10,333] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:17:10.333409+00:00

[2018-11-07 22:22:10,340] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.463 seconds
[2018-11-07 22:23:06,109] {jobs.py:385} INFO - Started process (PID=48518) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:11,128] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:23:11,135] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:23:11,137] {logging_mixin.py:95} INFO - [2018-11-07 22:23:11,136] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:23:11,143] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:11,172] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:23:11,179] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,194] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,204] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,215] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,227] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,241] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,250] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,260] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,270] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,282] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,293] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,303] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,313] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,324] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,334] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,343] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:11,451] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:23:11,458] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:23:11,462] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:23:11,465] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:23:11,468] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:23:11,472] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:23:11,475] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:23:11,479] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:23:11,483] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:23:11,491] {logging_mixin.py:95} INFO - [2018-11-07 22:23:11,491] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:23:11,492] {logging_mixin.py:95} INFO - [2018-11-07 22:23:11,491] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:18:11.491888+00:00

[2018-11-07 22:23:11,496] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.388 seconds
[2018-11-07 22:23:27,844] {jobs.py:385} INFO - Started process (PID=48549) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:27,847] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:23:27,851] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:23:27,852] {logging_mixin.py:95} INFO - [2018-11-07 22:23:27,852] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:23:27,858] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:27,888] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:23:27,895] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:27,912] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:27,926] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:27,938] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:27,951] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:27,962] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:27,977] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:27,990] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:28,001] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:28,014] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:28,027] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:28,038] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:28,050] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:28,064] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:28,077] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:28,089] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:28,215] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:23:28,223] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:23:28,231] {logging_mixin.py:95} INFO - [2018-11-07 22:23:28,231] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:23:28,232] {logging_mixin.py:95} INFO - [2018-11-07 22:23:28,231] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:18:28.231899+00:00

[2018-11-07 22:23:28,239] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 0.395 seconds
[2018-11-07 22:23:30,873] {jobs.py:385} INFO - Started process (PID=48553) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:30,877] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:23:30,878] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:23:30,879] {logging_mixin.py:95} INFO - [2018-11-07 22:23:30,879] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:23:30,887] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:30,912] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:23:30,920] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:30,935] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:30,946] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:30,956] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:30,967] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:30,977] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:30,987] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:30,997] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:31,009] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:31,020] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:31,033] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:31,043] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:31,053] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:31,063] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:31,073] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:31,083] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:31,192] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:23:31,199] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:23:31,207] {logging_mixin.py:95} INFO - [2018-11-07 22:23:31,207] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:23:31,208] {logging_mixin.py:95} INFO - [2018-11-07 22:23:31,207] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:18:31.207761+00:00

[2018-11-07 22:23:31,214] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 0.340 seconds
[2018-11-07 22:23:33,952] {jobs.py:385} INFO - Started process (PID=48557) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:33,955] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:23:33,957] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:23:33,958] {logging_mixin.py:95} INFO - [2018-11-07 22:23:33,957] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:23:33,966] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:33,995] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:23:34,003] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,018] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,029] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,040] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,051] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,063] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,074] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,086] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,096] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,106] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,116] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,130] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,145] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,157] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,169] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,181] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:34,291] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:23:34,297] {logging_mixin.py:95} INFO - [2018-11-07 22:23:34,297] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:23:34,298] {logging_mixin.py:95} INFO - [2018-11-07 22:23:34,297] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:18:34.297920+00:00

[2018-11-07 22:23:34,303] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 0.352 seconds
[2018-11-07 22:23:35,407] {jobs.py:385} INFO - Started process (PID=48558) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:35,411] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:23:35,413] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:23:35,414] {logging_mixin.py:95} INFO - [2018-11-07 22:23:35,414] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:23:35,421] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:35,448] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:23:35,457] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,478] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,492] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,505] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,522] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,533] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,546] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,556] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,567] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,577] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,589] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,599] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,609] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,620] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,632] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,642] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:35,753] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:23:35,760] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 22:23:35,768] {logging_mixin.py:95} INFO - [2018-11-07 22:23:35,767] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:23:35,769] {logging_mixin.py:95} INFO - [2018-11-07 22:23:35,768] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:18:35.768553+00:00

[2018-11-07 22:23:35,773] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 0.367 seconds
[2018-11-07 22:23:38,405] {jobs.py:385} INFO - Started process (PID=48563) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:38,409] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 22:23:38,413] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 22:23:38,414] {logging_mixin.py:95} INFO - [2018-11-07 22:23:38,414] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 22:23:38,419] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 22:23:38,446] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 22:23:38,454] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,469] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,479] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,492] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,503] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,514] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,525] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,535] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,545] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,556] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,567] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,577] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,588] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,597] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,608] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,619] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 22:23:38,728] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 22:23:38,734] {logging_mixin.py:95} INFO - [2018-11-07 22:23:38,734] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 22:23:38,735] {logging_mixin.py:95} INFO - [2018-11-07 22:23:38,735] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 00:18:38.735028+00:00

[2018-11-07 22:23:38,740] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 0.335 seconds
[2018-11-07 23:23:39,062] {jobs.py:385} INFO - Started process (PID=48564) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 23:23:39,184] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-07 23:23:39,186] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-07 23:23:39,188] {logging_mixin.py:95} INFO - [2018-11-07 23:23:39,187] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-07 23:23:39,207] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-07 23:23:39,539] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-07 23:23:39,554] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:39,626] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:39,674] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:39,749] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:39,831] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:39,914] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:39,956] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:40,019] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:40,100] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:40,390] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:40,511] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:40,656] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:40,854] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:40,952] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:41,053] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:41,075] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-07 23:23:41,640] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-07 23:23:41,676] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,687] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,694] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,732] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,764] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,769] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,790] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,808] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,815] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,838] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,855] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,893] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,899] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,912] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,918] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,932] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-07 23:23:41,955] {logging_mixin.py:95} INFO - [2018-11-07 23:23:41,954] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-07 23:23:41,957] {logging_mixin.py:95} INFO - [2018-11-07 23:23:41,956] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 01:18:41.955977+00:00

[2018-11-07 23:23:41,989] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 2.928 seconds
[2018-11-08 09:20:21,274] {jobs.py:385} INFO - Started process (PID=48795) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:20:26,283] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:20:26,286] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:20:26,288] {logging_mixin.py:95} INFO - [2018-11-08 09:20:26,287] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:20:26,296] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:20:26,327] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:20:26,344] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,375] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,386] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,398] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,410] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,421] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,432] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,442] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,453] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,464] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,474] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,485] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,496] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,506] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,517] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,527] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:20:26,649] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:20:26,669] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:20:26,674] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:20:26,678] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:20:26,682] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:20:26,685] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:20:26,688] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:20:26,697] {logging_mixin.py:95} INFO - [2018-11-08 09:20:26,697] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:20:26,699] {logging_mixin.py:95} INFO - [2018-11-08 09:20:26,698] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:15:26.698472+00:00

[2018-11-08 09:20:26,709] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.435 seconds
[2018-11-08 09:21:09,561] {jobs.py:385} INFO - Started process (PID=48830) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:21:14,565] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:21:14,568] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:21:14,570] {logging_mixin.py:95} INFO - [2018-11-08 09:21:14,569] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:21:14,579] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:21:14,608] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:21:14,616] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,630] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,640] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,651] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,662] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,673] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,683] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,696] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,708] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,719] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,729] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,740] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,750] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,760] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,770] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,781] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:21:14,890] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:21:14,897] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:21:14,901] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:21:14,905] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:21:14,909] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:21:14,913] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:21:14,916] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:21:14,920] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:21:14,924] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:21:14,927] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:21:14,936] {logging_mixin.py:95} INFO - [2018-11-08 09:21:14,935] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:21:14,937] {logging_mixin.py:95} INFO - [2018-11-08 09:21:14,936] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:16:14.936560+00:00

[2018-11-08 09:21:14,941] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.381 seconds
[2018-11-08 09:22:16,186] {jobs.py:385} INFO - Started process (PID=48873) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:22:21,193] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:22:21,195] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:22:21,196] {logging_mixin.py:95} INFO - [2018-11-08 09:22:21,196] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:22:21,203] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:22:21,236] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:22:21,243] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,258] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,269] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,279] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,291] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,307] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,319] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,332] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,343] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,354] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,366] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,377] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,389] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,400] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,411] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,422] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:22:21,545] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:22:21,553] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:22:21,557] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:22:21,561] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:22:21,565] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:22:21,569] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:22:21,573] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:22:21,576] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:22:21,587] {logging_mixin.py:95} INFO - [2018-11-08 09:22:21,587] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:22:21,588] {logging_mixin.py:95} INFO - [2018-11-08 09:22:21,588] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:17:21.587988+00:00

[2018-11-08 09:22:21,592] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.406 seconds
[2018-11-08 09:23:13,609] {jobs.py:385} INFO - Started process (PID=48908) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:23:18,617] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:23:18,620] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:23:18,621] {logging_mixin.py:95} INFO - [2018-11-08 09:23:18,621] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:23:18,628] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:23:18,659] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:23:18,668] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,685] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,697] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,710] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,724] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,737] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,751] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,765] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,780] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,793] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,806] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,817] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,829] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,840] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,851] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,863] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:23:18,983] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:23:18,990] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:23:18,995] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:23:18,999] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:23:19,004] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:23:19,007] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:23:19,011] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:23:19,015] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:23:19,021] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:23:19,025] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:23:19,034] {logging_mixin.py:95} INFO - [2018-11-08 09:23:19,034] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:23:19,035] {logging_mixin.py:95} INFO - [2018-11-08 09:23:19,034] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:18:19.034893+00:00

[2018-11-08 09:23:19,039] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.430 seconds
[2018-11-08 09:24:22,572] {jobs.py:385} INFO - Started process (PID=48952) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:24:27,579] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:24:27,581] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:24:27,583] {logging_mixin.py:95} INFO - [2018-11-08 09:24:27,582] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:24:27,589] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:24:27,619] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:24:27,628] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,643] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,655] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,669] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,681] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,693] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,703] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,714] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,725] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,738] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,753] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,765] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,777] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,788] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,801] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,812] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:24:27,937] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:24:27,944] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:24:27,952] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:24:27,956] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:24:27,959] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:24:27,963] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:24:27,966] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:24:27,970] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:24:27,983] {logging_mixin.py:95} INFO - [2018-11-08 09:24:27,982] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:24:27,984] {logging_mixin.py:95} INFO - [2018-11-08 09:24:27,983] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:19:27.983649+00:00

[2018-11-08 09:24:27,988] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.417 seconds
[2018-11-08 09:25:19,955] {jobs.py:385} INFO - Started process (PID=48987) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:25:24,961] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:25:24,964] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:25:24,966] {logging_mixin.py:95} INFO - [2018-11-08 09:25:24,966] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:25:24,978] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:25:25,009] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:25:25,017] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,039] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,051] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,063] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,077] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,087] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,099] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,109] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,121] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,132] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,143] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,154] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,166] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,177] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,187] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,199] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:25:25,314] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:25:25,321] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:25:25,325] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:25:25,329] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:25:25,333] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:25:25,336] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:25:25,340] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:25:25,344] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:25:25,347] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:25:25,351] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:25:25,360] {logging_mixin.py:95} INFO - [2018-11-08 09:25:25,360] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:25:25,362] {logging_mixin.py:95} INFO - [2018-11-08 09:25:25,361] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:20:25.361377+00:00

[2018-11-08 09:25:25,366] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.411 seconds
[2018-11-08 09:26:27,890] {jobs.py:385} INFO - Started process (PID=49030) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:26:32,896] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:26:32,899] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:26:32,901] {logging_mixin.py:95} INFO - [2018-11-08 09:26:32,900] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:26:32,910] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:26:32,939] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:26:32,947] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:32,962] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:32,973] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:32,984] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:32,994] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,005] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,016] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,026] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,038] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,050] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,061] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,079] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,092] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,106] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,120] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,131] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:26:33,277] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:26:33,288] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:26:33,295] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:26:33,299] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:26:33,304] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:26:33,309] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:26:33,314] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:26:33,318] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:26:33,329] {logging_mixin.py:95} INFO - [2018-11-08 09:26:33,328] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:26:33,330] {logging_mixin.py:95} INFO - [2018-11-08 09:26:33,329] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:21:33.329663+00:00

[2018-11-08 09:26:33,335] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.445 seconds
[2018-11-08 09:27:22,111] {jobs.py:385} INFO - Started process (PID=49065) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:27:27,116] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:27:27,121] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:27:27,122] {logging_mixin.py:95} INFO - [2018-11-08 09:27:27,122] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:27:27,130] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:27:27,178] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:27:27,187] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,208] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,226] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,239] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,253] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,268] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,280] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,291] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,302] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,314] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,326] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,339] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,352] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,370] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,389] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,403] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:27:27,527] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:27:27,534] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:27:27,538] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:27:27,543] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:27:27,546] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:27:27,550] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:27:27,554] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:27:27,557] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:27:27,562] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:27:27,569] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:27:27,579] {logging_mixin.py:95} INFO - [2018-11-08 09:27:27,578] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:27:27,580] {logging_mixin.py:95} INFO - [2018-11-08 09:27:27,579] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:22:27.579486+00:00

[2018-11-08 09:27:27,585] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.474 seconds
[2018-11-08 09:28:32,712] {jobs.py:385} INFO - Started process (PID=49108) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:28:37,722] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:28:37,726] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:28:37,727] {logging_mixin.py:95} INFO - [2018-11-08 09:28:37,727] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:28:37,738] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:28:37,797] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:28:37,813] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:37,840] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:37,863] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:37,881] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:37,901] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:37,919] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:37,946] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:37,970] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:38,004] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:38,024] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:38,049] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:38,071] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:38,090] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:38,111] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:38,130] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:38,172] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:28:38,401] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:28:38,417] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:28:38,421] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:28:38,428] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:28:38,433] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:28:38,439] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:28:38,446] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:28:38,452] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:28:38,486] {logging_mixin.py:95} INFO - [2018-11-08 09:28:38,474] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:28:38,488] {logging_mixin.py:95} INFO - [2018-11-08 09:28:38,487] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:23:38.487651+00:00

[2018-11-08 09:28:38,502] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.789 seconds
[2018-11-08 09:29:36,320] {jobs.py:385} INFO - Started process (PID=49158) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:29:41,337] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:29:41,340] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:29:41,342] {logging_mixin.py:95} INFO - [2018-11-08 09:29:41,341] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:29:41,353] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:29:41,398] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:29:41,413] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,437] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,455] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,476] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,504] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,528] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,554] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,579] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,605] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,632] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,661] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,692] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,716] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,742] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,769] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:41,789] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:29:42,052] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:29:42,063] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:29:42,074] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:29:42,079] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:29:42,085] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:29:42,090] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:29:42,098] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:29:42,103] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:29:42,110] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:29:42,117] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:29:42,128] {logging_mixin.py:95} INFO - [2018-11-08 09:29:42,128] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:29:42,130] {logging_mixin.py:95} INFO - [2018-11-08 09:29:42,129] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:24:42.129378+00:00

[2018-11-08 09:29:42,138] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.818 seconds
[2018-11-08 09:31:00,058] {jobs.py:385} INFO - Started process (PID=49224) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:31:05,066] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:31:05,069] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:31:05,071] {logging_mixin.py:95} INFO - [2018-11-08 09:31:05,070] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:31:05,079] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:31:05,127] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:31:05,140] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,172] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,191] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,208] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,222] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,237] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,249] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,264] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,281] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,300] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,317] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,332] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,353] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,373] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,395] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,416] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:31:05,634] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:31:05,649] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:31:05,655] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:31:05,660] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:31:05,665] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:31:05,671] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:31:05,676] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:31:05,683] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:31:05,693] {logging_mixin.py:95} INFO - [2018-11-08 09:31:05,693] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:31:05,696] {logging_mixin.py:95} INFO - [2018-11-08 09:31:05,695] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:26:05.695166+00:00

[2018-11-08 09:31:05,705] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.647 seconds
[2018-11-08 09:32:08,594] {jobs.py:385} INFO - Started process (PID=49299) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:32:13,605] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:32:13,612] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:32:13,614] {logging_mixin.py:95} INFO - [2018-11-08 09:32:13,613] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:32:13,625] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:32:13,664] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:32:13,679] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,706] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,728] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,745] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,764] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,783] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,802] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,824] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,865] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,881] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,896] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,909] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,921] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,933] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,949] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:13,962] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:32:14,106] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:32:14,117] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:32:14,123] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:32:14,127] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:32:14,130] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:32:14,134] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:32:14,140] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:32:14,148] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:32:14,158] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:32:14,161] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:32:14,170] {logging_mixin.py:95} INFO - [2018-11-08 09:32:14,169] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:32:14,171] {logging_mixin.py:95} INFO - [2018-11-08 09:32:14,170] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:27:14.170703+00:00

[2018-11-08 09:32:14,180] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.586 seconds
[2018-11-08 09:33:24,053] {jobs.py:385} INFO - Started process (PID=49380) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:33:29,060] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:33:29,063] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:33:29,064] {logging_mixin.py:95} INFO - [2018-11-08 09:33:29,064] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:33:29,072] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:33:29,108] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:33:29,117] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,140] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,151] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,163] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,176] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,190] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,206] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,222] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,235] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,248] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,263] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,275] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,289] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,306] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,325] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,339] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:33:29,492] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:33:29,501] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:33:29,507] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:33:29,514] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:33:29,521] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:33:29,527] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:33:29,533] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:33:29,536] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:33:29,547] {logging_mixin.py:95} INFO - [2018-11-08 09:33:29,547] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:33:29,548] {logging_mixin.py:95} INFO - [2018-11-08 09:33:29,548] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:28:29.548093+00:00

[2018-11-08 09:33:29,555] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.502 seconds
[2018-11-08 09:34:20,766] {jobs.py:385} INFO - Started process (PID=49452) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:34:25,773] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:34:25,783] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:34:25,785] {logging_mixin.py:95} INFO - [2018-11-08 09:34:25,784] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:34:25,791] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:34:25,819] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:34:25,826] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,843] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,854] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,865] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,877] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,887] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,898] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,911] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,925] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,939] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,952] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,962] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,974] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,984] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:25,996] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:26,010] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:34:26,145] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:34:26,152] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:34:26,159] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:34:26,163] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:34:26,166] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:34:26,170] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:34:26,173] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:34:26,179] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:34:26,182] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:34:26,186] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:34:26,196] {logging_mixin.py:95} INFO - [2018-11-08 09:34:26,194] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:34:26,198] {logging_mixin.py:95} INFO - [2018-11-08 09:34:26,197] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:29:26.197025+00:00

[2018-11-08 09:34:26,203] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.438 seconds
[2018-11-08 09:35:30,420] {jobs.py:385} INFO - Started process (PID=49501) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:35:35,427] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:35:35,429] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:35:35,431] {logging_mixin.py:95} INFO - [2018-11-08 09:35:35,430] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:35:35,442] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:35:35,512] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:35:35,539] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,569] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,586] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,603] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,620] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,638] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,659] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,677] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,693] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,713] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,730] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,746] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,762] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,780] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,797] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,814] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:35:35,986] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:35:35,998] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:35:36,009] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:35:36,013] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:35:36,018] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:35:36,023] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:35:36,031] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:35:36,035] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:35:36,051] {logging_mixin.py:95} INFO - [2018-11-08 09:35:36,050] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:35:36,054] {logging_mixin.py:95} INFO - [2018-11-08 09:35:36,052] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:30:36.052537+00:00

[2018-11-08 09:35:36,066] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.646 seconds
[2018-11-08 09:36:29,923] {jobs.py:385} INFO - Started process (PID=49538) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:36:34,931] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:36:34,933] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:36:34,935] {logging_mixin.py:95} INFO - [2018-11-08 09:36:34,934] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:36:34,949] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:36:34,993] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:36:35,008] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,036] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,056] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,073] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,090] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,108] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,124] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,154] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,170] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,189] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,216] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,244] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,271] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,349] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,391] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,421] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:36:35,698] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:36:35,718] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:36:35,730] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:36:35,739] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:36:35,750] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:36:35,763] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:36:35,775] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:36:35,786] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:36:35,794] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:36:35,806] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:36:35,834] {logging_mixin.py:95} INFO - [2018-11-08 09:36:35,834] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:36:35,836] {logging_mixin.py:95} INFO - [2018-11-08 09:36:35,835] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:31:35.835408+00:00

[2018-11-08 09:36:35,843] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.920 seconds
[2018-11-08 09:37:42,554] {jobs.py:385} INFO - Started process (PID=49589) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:37:47,563] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:37:47,566] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:37:47,568] {logging_mixin.py:95} INFO - [2018-11-08 09:37:47,567] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:37:47,577] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:37:47,615] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:37:47,625] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,649] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,661] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,674] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,685] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,699] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,712] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,724] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,734] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,744] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,759] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,769] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,780] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,794] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,805] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,816] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:37:47,961] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:37:47,970] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:37:47,975] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:37:47,983] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:37:47,987] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:37:47,990] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:37:47,993] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:37:48,005] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:37:48,021] {logging_mixin.py:95} INFO - [2018-11-08 09:37:48,021] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:37:48,022] {logging_mixin.py:95} INFO - [2018-11-08 09:37:48,022] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:32:48.022109+00:00

[2018-11-08 09:37:48,027] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.473 seconds
[2018-11-08 09:38:37,147] {jobs.py:385} INFO - Started process (PID=49643) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:38:42,161] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 09:38:42,164] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 09:38:42,166] {logging_mixin.py:95} INFO - [2018-11-08 09:38:42,165] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 09:38:42,174] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 09:38:42,201] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 09:38:42,210] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,229] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,244] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,258] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,268] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,278] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,288] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,299] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,310] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,320] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,330] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,344] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,355] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,365] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,375] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,386] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 09:38:42,499] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 09:38:42,506] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:38:42,510] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:38:42,515] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:38:42,520] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:38:42,524] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:38:42,531] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:38:42,535] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:38:42,539] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:38:42,542] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 09:38:42,554] {logging_mixin.py:95} INFO - [2018-11-08 09:38:42,553] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 09:38:42,556] {logging_mixin.py:95} INFO - [2018-11-08 09:38:42,555] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 11:33:42.555217+00:00

[2018-11-08 09:38:42,563] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.416 seconds
[2018-11-08 10:11:35,587] {jobs.py:385} INFO - Started process (PID=49734) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:11:40,597] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:11:40,600] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:11:40,601] {logging_mixin.py:95} INFO - [2018-11-08 10:11:40,601] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:11:40,618] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:11:40,697] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:11:40,718] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:40,769] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:40,799] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:40,820] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:40,840] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:40,861] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:40,880] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:40,907] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:40,945] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:40,974] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:41,018] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:41,064] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:41,083] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:41,102] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:41,122] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:41,144] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:11:41,430] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:11:41,444] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:11:41,452] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:11:41,458] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:11:41,465] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:11:41,473] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:11:41,479] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:11:41,487] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:11:41,502] {logging_mixin.py:95} INFO - [2018-11-08 10:11:41,502] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:11:41,505] {logging_mixin.py:95} INFO - [2018-11-08 10:11:41,503] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:06:41.503554+00:00

[2018-11-08 10:11:41,513] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.926 seconds
[2018-11-08 10:12:50,804] {jobs.py:385} INFO - Started process (PID=49815) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:12:55,814] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:12:55,821] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:12:55,822] {logging_mixin.py:95} INFO - [2018-11-08 10:12:55,822] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:12:55,830] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:12:55,903] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:12:55,926] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:55,964] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:55,986] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,014] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,043] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,065] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,096] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,122] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,172] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,219] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,276] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,323] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,351] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,384] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,423] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,453] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:12:56,642] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:12:56,665] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:12:56,672] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:12:56,681] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:12:56,687] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:12:56,696] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:12:56,708] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:12:56,715] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:12:56,721] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:12:56,727] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:12:56,744] {logging_mixin.py:95} INFO - [2018-11-08 10:12:56,744] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:12:56,746] {logging_mixin.py:95} INFO - [2018-11-08 10:12:56,745] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:07:56.745213+00:00

[2018-11-08 10:12:56,754] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.950 seconds
[2018-11-08 10:14:15,137] {jobs.py:385} INFO - Started process (PID=49873) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:14:20,146] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:14:20,149] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:14:20,150] {logging_mixin.py:95} INFO - [2018-11-08 10:14:20,149] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:14:20,161] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:14:20,220] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:14:20,242] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,277] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,297] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,326] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,347] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,373] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,393] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,408] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,426] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,443] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,460] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,477] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,491] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,505] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,519] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,536] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:14:20,707] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:14:20,720] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:14:20,725] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:14:20,732] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:14:20,737] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:14:20,742] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:14:20,747] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:14:20,752] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:14:20,765] {logging_mixin.py:95} INFO - [2018-11-08 10:14:20,764] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:14:20,767] {logging_mixin.py:95} INFO - [2018-11-08 10:14:20,766] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:09:20.766163+00:00

[2018-11-08 10:14:20,772] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.635 seconds
[2018-11-08 10:15:19,269] {jobs.py:385} INFO - Started process (PID=49945) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:15:24,281] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:15:24,288] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:15:24,291] {logging_mixin.py:95} INFO - [2018-11-08 10:15:24,290] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:15:24,300] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:15:24,347] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:15:24,369] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,425] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,458] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,496] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,541] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,600] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,626] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,650] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,671] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,700] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,731] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,752] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,775] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,791] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,807] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,821] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:15:24,964] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:15:24,979] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:15:24,987] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:15:24,993] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:15:24,996] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:15:24,999] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:15:25,005] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:15:25,014] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:15:25,018] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:15:25,022] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:15:25,034] {logging_mixin.py:95} INFO - [2018-11-08 10:15:25,033] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:15:25,035] {logging_mixin.py:95} INFO - [2018-11-08 10:15:25,034] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:10:25.034523+00:00

[2018-11-08 10:15:25,045] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.776 seconds
[2018-11-08 10:16:54,518] {jobs.py:385} INFO - Started process (PID=50032) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:16:59,526] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:16:59,528] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:16:59,531] {logging_mixin.py:95} INFO - [2018-11-08 10:16:59,530] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:16:59,541] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:16:59,595] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:16:59,613] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,693] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,717] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,743] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,767] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,800] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,822] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,843] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,866] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,893] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,936] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,975] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:16:59,993] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:00,015] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:00,035] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:00,071] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:00,249] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:17:00,258] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:00,263] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:00,267] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:00,271] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:00,275] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:00,280] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:00,285] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:00,297] {logging_mixin.py:95} INFO - [2018-11-08 10:17:00,297] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:17:00,300] {logging_mixin.py:95} INFO - [2018-11-08 10:17:00,298] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:12:00.298702+00:00

[2018-11-08 10:17:00,310] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.791 seconds
[2018-11-08 10:17:53,441] {jobs.py:385} INFO - Started process (PID=50081) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:17:58,453] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:17:58,461] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:17:58,462] {logging_mixin.py:95} INFO - [2018-11-08 10:17:58,462] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:17:58,469] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:17:58,502] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:17:58,515] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,538] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,560] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,577] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,593] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,610] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,626] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,640] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,655] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,669] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,683] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,698] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,714] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,734] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,764] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:58,800] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:17:59,248] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:17:59,266] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:59,280] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:59,289] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:59,295] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:59,304] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:59,314] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:59,327] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:59,332] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:59,342] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:17:59,355] {logging_mixin.py:95} INFO - [2018-11-08 10:17:59,354] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:17:59,357] {logging_mixin.py:95} INFO - [2018-11-08 10:17:59,355] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:12:59.355706+00:00

[2018-11-08 10:17:59,367] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.926 seconds
[2018-11-08 10:19:06,527] {jobs.py:385} INFO - Started process (PID=50126) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:19:11,538] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:19:11,540] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:19:11,542] {logging_mixin.py:95} INFO - [2018-11-08 10:19:11,541] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:19:11,553] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:19:11,599] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:19:11,622] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,644] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,656] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,673] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,690] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,713] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,732] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,758] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,790] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,811] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,843] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,862] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,892] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,918] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,947] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:11,966] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:19:12,179] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:19:12,191] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:19:12,197] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:19:12,206] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:19:12,211] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:19:12,214] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:19:12,220] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:19:12,224] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:19:12,236] {logging_mixin.py:95} INFO - [2018-11-08 10:19:12,236] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:19:12,238] {logging_mixin.py:95} INFO - [2018-11-08 10:19:12,237] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:14:12.237145+00:00

[2018-11-08 10:19:12,243] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.716 seconds
[2018-11-08 10:20:07,118] {jobs.py:385} INFO - Started process (PID=50165) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:20:12,144] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:20:12,153] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:20:12,155] {logging_mixin.py:95} INFO - [2018-11-08 10:20:12,154] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:20:12,192] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:20:12,289] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:20:12,312] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:12,504] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:12,552] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:12,583] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:12,627] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:12,663] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:12,700] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:12,755] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:12,858] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:12,923] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:12,969] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:13,012] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:13,058] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:13,246] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:13,278] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:13,323] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:20:13,832] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:20:13,872] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:20:13,880] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:20:13,895] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:20:13,905] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:20:13,912] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:20:13,922] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:20:13,929] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:20:13,943] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:20:13,951] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:20:13,994] {logging_mixin.py:95} INFO - [2018-11-08 10:20:13,994] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:20:14,009] {logging_mixin.py:95} INFO - [2018-11-08 10:20:13,995] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:15:13.995702+00:00

[2018-11-08 10:20:14,036] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.917 seconds
[2018-11-08 10:22:33,334] {jobs.py:385} INFO - Started process (PID=50235) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:22:38,347] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:22:38,352] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:22:38,353] {logging_mixin.py:95} INFO - [2018-11-08 10:22:38,353] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:22:38,359] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:22:38,401] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:22:38,417] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,456] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,469] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,483] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,502] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,514] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,532] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,547] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,562] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,573] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,586] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,600] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,617] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,630] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,641] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,656] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:22:38,802] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:22:38,817] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:22:38,829] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:22:38,837] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:22:38,846] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:22:38,855] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:22:38,861] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:22:38,871] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:22:38,892] {logging_mixin.py:95} INFO - [2018-11-08 10:22:38,891] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:22:38,894] {logging_mixin.py:95} INFO - [2018-11-08 10:22:38,893] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:17:38.893273+00:00

[2018-11-08 10:22:38,903] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.569 seconds
[2018-11-08 10:23:32,109] {jobs.py:385} INFO - Started process (PID=50271) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:23:37,122] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:23:37,127] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:23:37,129] {logging_mixin.py:95} INFO - [2018-11-08 10:23:37,129] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:23:37,142] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:23:37,179] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:23:37,193] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,223] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,237] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,252] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,266] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,285] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,300] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,313] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,327] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,345] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,360] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,381] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,397] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,411] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,421] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,434] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:23:37,565] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:23:37,574] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:23:37,580] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:23:37,584] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:23:37,588] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:23:37,593] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:23:37,597] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:23:37,600] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:23:37,605] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:23:37,609] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:23:37,626] {logging_mixin.py:95} INFO - [2018-11-08 10:23:37,625] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:23:37,630] {logging_mixin.py:95} INFO - [2018-11-08 10:23:37,628] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:18:37.628312+00:00

[2018-11-08 10:23:37,636] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.527 seconds
[2018-11-08 10:24:36,308] {jobs.py:385} INFO - Started process (PID=50314) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:24:41,321] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:24:41,332] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:24:41,334] {logging_mixin.py:95} INFO - [2018-11-08 10:24:41,333] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:24:41,342] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:24:41,384] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:24:41,394] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,439] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,455] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,466] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,478] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,490] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,503] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,517] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,531] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,546] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,561] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,573] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,588] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,601] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,614] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,627] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:24:41,776] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:24:41,788] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:24:41,796] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:24:41,802] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:24:41,805] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:24:41,810] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:24:41,815] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:24:41,821] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:24:41,832] {logging_mixin.py:95} INFO - [2018-11-08 10:24:41,832] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:24:41,833] {logging_mixin.py:95} INFO - [2018-11-08 10:24:41,832] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:19:41.832924+00:00

[2018-11-08 10:24:41,838] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.531 seconds
[2018-11-08 10:25:32,156] {jobs.py:385} INFO - Started process (PID=50350) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:25:37,168] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:25:37,173] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:25:37,176] {logging_mixin.py:95} INFO - [2018-11-08 10:25:37,176] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:25:37,189] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:25:37,232] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:25:37,240] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,278] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,290] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,305] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,316] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,328] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,341] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,355] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,369] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,384] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,400] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,416] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,430] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,457] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,479] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,498] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:25:37,677] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:25:37,685] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:25:37,690] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:25:37,695] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:25:37,699] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:25:37,703] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:25:37,707] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:25:37,712] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:25:37,717] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:25:37,721] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:25:37,731] {logging_mixin.py:95} INFO - [2018-11-08 10:25:37,731] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:25:37,733] {logging_mixin.py:95} INFO - [2018-11-08 10:25:37,732] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:20:37.732220+00:00

[2018-11-08 10:25:37,739] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.583 seconds
[2018-11-08 10:26:43,186] {jobs.py:385} INFO - Started process (PID=50393) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:26:48,193] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:26:48,198] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:26:48,200] {logging_mixin.py:95} INFO - [2018-11-08 10:26:48,199] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:26:48,212] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:26:48,248] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:26:48,260] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,292] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,303] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,315] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,326] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,338] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,354] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,365] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,379] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,390] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,400] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,413] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,428] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,444] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,454] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,465] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:26:48,600] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:26:48,610] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:26:48,617] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:26:48,627] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:26:48,634] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:26:48,641] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:26:48,647] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:26:48,652] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:26:48,670] {logging_mixin.py:95} INFO - [2018-11-08 10:26:48,670] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:26:48,672] {logging_mixin.py:95} INFO - [2018-11-08 10:26:48,671] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:21:48.671453+00:00

[2018-11-08 10:26:48,682] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.496 seconds
[2018-11-08 10:27:43,006] {jobs.py:385} INFO - Started process (PID=50433) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:27:48,014] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:27:48,017] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:27:48,019] {logging_mixin.py:95} INFO - [2018-11-08 10:27:48,018] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:27:48,031] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:27:48,087] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:27:48,097] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,127] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,142] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,154] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,164] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,176] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,187] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,199] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,212] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,225] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,238] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,252] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,262] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,274] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,288] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,299] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:27:48,442] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:27:48,455] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:27:48,464] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:27:48,467] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:27:48,472] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:27:48,477] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:27:48,481] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:27:48,490] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:27:48,495] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:27:48,499] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:27:48,510] {logging_mixin.py:95} INFO - [2018-11-08 10:27:48,510] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:27:48,511] {logging_mixin.py:95} INFO - [2018-11-08 10:27:48,511] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:22:48.511067+00:00

[2018-11-08 10:27:48,519] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.513 seconds
[2018-11-08 10:28:52,396] {jobs.py:385} INFO - Started process (PID=50477) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:28:57,407] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:28:57,410] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:28:57,412] {logging_mixin.py:95} INFO - [2018-11-08 10:28:57,411] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:28:57,423] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:28:57,460] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:28:57,471] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,505] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,520] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,541] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,558] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,579] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,596] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,621] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,644] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,666] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,686] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,706] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,724] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,742] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,760] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,792] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:28:57,983] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:28:57,994] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:28:58,001] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:28:58,010] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:28:58,017] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:28:58,025] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:28:58,032] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:28:58,038] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:28:58,051] {logging_mixin.py:95} INFO - [2018-11-08 10:28:58,051] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:28:58,053] {logging_mixin.py:95} INFO - [2018-11-08 10:28:58,052] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:23:58.052421+00:00

[2018-11-08 10:28:58,059] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.663 seconds
[2018-11-08 10:29:51,495] {jobs.py:385} INFO - Started process (PID=50516) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:29:56,505] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:29:56,512] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:29:56,514] {logging_mixin.py:95} INFO - [2018-11-08 10:29:56,513] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:29:56,530] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:29:56,582] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:29:56,605] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,633] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,648] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,664] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,680] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,699] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,715] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,729] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,745] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,758] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,773] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,790] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,801] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,815] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,834] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:56,850] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:29:57,031] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:29:57,045] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:29:57,062] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:29:57,065] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:29:57,069] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:29:57,074] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:29:57,080] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:29:57,090] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:29:57,093] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:29:57,098] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:29:57,110] {logging_mixin.py:95} INFO - [2018-11-08 10:29:57,110] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:29:57,112] {logging_mixin.py:95} INFO - [2018-11-08 10:29:57,111] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:24:57.111565+00:00

[2018-11-08 10:29:57,119] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.624 seconds
[2018-11-08 10:31:01,700] {jobs.py:385} INFO - Started process (PID=50560) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:31:06,711] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:31:06,715] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:31:06,717] {logging_mixin.py:95} INFO - [2018-11-08 10:31:06,716] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:31:06,732] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:31:06,780] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:31:06,794] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:06,822] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:06,846] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:06,864] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:06,883] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:06,898] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:06,913] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:06,930] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:06,948] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:06,973] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:06,984] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:06,995] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:07,006] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:07,019] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:07,031] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:07,044] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:31:07,187] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:31:07,195] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:31:07,199] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:31:07,203] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:31:07,206] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:31:07,210] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:31:07,214] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:31:07,217] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:31:07,227] {logging_mixin.py:95} INFO - [2018-11-08 10:31:07,226] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:31:07,228] {logging_mixin.py:95} INFO - [2018-11-08 10:31:07,227] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:26:07.227598+00:00

[2018-11-08 10:31:07,235] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.535 seconds
[2018-11-08 10:32:01,068] {jobs.py:385} INFO - Started process (PID=50596) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:32:06,077] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:32:06,079] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:32:06,081] {logging_mixin.py:95} INFO - [2018-11-08 10:32:06,080] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:32:06,100] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:32:06,141] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:32:06,150] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,174] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,184] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,197] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,209] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,222] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,236] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,250] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,266] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,281] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,296] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,314] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,329] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,342] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,355] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,366] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:32:06,500] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:32:06,511] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:32:06,519] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:32:06,523] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:32:06,527] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:32:06,531] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:32:06,536] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:32:06,542] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:32:06,549] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:32:06,557] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:32:06,578] {logging_mixin.py:95} INFO - [2018-11-08 10:32:06,577] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:32:06,579] {logging_mixin.py:95} INFO - [2018-11-08 10:32:06,578] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:27:06.578462+00:00

[2018-11-08 10:32:06,588] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.519 seconds
[2018-11-08 10:33:15,436] {jobs.py:385} INFO - Started process (PID=50646) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:33:20,448] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:33:20,454] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:33:20,456] {logging_mixin.py:95} INFO - [2018-11-08 10:33:20,455] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:33:20,469] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:33:20,516] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:33:20,524] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,554] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,564] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,578] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,589] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,603] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,615] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,626] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,637] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,650] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,662] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,673] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,684] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,697] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,708] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,719] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:33:20,839] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:33:20,848] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:33:20,853] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:33:20,856] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:33:20,860] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:33:20,865] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:33:20,870] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:33:20,875] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:33:20,885] {logging_mixin.py:95} INFO - [2018-11-08 10:33:20,885] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:33:20,887] {logging_mixin.py:95} INFO - [2018-11-08 10:33:20,886] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:28:20.886154+00:00

[2018-11-08 10:33:20,892] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.456 seconds
[2018-11-08 10:34:19,025] {jobs.py:385} INFO - Started process (PID=50685) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:34:24,037] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:34:24,040] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:34:24,045] {logging_mixin.py:95} INFO - [2018-11-08 10:34:24,044] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:34:24,060] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:34:24,124] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:34:24,145] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,186] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,210] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,232] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,262] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,297] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,321] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,342] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,396] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,432] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,453] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,476] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,500] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,527] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,550] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,589] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:34:24,795] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:34:24,822] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:34:24,829] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:34:24,838] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:34:24,846] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:34:24,855] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:34:24,870] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:34:24,889] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:34:24,931] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:34:24,995] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:34:25,021] {logging_mixin.py:95} INFO - [2018-11-08 10:34:25,020] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:34:25,024] {logging_mixin.py:95} INFO - [2018-11-08 10:34:25,022] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:29:25.022378+00:00

[2018-11-08 10:34:25,038] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.013 seconds
[2018-11-08 10:35:53,977] {jobs.py:385} INFO - Started process (PID=50750) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:35:58,985] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:35:58,988] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:35:58,989] {logging_mixin.py:95} INFO - [2018-11-08 10:35:58,988] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:35:59,000] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:35:59,040] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:35:59,050] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,084] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,099] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,120] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,134] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,148] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,163] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,180] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,191] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,202] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,224] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,235] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,249] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,265] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,281] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,296] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:35:59,458] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:35:59,475] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:35:59,481] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:35:59,486] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:35:59,490] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:35:59,494] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:35:59,499] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:35:59,506] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:35:59,518] {logging_mixin.py:95} INFO - [2018-11-08 10:35:59,517] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:35:59,519] {logging_mixin.py:95} INFO - [2018-11-08 10:35:59,519] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:30:59.519050+00:00

[2018-11-08 10:35:59,525] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.548 seconds
[2018-11-08 10:36:54,976] {jobs.py:385} INFO - Started process (PID=50791) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:36:59,990] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:36:59,994] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:36:59,996] {logging_mixin.py:95} INFO - [2018-11-08 10:36:59,995] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:37:00,015] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:37:00,085] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:37:00,111] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:00,208] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:00,257] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:00,298] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:00,357] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:00,396] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:00,504] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:00,573] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:00,637] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:00,740] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:00,851] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:00,886] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:01,003] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:01,044] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:01,103] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:01,142] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:37:01,651] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:37:01,696] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:37:01,708] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:37:01,718] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:37:01,727] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:37:01,736] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:37:01,744] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:37:01,753] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:37:01,762] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:37:01,772] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:37:01,789] {logging_mixin.py:95} INFO - [2018-11-08 10:37:01,788] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:37:01,792] {logging_mixin.py:95} INFO - [2018-11-08 10:37:01,790] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:32:01.790606+00:00

[2018-11-08 10:37:01,804] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.829 seconds
[2018-11-08 10:39:52,020] {jobs.py:385} INFO - Started process (PID=50855) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:39:57,033] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:39:57,040] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:39:57,041] {logging_mixin.py:95} INFO - [2018-11-08 10:39:57,041] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:39:57,049] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:39:57,090] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:39:57,108] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,145] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,161] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,175] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,186] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,197] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,212] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,225] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,239] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,268] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,279] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,297] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,307] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,320] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,333] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,344] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:39:57,472] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:39:57,485] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:39:57,489] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:39:57,496] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:39:57,499] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:39:57,503] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:39:57,507] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:39:57,511] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:39:57,521] {logging_mixin.py:95} INFO - [2018-11-08 10:39:57,520] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:39:57,522] {logging_mixin.py:95} INFO - [2018-11-08 10:39:57,521] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:34:57.521723+00:00

[2018-11-08 10:39:57,532] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.512 seconds
[2018-11-08 10:40:51,091] {jobs.py:385} INFO - Started process (PID=50891) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:40:56,102] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:40:56,106] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:40:56,110] {logging_mixin.py:95} INFO - [2018-11-08 10:40:56,109] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:40:56,118] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:40:56,153] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:40:56,168] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,193] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,210] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,221] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,231] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,242] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,253] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,263] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,275] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,286] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,297] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,306] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,319] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,330] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,340] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,351] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:40:56,468] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:40:56,481] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:40:56,488] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:40:56,492] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:40:56,495] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:40:56,499] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:40:56,502] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:40:56,506] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:40:56,510] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:40:56,514] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:40:56,522] {logging_mixin.py:95} INFO - [2018-11-08 10:40:56,522] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:40:56,523] {logging_mixin.py:95} INFO - [2018-11-08 10:40:56,523] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:35:56.523255+00:00

[2018-11-08 10:40:56,528] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.437 seconds
[2018-11-08 10:42:02,326] {jobs.py:385} INFO - Started process (PID=50936) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:42:07,342] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:42:07,349] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:42:07,351] {logging_mixin.py:95} INFO - [2018-11-08 10:42:07,350] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:42:07,358] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:42:07,414] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:42:07,432] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,481] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,498] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,535] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,559] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,592] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,632] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,659] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,688] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,714] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,744] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,796] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,822] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,864] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,890] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:07,919] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:42:08,873] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:42:08,882] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:42:08,894] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:42:08,900] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:42:08,908] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:42:08,913] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:42:08,918] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:42:08,929] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:42:08,942] {logging_mixin.py:95} INFO - [2018-11-08 10:42:08,941] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:42:08,944] {logging_mixin.py:95} INFO - [2018-11-08 10:42:08,942] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:37:08.942769+00:00

[2018-11-08 10:42:08,950] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.624 seconds
[2018-11-08 10:43:13,326] {jobs.py:385} INFO - Started process (PID=50973) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:43:18,342] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:43:18,357] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:43:18,359] {logging_mixin.py:95} INFO - [2018-11-08 10:43:18,359] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:43:18,374] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:43:18,419] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:43:18,434] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,475] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,495] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,520] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,543] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,558] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,577] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,605] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,637] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,661] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,687] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,702] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,726] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,759] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,828] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:18,915] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:43:19,435] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:43:19,444] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:43:19,452] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:43:19,457] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:43:19,461] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:43:19,465] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:43:19,471] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:43:19,477] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:43:19,485] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:43:19,493] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:43:19,513] {logging_mixin.py:95} INFO - [2018-11-08 10:43:19,512] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:43:19,514] {logging_mixin.py:95} INFO - [2018-11-08 10:43:19,513] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:38:19.513619+00:00

[2018-11-08 10:43:19,523] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.197 seconds
[2018-11-08 10:44:29,402] {jobs.py:385} INFO - Started process (PID=51024) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:44:34,411] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:44:34,413] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:44:34,415] {logging_mixin.py:95} INFO - [2018-11-08 10:44:34,414] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:44:34,427] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:44:34,492] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:44:34,509] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,554] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,602] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,637] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,662] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,859] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,874] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,889] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,904] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,922] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,939] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,958] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,972] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:34,993] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:35,018] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:35,034] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:44:35,267] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:44:35,286] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:44:35,291] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:44:35,295] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:44:35,303] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:44:35,308] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:44:35,315] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:44:35,326] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:44:35,341] {logging_mixin.py:95} INFO - [2018-11-08 10:44:35,341] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:44:35,343] {logging_mixin.py:95} INFO - [2018-11-08 10:44:35,342] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:39:35.342367+00:00

[2018-11-08 10:44:35,355] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.953 seconds
[2018-11-08 10:45:23,229] {jobs.py:385} INFO - Started process (PID=51058) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:45:28,244] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:45:28,249] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:45:28,252] {logging_mixin.py:95} INFO - [2018-11-08 10:45:28,251] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:45:28,259] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:45:28,298] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:45:28,316] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,341] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,354] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,367] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,380] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,396] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,409] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,422] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,437] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,447] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,462] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,473] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,486] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,498] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,609] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,619] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:45:28,753] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:45:28,760] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:45:28,764] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:45:28,768] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:45:28,771] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:45:28,776] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:45:28,783] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:45:28,788] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:45:28,799] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:45:28,804] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:45:28,820] {logging_mixin.py:95} INFO - [2018-11-08 10:45:28,819] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:45:28,821] {logging_mixin.py:95} INFO - [2018-11-08 10:45:28,820] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:40:28.820731+00:00

[2018-11-08 10:45:28,831] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.602 seconds
[2018-11-08 10:46:38,263] {jobs.py:385} INFO - Started process (PID=51106) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:46:43,274] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:46:43,280] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:46:43,284] {logging_mixin.py:95} INFO - [2018-11-08 10:46:43,283] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:46:43,302] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:46:43,342] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:46:43,362] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,700] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,714] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,726] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,742] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,759] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,775] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,793] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,810] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,825] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,840] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,857] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,877] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,896] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,908] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:43,919] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:46:44,064] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:46:44,072] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:46:44,076] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:46:44,080] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:46:44,083] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:46:44,087] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:46:44,092] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:46:44,095] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:46:44,105] {logging_mixin.py:95} INFO - [2018-11-08 10:46:44,105] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:46:44,106] {logging_mixin.py:95} INFO - [2018-11-08 10:46:44,105] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:41:44.105883+00:00

[2018-11-08 10:46:44,110] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.847 seconds
[2018-11-08 10:47:24,586] {jobs.py:385} INFO - Started process (PID=51138) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:47:29,595] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:47:29,598] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:47:29,602] {logging_mixin.py:95} INFO - [2018-11-08 10:47:29,602] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:47:29,611] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:47:29,658] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:47:29,677] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:29,721] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:29,739] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:29,756] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:29,771] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:29,788] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:29,804] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:29,819] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:29,836] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:29,855] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:29,874] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:30,047] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:30,082] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:30,108] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:30,127] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:30,153] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:47:30,404] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:47:30,420] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:47:30,426] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:47:30,431] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:47:30,438] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:47:30,444] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:47:30,448] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:47:30,456] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:47:30,461] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:47:30,472] {logging_mixin.py:95} INFO - [2018-11-08 10:47:30,472] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:47:30,474] {logging_mixin.py:95} INFO - [2018-11-08 10:47:30,473] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:42:30.473495+00:00

[2018-11-08 10:47:30,479] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.894 seconds
[2018-11-08 10:48:36,100] {jobs.py:385} INFO - Started process (PID=51181) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:48:41,111] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:48:41,113] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:48:41,115] {logging_mixin.py:95} INFO - [2018-11-08 10:48:41,114] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:48:41,130] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:48:41,232] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:48:41,408] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,429] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,457] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,478] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,499] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,516] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,527] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,544] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,558] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,587] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,606] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,621] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,711] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,745] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,763] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,778] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:48:41,929] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:48:41,939] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:48:41,944] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:48:41,947] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:48:41,954] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:48:41,959] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:48:41,968] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:48:41,972] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:48:41,976] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:48:41,986] {logging_mixin.py:95} INFO - [2018-11-08 10:48:41,986] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:48:41,987] {logging_mixin.py:95} INFO - [2018-11-08 10:48:41,987] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:43:41.987020+00:00

[2018-11-08 10:48:41,992] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.892 seconds
[2018-11-08 10:49:48,329] {jobs.py:385} INFO - Started process (PID=51223) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:49:53,339] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:49:53,342] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:49:53,343] {logging_mixin.py:95} INFO - [2018-11-08 10:49:53,343] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:49:53,353] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:49:53,404] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:49:53,418] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,450] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,475] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,503] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,531] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,559] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,579] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,604] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,636] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,669] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,693] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,728] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,799] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,849] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,870] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:53,900] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:49:54,119] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:49:54,131] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:49:54,139] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:49:54,143] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:49:54,147] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:49:54,151] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:49:54,156] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:49:54,162] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:49:54,166] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:49:54,177] {logging_mixin.py:95} INFO - [2018-11-08 10:49:54,177] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:49:54,178] {logging_mixin.py:95} INFO - [2018-11-08 10:49:54,177] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:44:54.177901+00:00

[2018-11-08 10:49:54,185] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.856 seconds
[2018-11-08 10:50:58,795] {jobs.py:385} INFO - Started process (PID=51264) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:51:03,806] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:51:03,809] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:51:03,811] {logging_mixin.py:95} INFO - [2018-11-08 10:51:03,810] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:51:03,821] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:51:03,856] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:51:03,870] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:03,889] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:03,907] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:03,924] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:03,936] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:03,947] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:03,959] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:03,970] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:03,981] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:03,993] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:04,006] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:04,018] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:04,033] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:04,045] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:04,055] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:04,066] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:51:04,197] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:51:04,205] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:51:04,210] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:51:04,214] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:51:04,218] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:51:04,221] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:51:04,224] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:51:04,228] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:51:04,233] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:51:04,242] {logging_mixin.py:95} INFO - [2018-11-08 10:51:04,241] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:51:04,243] {logging_mixin.py:95} INFO - [2018-11-08 10:51:04,242] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:46:04.242452+00:00

[2018-11-08 10:51:04,247] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.452 seconds
[2018-11-08 10:52:07,442] {jobs.py:385} INFO - Started process (PID=51304) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:52:12,452] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:52:12,460] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:52:12,462] {logging_mixin.py:95} INFO - [2018-11-08 10:52:12,461] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:52:12,469] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:52:12,506] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:52:12,516] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,537] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,550] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,562] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,575] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,589] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,604] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,619] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,631] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,642] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,653] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,664] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,675] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,690] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,701] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,713] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:52:12,846] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:52:12,855] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:52:12,860] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:52:12,864] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:52:12,869] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:52:12,874] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:52:12,878] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:52:12,881] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:52:12,887] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:52:12,898] {logging_mixin.py:95} INFO - [2018-11-08 10:52:12,898] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:52:12,900] {logging_mixin.py:95} INFO - [2018-11-08 10:52:12,898] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:47:12.898943+00:00

[2018-11-08 10:52:12,906] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.464 seconds
[2018-11-08 10:53:12,492] {jobs.py:385} INFO - Started process (PID=51346) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:53:17,503] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:53:17,510] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:53:17,513] {logging_mixin.py:95} INFO - [2018-11-08 10:53:17,512] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:53:17,521] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:53:17,551] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:53:17,559] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,576] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,588] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,599] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,614] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,624] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,635] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,645] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,656] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,670] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,681] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,692] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,704] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,714] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,726] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,736] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:53:17,851] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:53:17,858] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:53:17,863] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:53:17,866] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:53:17,870] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:53:17,873] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:53:17,876] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:53:17,880] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:53:17,884] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:53:17,892] {logging_mixin.py:95} INFO - [2018-11-08 10:53:17,891] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:53:17,893] {logging_mixin.py:95} INFO - [2018-11-08 10:53:17,892] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:48:17.892473+00:00

[2018-11-08 10:53:17,897] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.405 seconds
[2018-11-08 10:54:16,953] {jobs.py:385} INFO - Started process (PID=51386) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:54:21,959] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:54:21,967] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:54:21,968] {logging_mixin.py:95} INFO - [2018-11-08 10:54:21,967] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:54:21,975] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:54:22,013] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:54:22,027] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,046] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,062] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,084] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,103] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,119] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,132] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,147] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,160] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,172] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,195] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,232] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,264] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,283] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,303] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,334] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:54:22,601] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:54:22,614] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:54:22,620] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:54:22,628] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:54:22,633] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:54:22,637] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:54:22,643] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:54:22,648] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:54:22,660] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:54:22,677] {logging_mixin.py:95} INFO - [2018-11-08 10:54:22,676] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:54:22,679] {logging_mixin.py:95} INFO - [2018-11-08 10:54:22,677] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:49:22.677893+00:00

[2018-11-08 10:54:22,686] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.733 seconds
[2018-11-08 10:55:26,007] {jobs.py:385} INFO - Started process (PID=51426) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:55:31,016] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:55:31,020] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:55:31,021] {logging_mixin.py:95} INFO - [2018-11-08 10:55:31,021] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:55:31,031] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:55:31,062] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:55:31,072] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,097] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,110] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,121] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,157] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,235] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,252] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,269] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,281] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,295] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,307] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,323] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,335] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,348] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,362] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,376] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:55:31,519] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:55:31,528] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:55:31,532] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:55:31,536] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:55:31,539] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:55:31,543] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:55:31,548] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:55:31,552] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:55:31,558] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:55:31,571] {logging_mixin.py:95} INFO - [2018-11-08 10:55:31,569] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:55:31,573] {logging_mixin.py:95} INFO - [2018-11-08 10:55:31,572] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:50:31.572114+00:00

[2018-11-08 10:55:31,579] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.572 seconds
[2018-11-08 10:56:36,465] {jobs.py:385} INFO - Started process (PID=51473) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:56:41,475] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:56:41,479] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:56:41,481] {logging_mixin.py:95} INFO - [2018-11-08 10:56:41,480] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:56:41,490] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:56:41,529] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:56:41,542] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,561] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,579] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,593] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,609] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,624] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,638] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,657] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,681] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,700] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,724] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,755] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,773] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,790] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,809] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:41,833] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:56:42,043] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:56:42,057] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:56:42,066] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:56:42,073] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:56:42,080] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:56:42,085] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:56:42,091] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:56:42,099] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:56:42,108] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:56:42,130] {logging_mixin.py:95} INFO - [2018-11-08 10:56:42,130] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:56:42,131] {logging_mixin.py:95} INFO - [2018-11-08 10:56:42,131] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:51:42.131088+00:00

[2018-11-08 10:56:42,138] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.673 seconds
[2018-11-08 10:57:45,628] {jobs.py:385} INFO - Started process (PID=51512) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:57:50,638] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:57:50,643] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:57:50,644] {logging_mixin.py:95} INFO - [2018-11-08 10:57:50,644] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:57:50,653] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:57:50,748] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:57:50,770] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:50,841] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:50,868] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:50,903] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,014] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,110] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,181] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,215] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,243] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,269] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,294] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,376] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,425] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,494] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,522] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:51,544] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:57:52,162] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:57:52,170] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:57:52,182] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:57:52,192] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:57:52,199] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:57:52,211] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:57:52,226] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:57:52,250] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:57:52,260] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:57:52,286] {logging_mixin.py:95} INFO - [2018-11-08 10:57:52,286] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:57:52,289] {logging_mixin.py:95} INFO - [2018-11-08 10:57:52,287] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:52:52.287607+00:00

[2018-11-08 10:57:52,305] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.678 seconds
[2018-11-08 10:58:55,331] {jobs.py:385} INFO - Started process (PID=51554) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:59:00,337] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 10:59:00,341] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 10:59:00,344] {logging_mixin.py:95} INFO - [2018-11-08 10:59:00,343] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 10:59:00,365] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 10:59:00,428] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 10:59:00,450] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,490] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,511] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,531] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,555] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,578] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,597] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,614] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,633] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,648] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,661] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,674] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,685] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,700] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,711] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,724] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 10:59:00,924] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 10:59:00,936] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:59:00,941] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:59:00,944] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:59:00,948] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:59:00,954] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:59:00,958] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:59:00,962] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:59:00,966] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 10:59:00,977] {logging_mixin.py:95} INFO - [2018-11-08 10:59:00,977] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 10:59:00,979] {logging_mixin.py:95} INFO - [2018-11-08 10:59:00,978] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:54:00.978642+00:00

[2018-11-08 10:59:00,989] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.658 seconds
[2018-11-08 11:00:01,825] {jobs.py:385} INFO - Started process (PID=51597) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:00:06,832] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:00:06,835] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:00:06,837] {logging_mixin.py:95} INFO - [2018-11-08 11:00:06,836] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:00:06,847] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:00:06,881] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:00:06,889] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:06,910] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:06,924] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:06,936] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:06,950] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:06,961] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:06,974] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:06,985] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:06,996] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:07,008] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:07,020] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:07,030] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:07,047] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:07,063] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:07,077] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:07,088] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:00:07,211] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:00:07,221] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:00:07,225] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:00:07,228] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:00:07,232] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:00:07,237] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:00:07,241] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:00:07,246] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:00:07,249] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:00:07,259] {logging_mixin.py:95} INFO - [2018-11-08 11:00:07,259] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:00:07,260] {logging_mixin.py:95} INFO - [2018-11-08 11:00:07,260] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:55:07.260266+00:00

[2018-11-08 11:00:07,265] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.441 seconds
[2018-11-08 11:01:05,727] {jobs.py:385} INFO - Started process (PID=51637) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:01:10,736] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:01:10,740] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:01:10,742] {logging_mixin.py:95} INFO - [2018-11-08 11:01:10,741] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:01:10,755] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:01:10,793] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:01:10,806] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,837] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,854] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,868] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,878] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,893] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,915] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,925] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,935] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,949] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,964] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,977] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:10,990] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:11,002] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:11,013] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:11,026] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:01:11,150] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:01:11,159] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:01:11,163] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:01:11,166] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:01:11,170] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:01:11,173] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:01:11,176] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:01:11,179] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:01:11,183] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:01:11,194] {logging_mixin.py:95} INFO - [2018-11-08 11:01:11,193] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:01:11,196] {logging_mixin.py:95} INFO - [2018-11-08 11:01:11,195] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:56:11.195170+00:00

[2018-11-08 11:01:11,202] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.474 seconds
[2018-11-08 11:02:13,388] {jobs.py:385} INFO - Started process (PID=51679) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:02:18,396] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:02:18,397] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:02:18,399] {logging_mixin.py:95} INFO - [2018-11-08 11:02:18,398] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:02:18,406] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:02:18,458] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:02:18,467] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,498] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,517] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,539] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,550] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,565] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,583] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,600] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,620] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,633] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,646] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,657] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,675] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,689] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,706] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,717] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:02:18,874] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:02:18,887] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:02:18,895] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:02:18,899] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:02:18,903] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:02:18,908] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:02:18,912] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:02:18,923] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:02:18,927] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:02:18,939] {logging_mixin.py:95} INFO - [2018-11-08 11:02:18,939] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:02:18,941] {logging_mixin.py:95} INFO - [2018-11-08 11:02:18,940] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:57:18.940198+00:00

[2018-11-08 11:02:18,945] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.557 seconds
[2018-11-08 11:03:18,843] {jobs.py:385} INFO - Started process (PID=51719) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:03:23,852] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:03:23,856] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:03:23,857] {logging_mixin.py:95} INFO - [2018-11-08 11:03:23,857] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:03:23,866] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:03:23,899] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:03:23,913] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:23,941] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:23,958] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:23,979] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:23,993] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,012] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,036] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,061] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,091] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,114] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,145] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,178] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,212] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,238] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,261] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,281] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:03:24,461] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:03:24,471] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:03:24,477] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:03:24,482] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:03:24,485] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:03:24,489] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:03:24,493] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:03:24,497] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:03:24,501] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:03:24,513] {logging_mixin.py:95} INFO - [2018-11-08 11:03:24,512] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:03:24,514] {logging_mixin.py:95} INFO - [2018-11-08 11:03:24,513] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:58:24.513801+00:00

[2018-11-08 11:03:24,523] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.679 seconds
[2018-11-08 11:04:22,569] {jobs.py:385} INFO - Started process (PID=51759) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:04:27,582] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:04:27,584] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:04:27,586] {logging_mixin.py:95} INFO - [2018-11-08 11:04:27,585] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:04:27,602] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:04:27,649] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:04:27,666] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,705] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,722] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,735] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,750] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,767] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,785] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,800] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,818] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,834] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,866] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,892] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,924] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,946] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,965] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:27,987] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:04:28,349] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:04:28,367] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:04:28,379] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:04:28,392] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:04:28,401] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:04:28,412] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:04:28,422] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:04:28,432] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:04:28,445] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:04:28,463] {logging_mixin.py:95} INFO - [2018-11-08 11:04:28,463] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:04:28,466] {logging_mixin.py:95} INFO - [2018-11-08 11:04:28,464] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 12:59:28.464708+00:00

[2018-11-08 11:04:28,476] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.906 seconds
[2018-11-08 11:06:19,992] {jobs.py:385} INFO - Started process (PID=51809) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:06:25,045] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:06:25,055] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:06:25,058] {logging_mixin.py:95} INFO - [2018-11-08 11:06:25,057] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:06:25,065] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:06:25,146] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:06:25,161] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,215] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,248] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,290] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,329] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,350] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,367] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,397] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,416] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,439] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,455] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,472] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,492] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,509] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,527] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,545] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:06:25,896] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:06:25,920] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:06:25,933] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:06:25,950] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:06:25,960] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:06:25,968] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:06:26,031] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:06:26,045] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:06:26,079] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:06:26,114] {logging_mixin.py:95} INFO - [2018-11-08 11:06:26,114] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:06:26,116] {logging_mixin.py:95} INFO - [2018-11-08 11:06:26,115] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:01:26.115569+00:00

[2018-11-08 11:06:26,136] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.144 seconds
[2018-11-08 11:07:37,339] {jobs.py:385} INFO - Started process (PID=51857) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:07:42,352] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:07:42,359] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:07:42,361] {logging_mixin.py:95} INFO - [2018-11-08 11:07:42,360] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:07:42,374] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:07:42,463] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:07:42,477] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,510] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,537] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,557] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,577] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,601] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,615] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,641] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,666] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,685] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,738] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,771] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,798] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,854] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,895] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:42,926] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:07:43,134] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:07:43,143] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:07:43,147] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:07:43,152] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:07:43,157] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:07:43,160] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:07:43,163] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:07:43,169] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:07:43,174] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:07:43,186] {logging_mixin.py:95} INFO - [2018-11-08 11:07:43,185] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:07:43,191] {logging_mixin.py:95} INFO - [2018-11-08 11:07:43,187] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:02:43.187558+00:00

[2018-11-08 11:07:43,201] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.862 seconds
[2018-11-08 11:08:30,829] {jobs.py:385} INFO - Started process (PID=51892) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:08:35,841] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:08:35,843] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:08:35,845] {logging_mixin.py:95} INFO - [2018-11-08 11:08:35,844] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:08:35,860] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:08:35,928] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:08:35,957] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:35,995] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,012] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,027] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,049] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,077] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,122] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,165] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,208] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,282] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,305] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,327] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,355] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,372] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,391] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:36,414] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:08:37,172] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:08:37,212] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:08:37,223] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:08:37,229] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:08:37,233] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:08:37,242] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:08:37,250] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:08:37,256] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:08:37,265] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:08:37,281] {logging_mixin.py:95} INFO - [2018-11-08 11:08:37,281] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:08:37,283] {logging_mixin.py:95} INFO - [2018-11-08 11:08:37,282] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:03:37.282199+00:00

[2018-11-08 11:08:37,290] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.461 seconds
[2018-11-08 11:10:16,594] {jobs.py:385} INFO - Started process (PID=51937) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:10:21,605] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:10:21,612] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:10:21,616] {logging_mixin.py:95} INFO - [2018-11-08 11:10:21,614] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:10:21,634] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:10:21,743] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:10:21,825] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:22,083] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:22,199] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:22,246] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:22,318] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:22,381] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:22,415] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:22,649] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:22,860] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:22,984] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:23,038] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:23,086] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:23,127] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:23,184] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:23,218] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:23,256] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:10:24,039] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:10:24,082] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:10:24,110] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:10:24,126] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:10:24,136] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:10:24,150] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:10:24,160] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:10:24,171] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:10:24,178] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:10:24,195] {logging_mixin.py:95} INFO - [2018-11-08 11:10:24,195] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:10:24,198] {logging_mixin.py:95} INFO - [2018-11-08 11:10:24,196] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:05:24.196944+00:00

[2018-11-08 11:10:24,206] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.613 seconds
[2018-11-08 11:11:49,345] {jobs.py:385} INFO - Started process (PID=51995) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:11:54,366] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:11:54,370] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:11:54,372] {logging_mixin.py:95} INFO - [2018-11-08 11:11:54,371] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:11:54,380] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:11:54,419] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:11:54,451] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,513] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,538] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,558] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,578] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,590] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,602] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,613] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,625] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,635] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,647] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,659] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,670] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,683] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,693] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,703] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:11:54,835] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:11:54,846] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:11:54,854] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:11:54,861] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:11:54,865] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:11:54,871] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:11:54,874] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:11:54,878] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:11:54,883] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:11:54,896] {logging_mixin.py:95} INFO - [2018-11-08 11:11:54,896] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:11:54,898] {logging_mixin.py:95} INFO - [2018-11-08 11:11:54,897] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:06:54.897324+00:00

[2018-11-08 11:11:54,903] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.558 seconds
[2018-11-08 11:12:51,317] {jobs.py:385} INFO - Started process (PID=52035) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:12:56,335] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:12:56,339] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:12:56,341] {logging_mixin.py:95} INFO - [2018-11-08 11:12:56,340] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:12:56,352] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:12:56,381] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:12:56,398] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,414] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,425] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,436] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,447] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,459] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,469] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,479] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,492] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,502] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,519] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,540] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,550] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,563] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,577] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,588] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:12:56,704] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:12:56,713] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:12:56,717] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:12:56,721] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:12:56,725] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:12:56,728] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:12:56,731] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:12:56,736] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:12:56,740] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:12:56,750] {logging_mixin.py:95} INFO - [2018-11-08 11:12:56,749] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:12:56,751] {logging_mixin.py:95} INFO - [2018-11-08 11:12:56,750] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:07:56.750882+00:00

[2018-11-08 11:12:56,756] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.438 seconds
[2018-11-08 11:13:53,744] {jobs.py:385} INFO - Started process (PID=52074) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:13:58,751] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:13:58,754] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:13:58,756] {logging_mixin.py:95} INFO - [2018-11-08 11:13:58,755] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:13:58,764] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:13:58,793] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:13:58,801] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,816] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,828] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,841] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,852] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,862] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,873] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,883] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,896] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,907] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,917] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,929] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,939] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,952] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,962] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:58,972] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:13:59,093] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:13:59,101] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:13:59,105] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:13:59,108] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:13:59,111] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:13:59,114] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:13:59,118] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:13:59,122] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:13:59,125] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:13:59,137] {logging_mixin.py:95} INFO - [2018-11-08 11:13:59,136] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:13:59,139] {logging_mixin.py:95} INFO - [2018-11-08 11:13:59,138] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:08:59.138096+00:00

[2018-11-08 11:13:59,145] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.401 seconds
[2018-11-08 11:14:57,466] {jobs.py:385} INFO - Started process (PID=52120) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:15:02,477] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:15:02,485] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:15:02,487] {logging_mixin.py:95} INFO - [2018-11-08 11:15:02,486] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:15:02,498] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:15:02,531] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:15:02,541] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,574] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,587] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,602] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,612] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,624] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,636] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,647] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,658] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,667] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,678] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,690] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,700] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,711] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,721] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,736] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:15:02,866] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:15:02,877] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:15:02,881] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:15:02,884] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:15:02,887] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:15:02,891] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:15:02,902] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:15:02,909] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:15:02,912] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:15:02,922] {logging_mixin.py:95} INFO - [2018-11-08 11:15:02,921] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:15:02,923] {logging_mixin.py:95} INFO - [2018-11-08 11:15:02,922] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:10:02.922884+00:00

[2018-11-08 11:15:02,935] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.469 seconds
[2018-11-08 11:15:58,299] {jobs.py:385} INFO - Started process (PID=52159) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:16:03,311] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:16:03,314] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:16:03,316] {logging_mixin.py:95} INFO - [2018-11-08 11:16:03,315] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:16:03,325] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:16:03,364] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:16:03,379] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,411] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,422] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,441] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,456] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,466] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,479] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,493] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,510] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,525] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,538] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,549] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,560] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,572] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,583] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,595] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:16:03,708] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:16:03,717] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:16:03,720] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:16:03,725] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:16:03,728] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:16:03,731] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:16:03,735] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:16:03,738] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:16:03,742] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:16:03,752] {logging_mixin.py:95} INFO - [2018-11-08 11:16:03,752] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:16:03,754] {logging_mixin.py:95} INFO - [2018-11-08 11:16:03,753] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:11:03.753201+00:00

[2018-11-08 11:16:03,759] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.460 seconds
[2018-11-08 11:17:00,788] {jobs.py:385} INFO - Started process (PID=52201) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:17:05,810] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:17:05,818] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:17:05,829] {logging_mixin.py:95} INFO - [2018-11-08 11:17:05,819] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:17:05,869] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:17:06,082] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:17:06,231] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,324] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,357] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,383] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,404] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,421] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,443] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,461] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,483] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,499] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,520] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,541] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,561] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,577] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,599] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,614] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:17:06,835] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:17:06,855] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:17:06,859] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:17:06,873] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:17:06,884] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:17:06,890] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:17:06,899] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:17:06,906] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:17:06,921] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:17:06,940] {logging_mixin.py:95} INFO - [2018-11-08 11:17:06,940] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:17:06,943] {logging_mixin.py:95} INFO - [2018-11-08 11:17:06,941] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:12:06.941427+00:00

[2018-11-08 11:17:06,952] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.164 seconds
[2018-11-08 11:18:34,758] {jobs.py:385} INFO - Started process (PID=52256) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:18:39,769] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:18:39,772] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:18:39,773] {logging_mixin.py:95} INFO - [2018-11-08 11:18:39,772] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:18:39,785] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:18:39,828] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:18:39,845] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:39,897] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:39,916] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:39,942] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:39,983] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,024] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,057] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,083] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,105] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,133] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,165] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,183] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,201] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,219] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,236] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,255] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:18:40,457] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:18:40,469] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:18:40,475] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:18:40,482] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:18:40,491] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:18:40,498] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:18:40,503] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:18:40,509] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:18:40,515] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:18:40,526] {logging_mixin.py:95} INFO - [2018-11-08 11:18:40,526] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:18:40,528] {logging_mixin.py:95} INFO - [2018-11-08 11:18:40,527] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:13:40.527428+00:00

[2018-11-08 11:18:40,535] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.777 seconds
[2018-11-08 11:19:46,161] {jobs.py:385} INFO - Started process (PID=52310) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:19:51,171] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:19:51,174] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:19:51,176] {logging_mixin.py:95} INFO - [2018-11-08 11:19:51,175] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:19:51,185] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:19:51,231] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:19:51,245] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,282] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,297] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,313] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,331] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,348] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,363] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,380] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,397] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,417] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,434] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,450] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,465] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,482] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,496] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,512] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:19:51,699] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:19:51,713] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:19:51,718] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:19:51,723] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:19:51,729] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:19:51,733] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:19:51,738] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:19:51,743] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:19:51,748] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:19:51,758] {logging_mixin.py:95} INFO - [2018-11-08 11:19:51,757] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:19:51,759] {logging_mixin.py:95} INFO - [2018-11-08 11:19:51,758] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:14:51.758644+00:00

[2018-11-08 11:19:51,765] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.603 seconds
[2018-11-08 11:20:46,243] {jobs.py:385} INFO - Started process (PID=52360) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:20:51,259] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:20:51,263] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:20:51,265] {logging_mixin.py:95} INFO - [2018-11-08 11:20:51,264] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:20:51,276] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:20:51,321] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:20:51,331] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,367] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,389] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,409] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,422] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,441] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,458] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,471] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,486] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,504] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,550] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,575] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,601] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,630] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,667] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,700] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:20:51,980] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:20:51,995] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:20:52,003] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:20:52,008] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:20:52,013] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:20:52,018] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:20:52,021] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:20:52,028] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:20:52,032] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:20:52,043] {logging_mixin.py:95} INFO - [2018-11-08 11:20:52,042] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:20:52,044] {logging_mixin.py:95} INFO - [2018-11-08 11:20:52,043] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:15:52.043658+00:00

[2018-11-08 11:20:52,055] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.811 seconds
[2018-11-08 11:22:02,876] {jobs.py:385} INFO - Started process (PID=52413) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:22:07,886] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:22:07,889] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:22:07,891] {logging_mixin.py:95} INFO - [2018-11-08 11:22:07,890] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:22:07,900] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:22:07,947] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:22:07,966] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,017] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,034] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,056] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,074] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,086] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,113] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,130] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,147] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,162] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,179] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,195] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,208] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,223] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,243] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,255] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:22:08,424] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:22:08,433] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:22:08,439] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:22:08,444] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:22:08,447] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:22:08,451] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:22:08,456] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:22:08,462] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:22:08,466] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:22:08,478] {logging_mixin.py:95} INFO - [2018-11-08 11:22:08,478] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:22:08,480] {logging_mixin.py:95} INFO - [2018-11-08 11:22:08,479] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:17:08.479632+00:00

[2018-11-08 11:22:08,489] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.613 seconds
[2018-11-08 11:23:21,882] {jobs.py:385} INFO - Started process (PID=52473) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:23:26,894] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:23:26,896] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:23:26,897] {logging_mixin.py:95} INFO - [2018-11-08 11:23:26,897] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:23:26,912] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:23:26,951] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:23:26,962] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,002] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,018] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,033] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,048] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,066] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,090] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,111] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,127] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,145] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,161] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,182] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,199] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,216] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,234] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,249] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:23:27,422] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:23:27,431] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:23:27,438] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:23:27,443] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:23:27,446] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:23:27,450] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:23:27,454] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:23:27,462] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:23:27,466] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:23:27,478] {logging_mixin.py:95} INFO - [2018-11-08 11:23:27,477] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:23:27,479] {logging_mixin.py:95} INFO - [2018-11-08 11:23:27,478] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:18:27.478643+00:00

[2018-11-08 11:23:27,486] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.605 seconds
[2018-11-08 11:24:34,583] {jobs.py:385} INFO - Started process (PID=52513) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:24:39,600] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:24:39,608] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:24:39,609] {logging_mixin.py:95} INFO - [2018-11-08 11:24:39,608] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:24:39,617] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:24:39,672] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:24:39,685] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,718] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,741] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,768] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,791] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,812] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,835] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,860] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,883] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,897] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,919] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,947] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,960] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:39,981] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:40,001] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:40,017] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:24:40,205] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:24:40,225] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:24:40,234] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:24:40,238] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:24:40,243] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:24:40,250] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:24:40,254] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:24:40,261] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:24:40,267] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:24:40,287] {logging_mixin.py:95} INFO - [2018-11-08 11:24:40,286] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:24:40,291] {logging_mixin.py:95} INFO - [2018-11-08 11:24:40,288] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:19:40.288105+00:00

[2018-11-08 11:24:40,304] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.720 seconds
[2018-11-08 11:25:50,137] {jobs.py:385} INFO - Started process (PID=52566) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:25:55,152] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:25:55,159] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:25:55,161] {logging_mixin.py:95} INFO - [2018-11-08 11:25:55,160] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:25:55,172] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:25:55,216] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:25:55,229] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,263] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,279] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,297] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,312] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,332] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,347] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,364] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,386] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,402] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,442] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,460] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,485] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,513] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,547] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,571] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:25:55,775] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:25:55,786] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:25:55,792] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:25:55,797] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:25:55,802] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:25:55,807] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:25:55,811] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:25:55,816] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:25:55,822] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:25:55,833] {logging_mixin.py:95} INFO - [2018-11-08 11:25:55,833] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:25:55,836] {logging_mixin.py:95} INFO - [2018-11-08 11:25:55,835] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:20:55.835047+00:00

[2018-11-08 11:25:55,847] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.710 seconds
[2018-11-08 11:27:36,170] {jobs.py:385} INFO - Started process (PID=52635) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:27:41,186] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:27:41,191] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:27:41,193] {logging_mixin.py:95} INFO - [2018-11-08 11:27:41,192] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:27:41,205] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:27:41,276] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:27:41,306] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,363] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,402] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,428] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,463] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,496] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,535] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,578] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,616] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,663] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,704] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,744] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,778] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,805] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,868] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:41,901] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:27:42,343] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:27:42,366] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:27:42,375] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:27:42,394] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:27:42,405] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:27:42,430] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:27:42,442] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:27:42,465] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:27:42,490] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:27:42,515] {logging_mixin.py:95} INFO - [2018-11-08 11:27:42,514] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:27:42,527] {logging_mixin.py:95} INFO - [2018-11-08 11:27:42,520] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:22:42.517995+00:00

[2018-11-08 11:27:42,551] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.381 seconds
[2018-11-08 11:29:44,773] {jobs.py:385} INFO - Started process (PID=52722) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:29:49,800] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:29:49,846] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:29:49,847] {logging_mixin.py:95} INFO - [2018-11-08 11:29:49,847] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:29:49,863] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:29:49,978] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:29:50,029] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,088] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,126] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,177] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,209] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,246] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,331] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,381] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,418] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,451] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,496] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,526] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,548] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,580] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,605] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,632] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:29:50,987] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:29:51,020] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:29:51,028] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:29:51,048] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:29:51,064] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:29:51,080] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:29:51,089] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:29:51,095] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:29:51,100] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:29:51,138] {logging_mixin.py:95} INFO - [2018-11-08 11:29:51,133] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:29:51,143] {logging_mixin.py:95} INFO - [2018-11-08 11:29:51,139] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:24:51.139041+00:00

[2018-11-08 11:29:51,154] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.382 seconds
[2018-11-08 11:30:56,270] {jobs.py:385} INFO - Started process (PID=52765) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:31:01,285] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:31:01,289] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:31:01,291] {logging_mixin.py:95} INFO - [2018-11-08 11:31:01,291] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:31:01,300] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:31:01,338] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:31:01,353] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,416] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,449] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,488] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,517] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,547] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,565] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,584] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,603] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,619] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,653] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,673] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,699] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,715] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,733] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,754] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:31:01,964] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:31:01,986] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:31:01,993] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:31:02,004] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:31:02,015] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:31:02,021] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:31:02,031] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:31:02,048] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:31:02,055] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:31:02,072] {logging_mixin.py:95} INFO - [2018-11-08 11:31:02,071] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:31:02,076] {logging_mixin.py:95} INFO - [2018-11-08 11:31:02,072] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:26:02.072666+00:00

[2018-11-08 11:31:02,086] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.816 seconds
[2018-11-08 11:32:05,149] {jobs.py:385} INFO - Started process (PID=52821) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:32:10,160] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:32:10,164] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:32:10,167] {logging_mixin.py:95} INFO - [2018-11-08 11:32:10,166] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:32:10,175] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:32:10,216] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:32:10,230] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,258] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,272] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,287] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,300] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,315] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,334] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,350] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,364] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,377] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,395] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,405] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,417] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,434] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,446] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,460] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:32:10,613] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:32:10,624] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:32:10,629] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:32:10,632] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:32:10,636] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:32:10,641] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:32:10,646] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:32:10,650] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:32:10,653] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:32:10,667] {logging_mixin.py:95} INFO - [2018-11-08 11:32:10,666] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:32:10,668] {logging_mixin.py:95} INFO - [2018-11-08 11:32:10,667] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:27:10.667581+00:00

[2018-11-08 11:32:10,678] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.529 seconds
[2018-11-08 11:33:12,644] {jobs.py:385} INFO - Started process (PID=52866) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:33:17,655] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:33:17,659] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:33:17,660] {logging_mixin.py:95} INFO - [2018-11-08 11:33:17,660] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:33:17,671] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:33:17,728] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:33:17,754] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,787] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,805] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,824] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,838] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,866] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,886] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,916] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,935] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,949] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,965] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,980] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:17,993] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:18,007] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:18,019] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:18,035] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:33:18,228] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:33:18,238] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:33:18,244] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:33:18,254] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:33:18,261] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:33:18,264] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:33:18,268] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:33:18,272] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:33:18,278] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:33:18,287] {logging_mixin.py:95} INFO - [2018-11-08 11:33:18,286] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:33:18,288] {logging_mixin.py:95} INFO - [2018-11-08 11:33:18,287] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:28:18.287713+00:00

[2018-11-08 11:33:18,297] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.654 seconds
[2018-11-08 11:33:42,584] {jobs.py:385} INFO - Started process (PID=52904) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:33:47,592] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:33:47,595] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:33:47,596] {logging_mixin.py:95} INFO - [2018-11-08 11:33:47,595] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:33:47,608] {logging_mixin.py:95} INFO - [2018-11-08 11:33:47,604] {models.py:366} ERROR - Failed to import: /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/models.py", line 363, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/anaconda3/envs/etlv2/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py", line 8, in <module>
    from airflow.hooks.postgres_hook import PostgresHook
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/hooks/postgres_hook.py", line 21, in <module>
    import psycopg2
ModuleNotFoundError: No module named 'psycopg2'

[2018-11-08 11:33:47,608] {jobs.py:1796} WARNING - No viable dags retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:33:47,623] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.039 seconds
[2018-11-08 11:33:48,749] {jobs.py:385} INFO - Started process (PID=52914) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:33:53,753] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:33:53,755] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:33:53,757] {logging_mixin.py:95} INFO - [2018-11-08 11:33:53,756] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:33:53,783] {logging_mixin.py:95} INFO - [2018-11-08 11:33:53,776] {models.py:366} ERROR - Failed to import: /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/models.py", line 363, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/anaconda3/envs/etlv2/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py", line 8, in <module>
    from airflow.hooks.postgres_hook import PostgresHook
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/hooks/postgres_hook.py", line 21, in <module>
    import psycopg2
ModuleNotFoundError: No module named 'psycopg2'

[2018-11-08 11:33:53,783] {jobs.py:1796} WARNING - No viable dags retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:33:53,808] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.059 seconds
[2018-11-08 11:33:54,921] {jobs.py:385} INFO - Started process (PID=52916) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:33:59,927] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:33:59,930] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:33:59,933] {logging_mixin.py:95} INFO - [2018-11-08 11:33:59,932] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:33:59,946] {logging_mixin.py:95} INFO - [2018-11-08 11:33:59,942] {models.py:366} ERROR - Failed to import: /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/models.py", line 363, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/anaconda3/envs/etlv2/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py", line 8, in <module>
    from airflow.hooks.postgres_hook import PostgresHook
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/hooks/postgres_hook.py", line 21, in <module>
    import psycopg2
ModuleNotFoundError: No module named 'psycopg2'

[2018-11-08 11:33:59,946] {jobs.py:1796} WARNING - No viable dags retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:33:59,960] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.039 seconds
[2018-11-08 11:34:01,080] {jobs.py:385} INFO - Started process (PID=52918) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:34:06,088] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:34:06,091] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:34:06,092] {logging_mixin.py:95} INFO - [2018-11-08 11:34:06,092] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:34:06,108] {logging_mixin.py:95} INFO - [2018-11-08 11:34:06,104] {models.py:366} ERROR - Failed to import: /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/models.py", line 363, in process_file
    m = imp.load_source(mod_name, filepath)
  File "/anaconda3/envs/etlv2/lib/python3.6/imp.py", line 172, in load_source
    module = _load(spec)
  File "<frozen importlib._bootstrap>", line 684, in _load
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py", line 8, in <module>
    from airflow.hooks.postgres_hook import PostgresHook
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/hooks/postgres_hook.py", line 21, in <module>
    import psycopg2
ModuleNotFoundError: No module named 'psycopg2'

[2018-11-08 11:34:06,109] {jobs.py:1796} WARNING - No viable dags retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:34:06,128] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.048 seconds
[2018-11-08 11:34:07,273] {jobs.py:385} INFO - Started process (PID=52922) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:34:12,286] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:34:12,289] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:34:12,291] {logging_mixin.py:95} INFO - [2018-11-08 11:34:12,290] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:34:12,316] {logging_mixin.py:95} WARNING - /anaconda3/envs/etlv2/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
  """)

[2018-11-08 11:34:13,594] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:34:13,640] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:34:13,656] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,681] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,698] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,718] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,735] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,754] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,770] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,790] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,809] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,825] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,842] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,874] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,896] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,919] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,936] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:13,951] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:14,121] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:34:14,131] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,136] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,140] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,144] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,148] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,153] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,157] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 09:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,161] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 10:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,165] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 11:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,169] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 12:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,173] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 13:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,177] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 14:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,180] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 15:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,185] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 16:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,188] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 17:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,193] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 18:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:34:14,204] {logging_mixin.py:95} INFO - [2018-11-08 11:34:14,204] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:34:14,205] {logging_mixin.py:95} INFO - [2018-11-08 11:34:14,205] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:29:14.205167+00:00

[2018-11-08 11:34:14,210] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.937 seconds
[2018-11-08 11:34:15,304] {jobs.py:385} INFO - Started process (PID=52927) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:34:20,311] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:34:20,321] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:34:20,323] {logging_mixin.py:95} INFO - [2018-11-08 11:34:20,322] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:34:20,345] {logging_mixin.py:95} WARNING - /anaconda3/envs/etlv2/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
  """)

[2018-11-08 11:34:20,837] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:34:20,868] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:34:20,878] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:20,902] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:20,924] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:20,946] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:20,965] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:20,983] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:21,006] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:21,022] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:21,039] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:21,053] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:21,066] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:21,080] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:21,092] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:21,106] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:21,119] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:21,133] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:34:21,255] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:34:21,263] {logging_mixin.py:95} INFO - [2018-11-08 11:34:21,262] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:34:21,265] {logging_mixin.py:95} INFO - [2018-11-08 11:34:21,263] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:29:21.263773+00:00

[2018-11-08 11:34:21,269] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.965 seconds
[2018-11-08 11:36:14,550] {jobs.py:385} INFO - Started process (PID=53031) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:36:19,558] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:36:19,563] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:36:19,564] {logging_mixin.py:95} INFO - [2018-11-08 11:36:19,564] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:36:19,572] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:36:19,620] {jobs.py:1420} INFO - Processing extract_dag
[2018-11-08 11:36:19,635] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 03:00:00+00:00: scheduled__2018-10-31T03:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,665] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 04:00:00+00:00: scheduled__2018-10-31T04:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,689] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 05:00:00+00:00: scheduled__2018-10-31T05:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,724] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 06:00:00+00:00: scheduled__2018-10-31T06:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,759] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 07:00:00+00:00: scheduled__2018-10-31T07:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,787] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 08:00:00+00:00: scheduled__2018-10-31T08:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,811] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 09:00:00+00:00: scheduled__2018-10-31T09:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,832] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 10:00:00+00:00: scheduled__2018-10-31T10:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,850] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 11:00:00+00:00: scheduled__2018-10-31T11:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,871] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 12:00:00+00:00: scheduled__2018-10-31T12:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,888] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 13:00:00+00:00: scheduled__2018-10-31T13:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,906] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 14:00:00+00:00: scheduled__2018-10-31T14:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,922] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 15:00:00+00:00: scheduled__2018-10-31T15:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,938] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 16:00:00+00:00: scheduled__2018-10-31T16:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,954] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 17:00:00+00:00: scheduled__2018-10-31T17:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:19,972] {jobs.py:928} INFO - Examining DAG run <DagRun extract_dag @ 2018-10-31 18:00:00+00:00: scheduled__2018-10-31T18:00:00+00:00, externally triggered: False>
[2018-11-08 11:36:20,135] {jobs.py:615} INFO - Skipping SLA check for <DAG: extract_dag> because no tasks in DAG have SLAs
[2018-11-08 11:36:20,144] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 03:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:36:20,151] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 04:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:36:20,155] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 05:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:36:20,158] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 06:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:36:20,162] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 07:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:36:20,167] {jobs.py:1859} INFO - Creating / updating <TaskInstance: extract_dag.extract 2018-10-31 08:00:00+00:00 [scheduled]> in ORM
[2018-11-08 11:36:20,177] {logging_mixin.py:95} INFO - [2018-11-08 11:36:20,176] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:36:20,178] {logging_mixin.py:95} INFO - [2018-11-08 11:36:20,177] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:31:20.177637+00:00

[2018-11-08 11:36:20,185] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.635 seconds
[2018-11-08 11:37:03,390] {jobs.py:385} INFO - Started process (PID=53062) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:37:08,397] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:37:08,400] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:37:08,401] {logging_mixin.py:95} INFO - [2018-11-08 11:37:08,401] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:37:08,410] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:37:08,462] {logging_mixin.py:95} INFO - [2018-11-08 11:37:08,461] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:37:08,466] {logging_mixin.py:95} INFO - [2018-11-08 11:37:08,463] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:32:08.463028+00:00

[2018-11-08 11:37:08,473] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.083 seconds
[2018-11-08 11:37:09,585] {jobs.py:385} INFO - Started process (PID=53064) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:37:14,597] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:37:14,599] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:37:14,600] {logging_mixin.py:95} INFO - [2018-11-08 11:37:14,599] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:37:14,609] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:37:14,651] {logging_mixin.py:95} INFO - [2018-11-08 11:37:14,650] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:37:14,655] {logging_mixin.py:95} INFO - [2018-11-08 11:37:14,652] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:32:14.652575+00:00

[2018-11-08 11:37:14,667] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.084 seconds
[2018-11-08 11:37:15,757] {jobs.py:385} INFO - Started process (PID=53066) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:37:20,766] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:37:20,769] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:37:20,770] {logging_mixin.py:95} INFO - [2018-11-08 11:37:20,769] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:37:20,780] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:37:20,828] {logging_mixin.py:95} INFO - [2018-11-08 11:37:20,827] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:37:20,833] {logging_mixin.py:95} INFO - [2018-11-08 11:37:20,829] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:32:20.829453+00:00

[2018-11-08 11:37:20,843] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.085 seconds
[2018-11-08 11:55:12,607] {jobs.py:385} INFO - Started process (PID=53548) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:17,630] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:55:17,632] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:55:17,634] {logging_mixin.py:95} INFO - [2018-11-08 11:55:17,633] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:55:18,367] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:18,408] {logging_mixin.py:95} INFO - [2018-11-08 11:55:18,407] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:55:18,410] {logging_mixin.py:95} INFO - [2018-11-08 11:55:18,408] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:50:18.408939+00:00

[2018-11-08 11:55:18,417] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.810 seconds
[2018-11-08 11:55:19,484] {jobs.py:385} INFO - Started process (PID=53554) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:24,490] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:55:24,493] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:55:24,494] {logging_mixin.py:95} INFO - [2018-11-08 11:55:24,494] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:55:25,047] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:25,074] {logging_mixin.py:95} INFO - [2018-11-08 11:55:25,074] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:55:25,077] {logging_mixin.py:95} INFO - [2018-11-08 11:55:25,075] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:50:25.075906+00:00

[2018-11-08 11:55:25,084] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.600 seconds
[2018-11-08 11:55:26,166] {jobs.py:385} INFO - Started process (PID=53561) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:31,176] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:55:31,180] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:55:31,181] {logging_mixin.py:95} INFO - [2018-11-08 11:55:31,181] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:55:31,806] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:31,839] {logging_mixin.py:95} INFO - [2018-11-08 11:55:31,838] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:55:31,842] {logging_mixin.py:95} INFO - [2018-11-08 11:55:31,841] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:50:31.841494+00:00

[2018-11-08 11:55:31,851] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.686 seconds
[2018-11-08 11:55:32,970] {jobs.py:385} INFO - Started process (PID=53569) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:37,982] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:55:37,984] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:55:37,985] {logging_mixin.py:95} INFO - [2018-11-08 11:55:37,984] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:55:38,947] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:39,130] {logging_mixin.py:95} INFO - [2018-11-08 11:55:39,129] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:55:39,184] {logging_mixin.py:95} INFO - [2018-11-08 11:55:39,160] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:50:39.160643+00:00

[2018-11-08 11:55:39,230] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.260 seconds
[2018-11-08 11:55:40,319] {jobs.py:385} INFO - Started process (PID=53575) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:45,326] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:55:45,337] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:55:45,343] {logging_mixin.py:95} INFO - [2018-11-08 11:55:45,340] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:55:47,305] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:47,361] {logging_mixin.py:95} INFO - [2018-11-08 11:55:47,358] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:55:47,364] {logging_mixin.py:95} INFO - [2018-11-08 11:55:47,362] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:50:47.361966+00:00

[2018-11-08 11:55:47,373] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.053 seconds
[2018-11-08 11:55:48,514] {jobs.py:385} INFO - Started process (PID=53580) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:53,524] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:55:53,526] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:55:53,527] {logging_mixin.py:95} INFO - [2018-11-08 11:55:53,527] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:55:54,042] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:55:54,068] {logging_mixin.py:95} INFO - [2018-11-08 11:55:54,067] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:55:54,070] {logging_mixin.py:95} INFO - [2018-11-08 11:55:54,068] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:50:54.068856+00:00

[2018-11-08 11:55:54,076] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.562 seconds
[2018-11-08 11:55:55,236] {jobs.py:385} INFO - Started process (PID=53582) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:00,248] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:56:00,253] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:56:00,255] {logging_mixin.py:95} INFO - [2018-11-08 11:56:00,254] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:56:01,290] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:01,318] {logging_mixin.py:95} INFO - [2018-11-08 11:56:01,318] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:56:01,322] {logging_mixin.py:95} INFO - [2018-11-08 11:56:01,320] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:51:01.319995+00:00

[2018-11-08 11:56:01,330] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.095 seconds
[2018-11-08 11:56:02,418] {jobs.py:385} INFO - Started process (PID=53584) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:07,428] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:56:07,431] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:56:07,434] {logging_mixin.py:95} INFO - [2018-11-08 11:56:07,433] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:56:10,135] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:10,260] {logging_mixin.py:95} INFO - [2018-11-08 11:56:10,258] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:56:10,263] {logging_mixin.py:95} INFO - [2018-11-08 11:56:10,261] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:51:10.261455+00:00

[2018-11-08 11:56:10,274] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.855 seconds
[2018-11-08 11:56:11,338] {jobs.py:385} INFO - Started process (PID=53585) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:16,346] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:56:16,351] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:56:16,353] {logging_mixin.py:95} INFO - [2018-11-08 11:56:16,352] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:56:17,122] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:17,166] {logging_mixin.py:95} INFO - [2018-11-08 11:56:17,165] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:56:17,169] {logging_mixin.py:95} INFO - [2018-11-08 11:56:17,166] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:51:17.166947+00:00

[2018-11-08 11:56:17,179] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.840 seconds
[2018-11-08 11:56:18,248] {jobs.py:385} INFO - Started process (PID=53593) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:23,264] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:56:23,266] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:56:23,269] {logging_mixin.py:95} INFO - [2018-11-08 11:56:23,268] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:56:23,970] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:23,997] {logging_mixin.py:95} INFO - [2018-11-08 11:56:23,996] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:56:23,999] {logging_mixin.py:95} INFO - [2018-11-08 11:56:23,997] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:51:23.997659+00:00

[2018-11-08 11:56:24,009] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.761 seconds
[2018-11-08 11:56:25,079] {jobs.py:385} INFO - Started process (PID=53602) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:30,088] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:56:30,090] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:56:30,092] {logging_mixin.py:95} INFO - [2018-11-08 11:56:30,091] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:56:30,678] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:30,715] {logging_mixin.py:95} INFO - [2018-11-08 11:56:30,714] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:56:30,718] {logging_mixin.py:95} INFO - [2018-11-08 11:56:30,716] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:51:30.716593+00:00

[2018-11-08 11:56:30,726] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.647 seconds
[2018-11-08 11:56:31,870] {jobs.py:385} INFO - Started process (PID=53604) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:36,882] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:56:36,884] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:56:36,885] {logging_mixin.py:95} INFO - [2018-11-08 11:56:36,885] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:56:37,492] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:37,525] {logging_mixin.py:95} INFO - [2018-11-08 11:56:37,524] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:56:37,527] {logging_mixin.py:95} INFO - [2018-11-08 11:56:37,526] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:51:37.526132+00:00

[2018-11-08 11:56:37,535] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.665 seconds
[2018-11-08 11:56:38,657] {jobs.py:385} INFO - Started process (PID=53605) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:43,671] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:56:43,674] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:56:43,676] {logging_mixin.py:95} INFO - [2018-11-08 11:56:43,675] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:56:44,379] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:44,433] {logging_mixin.py:95} INFO - [2018-11-08 11:56:44,432] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:56:44,437] {logging_mixin.py:95} INFO - [2018-11-08 11:56:44,434] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:51:44.434143+00:00

[2018-11-08 11:56:44,455] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.798 seconds
[2018-11-08 11:56:45,575] {jobs.py:385} INFO - Started process (PID=53610) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:50,582] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:56:50,585] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:56:50,586] {logging_mixin.py:95} INFO - [2018-11-08 11:56:50,586] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:56:53,494] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:53,727] {logging_mixin.py:95} INFO - [2018-11-08 11:56:53,726] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:56:53,741] {logging_mixin.py:95} INFO - [2018-11-08 11:56:53,728] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:51:53.728276+00:00

[2018-11-08 11:56:53,780] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 8.205 seconds
[2018-11-08 11:56:54,915] {jobs.py:385} INFO - Started process (PID=53613) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:56:59,925] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:56:59,927] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:56:59,929] {logging_mixin.py:95} INFO - [2018-11-08 11:56:59,928] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:57:00,536] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:00,561] {logging_mixin.py:95} INFO - [2018-11-08 11:57:00,561] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:57:00,563] {logging_mixin.py:95} INFO - [2018-11-08 11:57:00,562] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:52:00.562422+00:00

[2018-11-08 11:57:00,572] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.656 seconds
[2018-11-08 11:57:01,671] {jobs.py:385} INFO - Started process (PID=53614) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:06,680] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:57:06,684] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:57:06,688] {logging_mixin.py:95} INFO - [2018-11-08 11:57:06,687] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:57:08,819] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:08,879] {logging_mixin.py:95} INFO - [2018-11-08 11:57:08,877] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:57:08,881] {logging_mixin.py:95} INFO - [2018-11-08 11:57:08,880] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:52:08.879990+00:00

[2018-11-08 11:57:08,891] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.220 seconds
[2018-11-08 11:57:10,004] {jobs.py:385} INFO - Started process (PID=53616) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:15,009] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:57:15,012] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:57:15,014] {logging_mixin.py:95} INFO - [2018-11-08 11:57:15,013] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:57:15,817] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:15,854] {logging_mixin.py:95} INFO - [2018-11-08 11:57:15,854] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:57:15,859] {logging_mixin.py:95} INFO - [2018-11-08 11:57:15,855] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:52:15.855850+00:00

[2018-11-08 11:57:15,878] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.875 seconds
[2018-11-08 11:57:17,024] {jobs.py:385} INFO - Started process (PID=53618) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:22,035] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:57:22,039] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:57:22,041] {logging_mixin.py:95} INFO - [2018-11-08 11:57:22,040] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:57:23,048] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:23,113] {logging_mixin.py:95} INFO - [2018-11-08 11:57:23,113] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:57:23,117] {logging_mixin.py:95} INFO - [2018-11-08 11:57:23,114] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:52:23.114553+00:00

[2018-11-08 11:57:23,129] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.105 seconds
[2018-11-08 11:57:24,245] {jobs.py:385} INFO - Started process (PID=53619) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:29,253] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:57:29,255] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:57:29,256] {logging_mixin.py:95} INFO - [2018-11-08 11:57:29,256] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:57:29,918] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:29,961] {logging_mixin.py:95} INFO - [2018-11-08 11:57:29,960] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:57:29,963] {logging_mixin.py:95} INFO - [2018-11-08 11:57:29,961] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:52:29.961568+00:00

[2018-11-08 11:57:29,971] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.726 seconds
[2018-11-08 11:57:31,053] {jobs.py:385} INFO - Started process (PID=53622) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:36,059] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:57:36,064] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:57:36,066] {logging_mixin.py:95} INFO - [2018-11-08 11:57:36,065] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:57:36,648] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:36,701] {logging_mixin.py:95} INFO - [2018-11-08 11:57:36,700] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:57:36,703] {logging_mixin.py:95} INFO - [2018-11-08 11:57:36,702] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:52:36.702151+00:00

[2018-11-08 11:57:36,722] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.670 seconds
[2018-11-08 11:57:37,880] {jobs.py:385} INFO - Started process (PID=53623) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:42,888] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:57:42,890] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:57:42,891] {logging_mixin.py:95} INFO - [2018-11-08 11:57:42,890] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:57:43,799] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:43,856] {logging_mixin.py:95} INFO - [2018-11-08 11:57:43,855] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:57:43,860] {logging_mixin.py:95} INFO - [2018-11-08 11:57:43,857] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:52:43.857015+00:00

[2018-11-08 11:57:43,869] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.989 seconds
[2018-11-08 11:57:44,972] {jobs.py:385} INFO - Started process (PID=53628) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:49,983] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:57:49,987] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:57:49,989] {logging_mixin.py:95} INFO - [2018-11-08 11:57:49,988] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:57:50,834] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:50,871] {logging_mixin.py:95} INFO - [2018-11-08 11:57:50,870] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:57:50,872] {logging_mixin.py:95} INFO - [2018-11-08 11:57:50,871] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:52:50.871805+00:00

[2018-11-08 11:57:50,879] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.907 seconds
[2018-11-08 11:57:51,963] {jobs.py:385} INFO - Started process (PID=53630) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:56,972] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:57:56,975] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:57:56,977] {logging_mixin.py:95} INFO - [2018-11-08 11:57:56,977] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:57:57,932] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:57:57,961] {logging_mixin.py:95} INFO - [2018-11-08 11:57:57,960] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:57:57,964] {logging_mixin.py:95} INFO - [2018-11-08 11:57:57,962] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:52:57.961974+00:00

[2018-11-08 11:57:57,973] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.010 seconds
[2018-11-08 11:57:59,110] {jobs.py:385} INFO - Started process (PID=53631) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:04,118] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:58:04,120] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:58:04,122] {logging_mixin.py:95} INFO - [2018-11-08 11:58:04,121] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:58:06,354] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:06,382] {logging_mixin.py:95} INFO - [2018-11-08 11:58:06,382] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:58:06,384] {logging_mixin.py:95} INFO - [2018-11-08 11:58:06,383] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:53:06.383271+00:00

[2018-11-08 11:58:06,390] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.281 seconds
[2018-11-08 11:58:07,455] {jobs.py:385} INFO - Started process (PID=53634) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:12,462] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:58:12,464] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:58:12,465] {logging_mixin.py:95} INFO - [2018-11-08 11:58:12,464] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:58:13,051] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:13,080] {logging_mixin.py:95} INFO - [2018-11-08 11:58:13,080] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:58:13,082] {logging_mixin.py:95} INFO - [2018-11-08 11:58:13,080] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:53:13.080966+00:00

[2018-11-08 11:58:13,090] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.635 seconds
[2018-11-08 11:58:14,157] {jobs.py:385} INFO - Started process (PID=53635) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:19,166] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:58:19,170] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:58:19,172] {logging_mixin.py:95} INFO - [2018-11-08 11:58:19,171] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:58:19,808] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:19,868] {logging_mixin.py:95} INFO - [2018-11-08 11:58:19,867] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:58:19,869] {logging_mixin.py:95} INFO - [2018-11-08 11:58:19,868] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:53:19.868695+00:00

[2018-11-08 11:58:19,875] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.718 seconds
[2018-11-08 11:58:20,979] {jobs.py:385} INFO - Started process (PID=53637) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:25,992] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:58:25,995] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:58:25,996] {logging_mixin.py:95} INFO - [2018-11-08 11:58:25,996] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:58:26,552] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:26,600] {logging_mixin.py:95} INFO - [2018-11-08 11:58:26,600] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:58:26,602] {logging_mixin.py:95} INFO - [2018-11-08 11:58:26,601] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:53:26.601235+00:00

[2018-11-08 11:58:26,613] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.634 seconds
[2018-11-08 11:58:27,752] {jobs.py:385} INFO - Started process (PID=53639) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:32,759] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:58:32,763] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:58:32,767] {logging_mixin.py:95} INFO - [2018-11-08 11:58:32,765] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:58:33,332] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:33,357] {logging_mixin.py:95} INFO - [2018-11-08 11:58:33,356] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:58:33,359] {logging_mixin.py:95} INFO - [2018-11-08 11:58:33,358] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:53:33.358186+00:00

[2018-11-08 11:58:33,365] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.613 seconds
[2018-11-08 11:58:34,432] {jobs.py:385} INFO - Started process (PID=53640) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:39,441] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:58:39,446] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:58:39,448] {logging_mixin.py:95} INFO - [2018-11-08 11:58:39,447] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:58:40,441] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:40,491] {logging_mixin.py:95} INFO - [2018-11-08 11:58:40,490] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:58:40,493] {logging_mixin.py:95} INFO - [2018-11-08 11:58:40,491] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:53:40.491824+00:00

[2018-11-08 11:58:40,508] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.077 seconds
[2018-11-08 11:58:41,673] {jobs.py:385} INFO - Started process (PID=53659) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:46,681] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:58:46,683] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:58:46,686] {logging_mixin.py:95} INFO - [2018-11-08 11:58:46,685] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:58:47,398] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:47,421] {logging_mixin.py:95} INFO - [2018-11-08 11:58:47,421] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:58:47,423] {logging_mixin.py:95} INFO - [2018-11-08 11:58:47,422] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:53:47.422196+00:00

[2018-11-08 11:58:47,429] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.757 seconds
[2018-11-08 11:58:48,480] {jobs.py:385} INFO - Started process (PID=53672) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:53,486] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:58:53,489] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:58:53,491] {logging_mixin.py:95} INFO - [2018-11-08 11:58:53,490] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:58:54,156] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:58:54,176] {logging_mixin.py:95} INFO - [2018-11-08 11:58:54,176] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:58:54,178] {logging_mixin.py:95} INFO - [2018-11-08 11:58:54,177] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:53:54.177232+00:00

[2018-11-08 11:58:54,184] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.704 seconds
[2018-11-08 11:58:55,262] {jobs.py:385} INFO - Started process (PID=53676) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:00,269] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:59:00,271] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:59:00,272] {logging_mixin.py:95} INFO - [2018-11-08 11:59:00,271] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:59:00,692] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:00,715] {logging_mixin.py:95} INFO - [2018-11-08 11:59:00,714] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:59:00,717] {logging_mixin.py:95} INFO - [2018-11-08 11:59:00,716] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:54:00.716389+00:00

[2018-11-08 11:59:00,724] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.462 seconds
[2018-11-08 11:59:01,847] {jobs.py:385} INFO - Started process (PID=53678) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:06,855] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:59:06,858] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:59:06,859] {logging_mixin.py:95} INFO - [2018-11-08 11:59:06,858] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:59:07,326] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:07,347] {logging_mixin.py:95} INFO - [2018-11-08 11:59:07,347] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:59:07,349] {logging_mixin.py:95} INFO - [2018-11-08 11:59:07,348] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:54:07.348259+00:00

[2018-11-08 11:59:07,356] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.509 seconds
[2018-11-08 11:59:08,454] {jobs.py:385} INFO - Started process (PID=53679) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:13,459] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:59:13,462] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:59:13,464] {logging_mixin.py:95} INFO - [2018-11-08 11:59:13,464] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:59:13,897] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:13,919] {logging_mixin.py:95} INFO - [2018-11-08 11:59:13,918] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:59:13,921] {logging_mixin.py:95} INFO - [2018-11-08 11:59:13,919] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:54:13.919532+00:00

[2018-11-08 11:59:13,926] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.472 seconds
[2018-11-08 11:59:15,032] {jobs.py:385} INFO - Started process (PID=53682) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:20,037] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:59:20,048] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:59:20,050] {logging_mixin.py:95} INFO - [2018-11-08 11:59:20,050] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:59:20,623] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:20,654] {logging_mixin.py:95} INFO - [2018-11-08 11:59:20,652] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:59:20,657] {logging_mixin.py:95} INFO - [2018-11-08 11:59:20,655] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:54:20.655840+00:00

[2018-11-08 11:59:20,667] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.635 seconds
[2018-11-08 11:59:21,736] {jobs.py:385} INFO - Started process (PID=53683) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:26,749] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:59:26,752] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:59:26,753] {logging_mixin.py:95} INFO - [2018-11-08 11:59:26,753] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:59:27,147] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:27,170] {logging_mixin.py:95} INFO - [2018-11-08 11:59:27,170] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:59:27,172] {logging_mixin.py:95} INFO - [2018-11-08 11:59:27,171] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:54:27.171190+00:00

[2018-11-08 11:59:27,179] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.443 seconds
[2018-11-08 11:59:28,251] {jobs.py:385} INFO - Started process (PID=53685) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:33,260] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:59:33,262] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:59:33,264] {logging_mixin.py:95} INFO - [2018-11-08 11:59:33,263] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:59:33,744] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:33,767] {logging_mixin.py:95} INFO - [2018-11-08 11:59:33,766] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:59:33,768] {logging_mixin.py:95} INFO - [2018-11-08 11:59:33,767] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:54:33.767684+00:00

[2018-11-08 11:59:33,773] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.522 seconds
[2018-11-08 11:59:34,867] {jobs.py:385} INFO - Started process (PID=53686) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:39,876] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:59:39,879] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:59:39,880] {logging_mixin.py:95} INFO - [2018-11-08 11:59:39,880] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:59:40,541] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:40,583] {logging_mixin.py:95} INFO - [2018-11-08 11:59:40,583] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:59:40,585] {logging_mixin.py:95} INFO - [2018-11-08 11:59:40,584] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:54:40.584389+00:00

[2018-11-08 11:59:40,596] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.730 seconds
[2018-11-08 11:59:41,699] {jobs.py:385} INFO - Started process (PID=53689) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:46,746] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:59:46,750] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:59:46,753] {logging_mixin.py:95} INFO - [2018-11-08 11:59:46,752] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:59:47,497] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:47,520] {logging_mixin.py:95} INFO - [2018-11-08 11:59:47,519] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:59:47,521] {logging_mixin.py:95} INFO - [2018-11-08 11:59:47,520] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:54:47.520472+00:00

[2018-11-08 11:59:47,526] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.827 seconds
[2018-11-08 11:59:48,659] {jobs.py:385} INFO - Started process (PID=53693) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:53,670] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 11:59:53,672] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 11:59:53,673] {logging_mixin.py:95} INFO - [2018-11-08 11:59:53,672] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 11:59:54,306] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 11:59:54,341] {logging_mixin.py:95} INFO - [2018-11-08 11:59:54,340] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 11:59:54,344] {logging_mixin.py:95} INFO - [2018-11-08 11:59:54,342] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:54:54.342350+00:00

[2018-11-08 11:59:54,352] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.693 seconds
[2018-11-08 11:59:55,476] {jobs.py:385} INFO - Started process (PID=53695) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:00,484] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:00:00,487] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:00:00,490] {logging_mixin.py:95} INFO - [2018-11-08 12:00:00,488] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:00:01,095] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:01,125] {logging_mixin.py:95} INFO - [2018-11-08 12:00:01,125] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:00:01,127] {logging_mixin.py:95} INFO - [2018-11-08 12:00:01,126] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:55:01.125972+00:00

[2018-11-08 12:00:01,132] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.656 seconds
[2018-11-08 12:00:02,262] {jobs.py:385} INFO - Started process (PID=53698) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:07,269] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:00:07,276] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:00:07,277] {logging_mixin.py:95} INFO - [2018-11-08 12:00:07,277] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:00:08,050] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:08,085] {logging_mixin.py:95} INFO - [2018-11-08 12:00:08,084] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:00:08,088] {logging_mixin.py:95} INFO - [2018-11-08 12:00:08,086] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:55:08.086035+00:00

[2018-11-08 12:00:08,095] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.833 seconds
[2018-11-08 12:00:09,161] {jobs.py:385} INFO - Started process (PID=53699) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:14,177] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:00:14,180] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:00:14,181] {logging_mixin.py:95} INFO - [2018-11-08 12:00:14,181] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:00:14,729] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:14,752] {logging_mixin.py:95} INFO - [2018-11-08 12:00:14,752] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:00:14,754] {logging_mixin.py:95} INFO - [2018-11-08 12:00:14,753] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:55:14.753039+00:00

[2018-11-08 12:00:14,760] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.599 seconds
[2018-11-08 12:00:15,863] {jobs.py:385} INFO - Started process (PID=53702) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:20,868] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:00:20,877] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:00:20,879] {logging_mixin.py:95} INFO - [2018-11-08 12:00:20,878] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:00:21,354] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:21,400] {logging_mixin.py:95} INFO - [2018-11-08 12:00:21,400] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:00:21,403] {logging_mixin.py:95} INFO - [2018-11-08 12:00:21,401] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:55:21.401757+00:00

[2018-11-08 12:00:21,412] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.549 seconds
[2018-11-08 12:00:22,460] {jobs.py:385} INFO - Started process (PID=53703) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:27,469] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:00:27,475] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:00:27,478] {logging_mixin.py:95} INFO - [2018-11-08 12:00:27,477] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:00:28,628] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:28,654] {logging_mixin.py:95} INFO - [2018-11-08 12:00:28,654] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:00:28,663] {logging_mixin.py:95} INFO - [2018-11-08 12:00:28,658] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:55:28.658747+00:00

[2018-11-08 12:00:28,682] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.223 seconds
[2018-11-08 12:00:29,789] {jobs.py:385} INFO - Started process (PID=53705) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:34,796] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:00:34,798] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:00:34,800] {logging_mixin.py:95} INFO - [2018-11-08 12:00:34,799] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:00:35,329] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:35,354] {logging_mixin.py:95} INFO - [2018-11-08 12:00:35,354] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:00:35,356] {logging_mixin.py:95} INFO - [2018-11-08 12:00:35,355] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:55:35.355270+00:00

[2018-11-08 12:00:35,362] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.573 seconds
[2018-11-08 12:00:36,476] {jobs.py:385} INFO - Started process (PID=53706) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:41,481] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:00:41,484] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:00:41,485] {logging_mixin.py:95} INFO - [2018-11-08 12:00:41,485] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:00:42,018] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:42,042] {logging_mixin.py:95} INFO - [2018-11-08 12:00:42,041] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:00:42,044] {logging_mixin.py:95} INFO - [2018-11-08 12:00:42,042] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:55:42.042923+00:00

[2018-11-08 12:00:42,049] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.573 seconds
[2018-11-08 12:00:43,173] {jobs.py:385} INFO - Started process (PID=53711) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:48,182] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:00:48,184] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:00:48,186] {logging_mixin.py:95} INFO - [2018-11-08 12:00:48,185] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:00:48,713] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:48,735] {logging_mixin.py:95} INFO - [2018-11-08 12:00:48,735] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:00:48,737] {logging_mixin.py:95} INFO - [2018-11-08 12:00:48,736] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:55:48.736551+00:00

[2018-11-08 12:00:48,746] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.573 seconds
[2018-11-08 12:00:49,855] {jobs.py:385} INFO - Started process (PID=53713) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:54,870] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:00:54,878] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:00:54,882] {logging_mixin.py:95} INFO - [2018-11-08 12:00:54,881] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:00:55,355] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:00:55,381] {logging_mixin.py:95} INFO - [2018-11-08 12:00:55,380] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:00:55,382] {logging_mixin.py:95} INFO - [2018-11-08 12:00:55,381] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:55:55.381942+00:00

[2018-11-08 12:00:55,388] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.532 seconds
[2018-11-08 12:00:56,441] {jobs.py:385} INFO - Started process (PID=53720) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:01,446] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:01:01,451] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:01:01,452] {logging_mixin.py:95} INFO - [2018-11-08 12:01:01,452] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:01:02,070] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:02,095] {logging_mixin.py:95} INFO - [2018-11-08 12:01:02,094] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:01:02,096] {logging_mixin.py:95} INFO - [2018-11-08 12:01:02,095] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:56:02.095598+00:00

[2018-11-08 12:01:02,102] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.662 seconds
[2018-11-08 12:01:03,182] {jobs.py:385} INFO - Started process (PID=53722) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:08,186] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:01:08,188] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:01:08,189] {logging_mixin.py:95} INFO - [2018-11-08 12:01:08,188] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:01:08,671] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:08,694] {logging_mixin.py:95} INFO - [2018-11-08 12:01:08,693] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:01:08,696] {logging_mixin.py:95} INFO - [2018-11-08 12:01:08,694] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:56:08.694739+00:00

[2018-11-08 12:01:08,700] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.518 seconds
[2018-11-08 12:01:09,769] {jobs.py:385} INFO - Started process (PID=53723) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:14,778] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:01:14,782] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:01:14,783] {logging_mixin.py:95} INFO - [2018-11-08 12:01:14,783] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:01:15,272] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:15,293] {logging_mixin.py:95} INFO - [2018-11-08 12:01:15,293] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:01:15,295] {logging_mixin.py:95} INFO - [2018-11-08 12:01:15,294] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:56:15.294307+00:00

[2018-11-08 12:01:15,300] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.531 seconds
[2018-11-08 12:01:16,365] {jobs.py:385} INFO - Started process (PID=53725) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:21,370] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:01:21,375] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:01:21,379] {logging_mixin.py:95} INFO - [2018-11-08 12:01:21,378] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:01:22,687] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:22,725] {logging_mixin.py:95} INFO - [2018-11-08 12:01:22,724] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:01:22,727] {logging_mixin.py:95} INFO - [2018-11-08 12:01:22,725] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:56:22.725564+00:00

[2018-11-08 12:01:22,737] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.372 seconds
[2018-11-08 12:01:23,882] {jobs.py:385} INFO - Started process (PID=53727) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:28,895] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:01:28,898] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:01:28,900] {logging_mixin.py:95} INFO - [2018-11-08 12:01:28,899] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:01:29,306] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:29,333] {logging_mixin.py:95} INFO - [2018-11-08 12:01:29,333] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:01:29,335] {logging_mixin.py:95} INFO - [2018-11-08 12:01:29,334] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:56:29.334075+00:00

[2018-11-08 12:01:29,341] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.459 seconds
[2018-11-08 12:01:30,439] {jobs.py:385} INFO - Started process (PID=53730) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:35,454] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:01:35,465] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:01:35,467] {logging_mixin.py:95} INFO - [2018-11-08 12:01:35,466] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:01:36,370] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:36,433] {logging_mixin.py:95} INFO - [2018-11-08 12:01:36,433] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:01:36,436] {logging_mixin.py:95} INFO - [2018-11-08 12:01:36,434] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:56:36.434741+00:00

[2018-11-08 12:01:36,452] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.014 seconds
[2018-11-08 12:01:37,552] {jobs.py:385} INFO - Started process (PID=53734) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:42,560] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:01:42,563] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:01:42,565] {logging_mixin.py:95} INFO - [2018-11-08 12:01:42,564] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:01:43,271] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:43,308] {logging_mixin.py:95} INFO - [2018-11-08 12:01:43,308] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:01:43,310] {logging_mixin.py:95} INFO - [2018-11-08 12:01:43,309] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:56:43.309268+00:00

[2018-11-08 12:01:43,318] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.766 seconds
[2018-11-08 12:01:44,434] {jobs.py:385} INFO - Started process (PID=53757) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:49,440] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:01:49,443] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:01:49,445] {logging_mixin.py:95} INFO - [2018-11-08 12:01:49,444] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:01:50,266] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:50,309] {logging_mixin.py:95} INFO - [2018-11-08 12:01:50,308] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:01:50,321] {logging_mixin.py:95} INFO - [2018-11-08 12:01:50,318] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:56:50.318785+00:00

[2018-11-08 12:01:50,343] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.909 seconds
[2018-11-08 12:01:51,434] {jobs.py:385} INFO - Started process (PID=53759) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:56,439] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:01:56,441] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:01:56,443] {logging_mixin.py:95} INFO - [2018-11-08 12:01:56,442] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:01:57,219] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:01:57,243] {logging_mixin.py:95} INFO - [2018-11-08 12:01:57,243] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:01:57,246] {logging_mixin.py:95} INFO - [2018-11-08 12:01:57,244] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:56:57.244337+00:00

[2018-11-08 12:01:57,253] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.819 seconds
[2018-11-08 12:01:58,387] {jobs.py:385} INFO - Started process (PID=53761) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:03,395] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:02:03,398] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:02:03,399] {logging_mixin.py:95} INFO - [2018-11-08 12:02:03,399] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:02:03,984] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:04,027] {logging_mixin.py:95} INFO - [2018-11-08 12:02:04,027] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:02:04,032] {logging_mixin.py:95} INFO - [2018-11-08 12:02:04,030] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:57:04.030270+00:00

[2018-11-08 12:02:04,042] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.655 seconds
[2018-11-08 12:02:05,119] {jobs.py:385} INFO - Started process (PID=53763) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:10,141] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:02:10,148] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:02:10,151] {logging_mixin.py:95} INFO - [2018-11-08 12:02:10,150] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:02:10,730] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:10,767] {logging_mixin.py:95} INFO - [2018-11-08 12:02:10,765] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:02:10,770] {logging_mixin.py:95} INFO - [2018-11-08 12:02:10,768] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:57:10.768850+00:00

[2018-11-08 12:02:10,780] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.662 seconds
[2018-11-08 12:02:11,877] {jobs.py:385} INFO - Started process (PID=53764) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:16,885] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:02:16,888] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:02:16,891] {logging_mixin.py:95} INFO - [2018-11-08 12:02:16,890] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:02:17,382] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:17,407] {logging_mixin.py:95} INFO - [2018-11-08 12:02:17,406] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:02:17,408] {logging_mixin.py:95} INFO - [2018-11-08 12:02:17,407] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:57:17.407524+00:00

[2018-11-08 12:02:17,416] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.539 seconds
[2018-11-08 12:02:18,465] {jobs.py:385} INFO - Started process (PID=53766) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:23,470] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:02:23,472] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:02:23,474] {logging_mixin.py:95} INFO - [2018-11-08 12:02:23,473] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:02:24,610] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:24,635] {logging_mixin.py:95} INFO - [2018-11-08 12:02:24,635] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:02:24,637] {logging_mixin.py:95} INFO - [2018-11-08 12:02:24,636] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:57:24.636208+00:00

[2018-11-08 12:02:24,643] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.179 seconds
[2018-11-08 12:02:25,766] {jobs.py:385} INFO - Started process (PID=53767) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:30,780] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:02:30,792] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:02:30,796] {logging_mixin.py:95} INFO - [2018-11-08 12:02:30,794] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:02:32,919] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:32,987] {logging_mixin.py:95} INFO - [2018-11-08 12:02:32,986] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:02:32,990] {logging_mixin.py:95} INFO - [2018-11-08 12:02:32,988] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:57:32.987871+00:00

[2018-11-08 12:02:33,002] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.236 seconds
[2018-11-08 12:02:34,175] {jobs.py:385} INFO - Started process (PID=53770) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:39,190] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:02:39,196] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:02:39,200] {logging_mixin.py:95} INFO - [2018-11-08 12:02:39,198] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:02:41,391] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:41,429] {logging_mixin.py:95} INFO - [2018-11-08 12:02:41,427] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:02:41,440] {logging_mixin.py:95} INFO - [2018-11-08 12:02:41,431] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:57:41.431879+00:00

[2018-11-08 12:02:41,453] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.278 seconds
[2018-11-08 12:02:42,584] {jobs.py:385} INFO - Started process (PID=53772) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:47,593] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:02:47,596] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:02:47,598] {logging_mixin.py:95} INFO - [2018-11-08 12:02:47,597] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:02:48,406] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:48,444] {logging_mixin.py:95} INFO - [2018-11-08 12:02:48,443] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:02:48,449] {logging_mixin.py:95} INFO - [2018-11-08 12:02:48,447] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:57:48.447415+00:00

[2018-11-08 12:02:48,462] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.878 seconds
[2018-11-08 12:02:49,571] {jobs.py:385} INFO - Started process (PID=53776) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:54,579] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:02:54,589] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:02:54,593] {logging_mixin.py:95} INFO - [2018-11-08 12:02:54,590] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:02:55,709] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:02:55,759] {logging_mixin.py:95} INFO - [2018-11-08 12:02:55,758] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:02:55,773] {logging_mixin.py:95} INFO - [2018-11-08 12:02:55,765] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:57:55.765726+00:00

[2018-11-08 12:02:55,789] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.219 seconds
[2018-11-08 12:02:56,894] {jobs.py:385} INFO - Started process (PID=53778) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:01,906] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:03:01,908] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:03:01,910] {logging_mixin.py:95} INFO - [2018-11-08 12:03:01,909] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:03:04,479] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:04,522] {logging_mixin.py:95} INFO - [2018-11-08 12:03:04,521] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:03:04,529] {logging_mixin.py:95} INFO - [2018-11-08 12:03:04,526] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:58:04.526277+00:00

[2018-11-08 12:03:04,541] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.647 seconds
[2018-11-08 12:03:05,703] {jobs.py:385} INFO - Started process (PID=53781) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:10,717] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:03:10,721] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:03:10,724] {logging_mixin.py:95} INFO - [2018-11-08 12:03:10,723] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:03:12,066] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:12,184] {logging_mixin.py:95} INFO - [2018-11-08 12:03:12,184] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:03:12,190] {logging_mixin.py:95} INFO - [2018-11-08 12:03:12,185] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:58:12.185640+00:00

[2018-11-08 12:03:12,207] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.504 seconds
[2018-11-08 12:03:13,335] {jobs.py:385} INFO - Started process (PID=53782) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:18,345] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:03:18,348] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:03:18,349] {logging_mixin.py:95} INFO - [2018-11-08 12:03:18,349] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:03:19,117] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:19,159] {logging_mixin.py:95} INFO - [2018-11-08 12:03:19,152] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:03:19,162] {logging_mixin.py:95} INFO - [2018-11-08 12:03:19,160] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:58:19.160434+00:00

[2018-11-08 12:03:19,174] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.839 seconds
[2018-11-08 12:03:20,310] {jobs.py:385} INFO - Started process (PID=53784) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:25,336] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:03:25,343] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:03:25,345] {logging_mixin.py:95} INFO - [2018-11-08 12:03:25,344] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:03:26,283] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:26,333] {logging_mixin.py:95} INFO - [2018-11-08 12:03:26,332] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:03:26,336] {logging_mixin.py:95} INFO - [2018-11-08 12:03:26,335] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:58:26.335380+00:00

[2018-11-08 12:03:26,346] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.037 seconds
[2018-11-08 12:03:27,428] {jobs.py:385} INFO - Started process (PID=53786) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:32,433] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:03:32,435] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:03:32,437] {logging_mixin.py:95} INFO - [2018-11-08 12:03:32,436] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:03:33,188] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:33,235] {logging_mixin.py:95} INFO - [2018-11-08 12:03:33,234] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:03:33,236] {logging_mixin.py:95} INFO - [2018-11-08 12:03:33,235] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:58:33.235653+00:00

[2018-11-08 12:03:33,244] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.817 seconds
[2018-11-08 12:03:34,350] {jobs.py:385} INFO - Started process (PID=53787) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:39,358] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:03:39,362] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:03:39,365] {logging_mixin.py:95} INFO - [2018-11-08 12:03:39,364] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:03:39,868] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:39,895] {logging_mixin.py:95} INFO - [2018-11-08 12:03:39,894] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:03:39,896] {logging_mixin.py:95} INFO - [2018-11-08 12:03:39,895] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:58:39.895639+00:00

[2018-11-08 12:03:39,903] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.554 seconds
[2018-11-08 12:03:41,004] {jobs.py:385} INFO - Started process (PID=53794) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:46,010] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:03:46,016] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:03:46,018] {logging_mixin.py:95} INFO - [2018-11-08 12:03:46,017] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:03:46,587] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:46,613] {logging_mixin.py:95} INFO - [2018-11-08 12:03:46,612] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:03:46,615] {logging_mixin.py:95} INFO - [2018-11-08 12:03:46,613] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:58:46.613696+00:00

[2018-11-08 12:03:46,622] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.619 seconds
[2018-11-08 12:03:47,696] {jobs.py:385} INFO - Started process (PID=53804) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:52,706] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:03:52,709] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:03:52,710] {logging_mixin.py:95} INFO - [2018-11-08 12:03:52,710] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:03:53,333] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:53,357] {logging_mixin.py:95} INFO - [2018-11-08 12:03:53,357] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:03:53,360] {logging_mixin.py:95} INFO - [2018-11-08 12:03:53,358] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:58:53.358603+00:00

[2018-11-08 12:03:53,365] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.669 seconds
[2018-11-08 12:03:54,516] {jobs.py:385} INFO - Started process (PID=53806) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:03:59,521] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:03:59,523] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:03:59,524] {logging_mixin.py:95} INFO - [2018-11-08 12:03:59,523] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:04:00,073] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:00,100] {logging_mixin.py:95} INFO - [2018-11-08 12:04:00,100] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:04:00,102] {logging_mixin.py:95} INFO - [2018-11-08 12:04:00,101] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:59:00.101092+00:00

[2018-11-08 12:04:00,107] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.591 seconds
[2018-11-08 12:04:01,163] {jobs.py:385} INFO - Started process (PID=53807) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:06,168] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:04:06,194] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:04:06,195] {logging_mixin.py:95} INFO - [2018-11-08 12:04:06,195] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:04:06,879] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:06,906] {logging_mixin.py:95} INFO - [2018-11-08 12:04:06,906] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:04:06,908] {logging_mixin.py:95} INFO - [2018-11-08 12:04:06,907] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:59:06.907019+00:00

[2018-11-08 12:04:06,917] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.754 seconds
[2018-11-08 12:04:08,044] {jobs.py:385} INFO - Started process (PID=53809) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:13,063] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:04:13,069] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:04:13,071] {logging_mixin.py:95} INFO - [2018-11-08 12:04:13,070] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:04:13,705] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:13,732] {logging_mixin.py:95} INFO - [2018-11-08 12:04:13,731] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:04:13,735] {logging_mixin.py:95} INFO - [2018-11-08 12:04:13,732] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:59:13.732955+00:00

[2018-11-08 12:04:13,745] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.701 seconds
[2018-11-08 12:04:14,831] {jobs.py:385} INFO - Started process (PID=53812) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:19,836] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:04:19,838] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:04:19,840] {logging_mixin.py:95} INFO - [2018-11-08 12:04:19,839] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:04:21,527] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:21,577] {logging_mixin.py:95} INFO - [2018-11-08 12:04:21,574] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:04:21,581] {logging_mixin.py:95} INFO - [2018-11-08 12:04:21,578] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:59:21.578865+00:00

[2018-11-08 12:04:21,597] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.766 seconds
[2018-11-08 12:04:22,756] {jobs.py:385} INFO - Started process (PID=53814) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:27,771] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:04:27,776] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:04:27,781] {logging_mixin.py:95} INFO - [2018-11-08 12:04:27,778] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:04:28,471] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:28,499] {logging_mixin.py:95} INFO - [2018-11-08 12:04:28,499] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:04:28,501] {logging_mixin.py:95} INFO - [2018-11-08 12:04:28,500] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:59:28.500294+00:00

[2018-11-08 12:04:28,510] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.755 seconds
[2018-11-08 12:04:29,676] {jobs.py:385} INFO - Started process (PID=53816) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:34,687] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:04:34,689] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:04:34,692] {logging_mixin.py:95} INFO - [2018-11-08 12:04:34,691] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:04:35,877] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:35,910] {logging_mixin.py:95} INFO - [2018-11-08 12:04:35,910] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:04:35,911] {logging_mixin.py:95} INFO - [2018-11-08 12:04:35,910] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:59:35.910840+00:00

[2018-11-08 12:04:35,923] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.247 seconds
[2018-11-08 12:04:37,009] {jobs.py:385} INFO - Started process (PID=53817) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:42,017] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:04:42,024] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:04:42,026] {logging_mixin.py:95} INFO - [2018-11-08 12:04:42,025] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:04:43,255] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:43,289] {logging_mixin.py:95} INFO - [2018-11-08 12:04:43,288] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:04:43,293] {logging_mixin.py:95} INFO - [2018-11-08 12:04:43,289] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:59:43.289609+00:00

[2018-11-08 12:04:43,303] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.293 seconds
[2018-11-08 12:04:44,427] {jobs.py:385} INFO - Started process (PID=53822) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:49,437] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:04:49,444] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:04:49,445] {logging_mixin.py:95} INFO - [2018-11-08 12:04:49,444] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:04:50,177] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:50,206] {logging_mixin.py:95} INFO - [2018-11-08 12:04:50,205] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:04:50,209] {logging_mixin.py:95} INFO - [2018-11-08 12:04:50,207] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:59:50.207012+00:00

[2018-11-08 12:04:50,214] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.787 seconds
[2018-11-08 12:04:51,311] {jobs.py:385} INFO - Started process (PID=53823) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:56,322] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:04:56,327] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:04:56,329] {logging_mixin.py:95} INFO - [2018-11-08 12:04:56,328] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:04:56,936] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:04:56,963] {logging_mixin.py:95} INFO - [2018-11-08 12:04:56,962] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:04:56,964] {logging_mixin.py:95} INFO - [2018-11-08 12:04:56,963] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 13:59:56.963528+00:00

[2018-11-08 12:04:56,973] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.662 seconds
[2018-11-08 12:04:58,093] {jobs.py:385} INFO - Started process (PID=53826) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:05:03,104] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:05:03,113] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:05:03,128] {logging_mixin.py:95} INFO - [2018-11-08 12:05:03,127] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:05:04,787] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:05:04,814] {logging_mixin.py:95} INFO - [2018-11-08 12:05:04,813] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:05:04,815] {logging_mixin.py:95} INFO - [2018-11-08 12:05:04,814] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:00:04.814612+00:00

[2018-11-08 12:05:04,822] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.729 seconds
[2018-11-08 12:05:05,933] {jobs.py:385} INFO - Started process (PID=53828) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:05:10,944] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:05:10,947] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:05:10,948] {logging_mixin.py:95} INFO - [2018-11-08 12:05:10,948] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:05:11,998] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:05:12,038] {logging_mixin.py:95} INFO - [2018-11-08 12:05:12,037] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:05:12,047] {logging_mixin.py:95} INFO - [2018-11-08 12:05:12,038] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:00:12.038658+00:00

[2018-11-08 12:05:12,055] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.121 seconds
[2018-11-08 12:05:13,133] {jobs.py:385} INFO - Started process (PID=53829) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:05:18,139] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:05:18,142] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:05:18,143] {logging_mixin.py:95} INFO - [2018-11-08 12:05:18,142] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:05:18,619] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:05:18,642] {logging_mixin.py:95} INFO - [2018-11-08 12:05:18,642] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:05:18,644] {logging_mixin.py:95} INFO - [2018-11-08 12:05:18,643] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:00:18.643425+00:00

[2018-11-08 12:05:18,650] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.517 seconds
[2018-11-08 12:05:19,739] {jobs.py:385} INFO - Started process (PID=53831) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:05:24,745] {jobs.py:396} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 569, in __init__
    super(SchedulerJob, self).__init__(*args, **kwargs)
  File "<string>", line 6, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 107, in __init__
    self.hostname = get_hostname()
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/utils/net.py", line 45, in get_hostname
    return callable()
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 386, in helper
    scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)
  File "<string>", line 4, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0
[2018-11-08 12:09:38,582] {jobs.py:385} INFO - Started process (PID=54095) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:09:43,589] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:09:43,591] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:09:43,592] {logging_mixin.py:95} INFO - [2018-11-08 12:09:43,592] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:09:44,008] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:09:44,037] {logging_mixin.py:95} INFO - [2018-11-08 12:09:44,036] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:09:44,039] {logging_mixin.py:95} INFO - [2018-11-08 12:09:44,037] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:04:44.037839+00:00

[2018-11-08 12:09:44,046] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.463 seconds
[2018-11-08 12:09:45,167] {jobs.py:385} INFO - Started process (PID=54108) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:09:50,171] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:09:50,173] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:09:50,175] {logging_mixin.py:95} INFO - [2018-11-08 12:09:50,174] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:09:50,661] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:09:50,685] {logging_mixin.py:95} INFO - [2018-11-08 12:09:50,684] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:09:50,686] {logging_mixin.py:95} INFO - [2018-11-08 12:09:50,685] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:04:50.685599+00:00

[2018-11-08 12:09:50,693] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.526 seconds
[2018-11-08 12:09:51,779] {jobs.py:385} INFO - Started process (PID=54127) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:09:56,787] {jobs.py:396} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 569, in __init__
    super(SchedulerJob, self).__init__(*args, **kwargs)
  File "<string>", line 6, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 107, in __init__
    self.hostname = get_hostname()
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/utils/net.py", line 45, in get_hostname
    return callable()
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 386, in helper
    scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)
  File "<string>", line 4, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0
[2018-11-08 12:10:04,085] {jobs.py:385} INFO - Started process (PID=54145) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:09,097] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:10:09,100] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:10:09,101] {logging_mixin.py:95} INFO - [2018-11-08 12:10:09,100] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:10:09,960] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:09,986] {logging_mixin.py:95} INFO - [2018-11-08 12:10:09,986] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:10:09,988] {logging_mixin.py:95} INFO - [2018-11-08 12:10:09,987] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:05:09.987286+00:00

[2018-11-08 12:10:09,995] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.909 seconds
[2018-11-08 12:10:11,107] {jobs.py:385} INFO - Started process (PID=54146) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:16,114] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:10:16,118] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:10:16,119] {logging_mixin.py:95} INFO - [2018-11-08 12:10:16,119] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:10:16,542] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:16,569] {logging_mixin.py:95} INFO - [2018-11-08 12:10:16,568] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:10:16,572] {logging_mixin.py:95} INFO - [2018-11-08 12:10:16,570] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:05:16.570584+00:00

[2018-11-08 12:10:16,580] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.473 seconds
[2018-11-08 12:10:17,697] {jobs.py:385} INFO - Started process (PID=54156) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:22,703] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:10:22,706] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:10:22,708] {logging_mixin.py:95} INFO - [2018-11-08 12:10:22,707] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:10:23,130] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:23,151] {logging_mixin.py:95} INFO - [2018-11-08 12:10:23,151] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:10:23,153] {logging_mixin.py:95} INFO - [2018-11-08 12:10:23,152] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:05:23.152611+00:00

[2018-11-08 12:10:23,158] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.461 seconds
[2018-11-08 12:10:24,201] {jobs.py:385} INFO - Started process (PID=54163) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:29,210] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:10:29,213] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:10:29,215] {logging_mixin.py:95} INFO - [2018-11-08 12:10:29,214] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:10:29,601] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:29,622] {logging_mixin.py:95} INFO - [2018-11-08 12:10:29,622] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:10:29,624] {logging_mixin.py:95} INFO - [2018-11-08 12:10:29,623] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:05:29.623099+00:00

[2018-11-08 12:10:29,629] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.428 seconds
[2018-11-08 12:10:30,697] {jobs.py:385} INFO - Started process (PID=54223) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:35,706] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:10:35,709] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:10:35,710] {logging_mixin.py:95} INFO - [2018-11-08 12:10:35,710] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:10:36,147] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:36,169] {logging_mixin.py:95} INFO - [2018-11-08 12:10:36,169] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:10:36,171] {logging_mixin.py:95} INFO - [2018-11-08 12:10:36,170] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:05:36.170298+00:00

[2018-11-08 12:10:36,177] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.479 seconds
[2018-11-08 12:10:37,282] {jobs.py:385} INFO - Started process (PID=54237) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:42,294] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:10:42,296] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:10:42,298] {logging_mixin.py:95} INFO - [2018-11-08 12:10:42,298] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:10:42,746] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:42,776] {logging_mixin.py:95} INFO - [2018-11-08 12:10:42,775] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:10:42,777] {logging_mixin.py:95} INFO - [2018-11-08 12:10:42,776] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:05:42.776677+00:00

[2018-11-08 12:10:42,786] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.504 seconds
[2018-11-08 12:10:43,847] {jobs.py:385} INFO - Started process (PID=54242) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:48,854] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:10:48,864] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:10:48,866] {logging_mixin.py:95} INFO - [2018-11-08 12:10:48,866] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:10:49,889] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:49,938] {logging_mixin.py:95} INFO - [2018-11-08 12:10:49,937] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:10:49,960] {logging_mixin.py:95} INFO - [2018-11-08 12:10:49,956] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:05:49.956666+00:00

[2018-11-08 12:10:50,015] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.168 seconds
[2018-11-08 12:10:51,155] {jobs.py:385} INFO - Started process (PID=54243) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:56,179] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:10:56,182] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:10:56,183] {logging_mixin.py:95} INFO - [2018-11-08 12:10:56,183] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:10:56,756] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:10:56,781] {logging_mixin.py:95} INFO - [2018-11-08 12:10:56,781] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:10:56,784] {logging_mixin.py:95} INFO - [2018-11-08 12:10:56,782] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:05:56.782476+00:00

[2018-11-08 12:10:56,790] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.635 seconds
[2018-11-08 12:10:57,848] {jobs.py:385} INFO - Started process (PID=54245) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:02,853] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:11:02,856] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:11:02,857] {logging_mixin.py:95} INFO - [2018-11-08 12:11:02,856] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:11:03,465] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:03,490] {logging_mixin.py:95} INFO - [2018-11-08 12:11:03,489] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:11:03,491] {logging_mixin.py:95} INFO - [2018-11-08 12:11:03,490] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:06:03.490681+00:00

[2018-11-08 12:11:03,497] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.650 seconds
[2018-11-08 12:11:04,645] {jobs.py:385} INFO - Started process (PID=54247) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:09,654] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:11:09,656] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:11:09,657] {logging_mixin.py:95} INFO - [2018-11-08 12:11:09,656] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:11:10,096] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:10,121] {logging_mixin.py:95} INFO - [2018-11-08 12:11:10,120] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:11:10,122] {logging_mixin.py:95} INFO - [2018-11-08 12:11:10,121] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:06:10.121474+00:00

[2018-11-08 12:11:10,128] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.484 seconds
[2018-11-08 12:11:11,222] {jobs.py:385} INFO - Started process (PID=54249) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:16,232] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:11:16,236] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:11:16,237] {logging_mixin.py:95} INFO - [2018-11-08 12:11:16,237] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:11:16,721] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:16,744] {logging_mixin.py:95} INFO - [2018-11-08 12:11:16,743] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:11:16,746] {logging_mixin.py:95} INFO - [2018-11-08 12:11:16,745] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:06:16.745023+00:00

[2018-11-08 12:11:16,751] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.528 seconds
[2018-11-08 12:11:17,820] {jobs.py:385} INFO - Started process (PID=54251) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:22,824] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:11:22,826] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:11:22,828] {logging_mixin.py:95} INFO - [2018-11-08 12:11:22,828] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:11:23,248] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:23,269] {logging_mixin.py:95} INFO - [2018-11-08 12:11:23,268] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:11:23,271] {logging_mixin.py:95} INFO - [2018-11-08 12:11:23,269] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:06:23.269568+00:00

[2018-11-08 12:11:23,276] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.456 seconds
[2018-11-08 12:11:24,332] {jobs.py:385} INFO - Started process (PID=54252) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:29,340] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:11:29,343] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:11:29,345] {logging_mixin.py:95} INFO - [2018-11-08 12:11:29,344] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:11:30,083] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:30,142] {logging_mixin.py:95} INFO - [2018-11-08 12:11:30,141] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:11:30,147] {logging_mixin.py:95} INFO - [2018-11-08 12:11:30,142] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:06:30.142909+00:00

[2018-11-08 12:11:30,170] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.838 seconds
[2018-11-08 12:11:31,233] {jobs.py:385} INFO - Started process (PID=54259) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:36,241] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:11:36,245] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:11:36,246] {logging_mixin.py:95} INFO - [2018-11-08 12:11:36,246] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:11:37,098] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:37,130] {logging_mixin.py:95} INFO - [2018-11-08 12:11:37,130] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:11:37,132] {logging_mixin.py:95} INFO - [2018-11-08 12:11:37,131] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:06:37.131647+00:00

[2018-11-08 12:11:37,142] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.909 seconds
[2018-11-08 12:11:38,190] {jobs.py:385} INFO - Started process (PID=54290) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:43,198] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:11:43,201] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:11:43,203] {logging_mixin.py:95} INFO - [2018-11-08 12:11:43,202] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:11:43,626] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:43,648] {logging_mixin.py:95} INFO - [2018-11-08 12:11:43,648] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:11:43,650] {logging_mixin.py:95} INFO - [2018-11-08 12:11:43,649] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:06:43.649056+00:00

[2018-11-08 12:11:43,655] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.466 seconds
[2018-11-08 12:11:44,782] {jobs.py:385} INFO - Started process (PID=54298) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:49,790] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:11:49,792] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:11:49,794] {logging_mixin.py:95} INFO - [2018-11-08 12:11:49,793] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:11:50,339] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:50,362] {logging_mixin.py:95} INFO - [2018-11-08 12:11:50,362] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:11:50,364] {logging_mixin.py:95} INFO - [2018-11-08 12:11:50,363] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:06:50.363304+00:00

[2018-11-08 12:11:50,370] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.588 seconds
[2018-11-08 12:11:51,464] {jobs.py:385} INFO - Started process (PID=54299) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:56,468] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:11:56,472] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:11:56,473] {logging_mixin.py:95} INFO - [2018-11-08 12:11:56,473] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:11:56,966] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:11:56,989] {logging_mixin.py:95} INFO - [2018-11-08 12:11:56,988] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:11:56,990] {logging_mixin.py:95} INFO - [2018-11-08 12:11:56,989] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:06:56.989591+00:00

[2018-11-08 12:11:56,995] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.531 seconds
[2018-11-08 12:11:58,061] {jobs.py:385} INFO - Started process (PID=54302) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:03,068] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:12:03,071] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:12:03,073] {logging_mixin.py:95} INFO - [2018-11-08 12:12:03,073] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:12:03,484] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:03,510] {logging_mixin.py:95} INFO - [2018-11-08 12:12:03,509] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:12:03,512] {logging_mixin.py:95} INFO - [2018-11-08 12:12:03,510] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:07:03.510732+00:00

[2018-11-08 12:12:03,517] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.457 seconds
[2018-11-08 12:12:04,652] {jobs.py:385} INFO - Started process (PID=54304) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:09,660] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:12:09,664] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:12:09,665] {logging_mixin.py:95} INFO - [2018-11-08 12:12:09,665] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:12:10,299] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:10,344] {logging_mixin.py:95} INFO - [2018-11-08 12:12:10,344] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:12:10,347] {logging_mixin.py:95} INFO - [2018-11-08 12:12:10,345] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:07:10.345674+00:00

[2018-11-08 12:12:10,356] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.704 seconds
[2018-11-08 12:12:11,434] {jobs.py:385} INFO - Started process (PID=54318) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:16,443] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:12:16,446] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:12:16,448] {logging_mixin.py:95} INFO - [2018-11-08 12:12:16,447] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:12:16,932] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:16,959] {logging_mixin.py:95} INFO - [2018-11-08 12:12:16,959] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:12:16,961] {logging_mixin.py:95} INFO - [2018-11-08 12:12:16,960] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:07:16.960419+00:00

[2018-11-08 12:12:16,968] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.535 seconds
[2018-11-08 12:12:18,107] {jobs.py:385} INFO - Started process (PID=54323) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:23,114] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:12:23,120] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:12:23,122] {logging_mixin.py:95} INFO - [2018-11-08 12:12:23,121] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:12:23,668] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:23,690] {logging_mixin.py:95} INFO - [2018-11-08 12:12:23,690] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:12:23,692] {logging_mixin.py:95} INFO - [2018-11-08 12:12:23,691] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:07:23.691104+00:00

[2018-11-08 12:12:23,697] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.590 seconds
[2018-11-08 12:12:24,767] {jobs.py:385} INFO - Started process (PID=54324) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:29,772] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:12:29,774] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:12:29,774] {logging_mixin.py:95} INFO - [2018-11-08 12:12:29,774] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:12:30,278] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:30,300] {logging_mixin.py:95} INFO - [2018-11-08 12:12:30,299] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:12:30,301] {logging_mixin.py:95} INFO - [2018-11-08 12:12:30,300] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:07:30.300645+00:00

[2018-11-08 12:12:30,306] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.540 seconds
[2018-11-08 12:12:31,358] {jobs.py:385} INFO - Started process (PID=54326) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:36,365] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:12:36,368] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:12:36,370] {logging_mixin.py:95} INFO - [2018-11-08 12:12:36,370] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:12:36,929] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:36,950] {logging_mixin.py:95} INFO - [2018-11-08 12:12:36,950] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:12:36,952] {logging_mixin.py:95} INFO - [2018-11-08 12:12:36,951] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:07:36.951444+00:00

[2018-11-08 12:12:36,958] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.600 seconds
[2018-11-08 12:12:38,057] {jobs.py:385} INFO - Started process (PID=54341) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:43,066] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:12:43,070] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:12:43,072] {logging_mixin.py:95} INFO - [2018-11-08 12:12:43,071] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:12:43,617] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:43,637] {logging_mixin.py:95} INFO - [2018-11-08 12:12:43,637] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:12:43,639] {logging_mixin.py:95} INFO - [2018-11-08 12:12:43,638] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:07:43.638282+00:00

[2018-11-08 12:12:43,644] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.587 seconds
[2018-11-08 12:12:44,776] {jobs.py:385} INFO - Started process (PID=54346) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:49,784] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:12:49,786] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:12:49,788] {logging_mixin.py:95} INFO - [2018-11-08 12:12:49,787] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:12:50,218] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:50,247] {logging_mixin.py:95} INFO - [2018-11-08 12:12:50,246] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:12:50,249] {logging_mixin.py:95} INFO - [2018-11-08 12:12:50,247] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:07:50.247512+00:00

[2018-11-08 12:12:50,255] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.479 seconds
[2018-11-08 12:12:51,393] {jobs.py:385} INFO - Started process (PID=54348) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:56,401] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:12:56,405] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:12:56,407] {logging_mixin.py:95} INFO - [2018-11-08 12:12:56,406] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:12:57,219] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:12:57,241] {logging_mixin.py:95} INFO - [2018-11-08 12:12:57,241] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:12:57,243] {logging_mixin.py:95} INFO - [2018-11-08 12:12:57,242] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:07:57.242026+00:00

[2018-11-08 12:12:57,249] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.857 seconds
[2018-11-08 12:12:58,406] {jobs.py:385} INFO - Started process (PID=54412) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:03,416] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:13:03,419] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:13:03,421] {logging_mixin.py:95} INFO - [2018-11-08 12:13:03,420] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:13:03,845] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:03,870] {logging_mixin.py:95} INFO - [2018-11-08 12:13:03,870] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:13:03,871] {logging_mixin.py:95} INFO - [2018-11-08 12:13:03,870] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:08:03.870950+00:00

[2018-11-08 12:13:03,877] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.471 seconds
[2018-11-08 12:13:04,966] {jobs.py:385} INFO - Started process (PID=54414) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:09,974] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:13:09,978] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:13:09,981] {logging_mixin.py:95} INFO - [2018-11-08 12:13:09,980] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:13:10,439] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:10,465] {logging_mixin.py:95} INFO - [2018-11-08 12:13:10,465] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:13:10,467] {logging_mixin.py:95} INFO - [2018-11-08 12:13:10,466] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:08:10.466201+00:00

[2018-11-08 12:13:10,473] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.507 seconds
[2018-11-08 12:13:11,534] {jobs.py:385} INFO - Started process (PID=54416) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:16,540] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:13:16,552] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:13:16,572] {logging_mixin.py:95} INFO - [2018-11-08 12:13:16,571] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:13:17,147] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:17,168] {logging_mixin.py:95} INFO - [2018-11-08 12:13:17,167] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:13:17,169] {logging_mixin.py:95} INFO - [2018-11-08 12:13:17,168] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:08:17.168453+00:00

[2018-11-08 12:13:17,176] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.642 seconds
[2018-11-08 12:13:18,311] {jobs.py:385} INFO - Started process (PID=54423) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:23,317] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:13:23,321] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:13:23,323] {logging_mixin.py:95} INFO - [2018-11-08 12:13:23,322] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:13:24,118] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:24,151] {logging_mixin.py:95} INFO - [2018-11-08 12:13:24,151] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:13:24,153] {logging_mixin.py:95} INFO - [2018-11-08 12:13:24,152] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:08:24.152332+00:00

[2018-11-08 12:13:24,161] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.849 seconds
[2018-11-08 12:13:25,279] {jobs.py:385} INFO - Started process (PID=54431) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:30,285] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:13:30,288] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:13:30,290] {logging_mixin.py:95} INFO - [2018-11-08 12:13:30,289] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:13:30,852] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:30,980] {logging_mixin.py:95} INFO - [2018-11-08 12:13:30,979] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:13:30,994] {logging_mixin.py:95} INFO - [2018-11-08 12:13:30,990] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:08:30.990696+00:00

[2018-11-08 12:13:31,012] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.734 seconds
[2018-11-08 12:13:32,149] {jobs.py:385} INFO - Started process (PID=54433) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:37,155] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:13:37,159] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:13:37,161] {logging_mixin.py:95} INFO - [2018-11-08 12:13:37,160] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:13:37,686] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:37,720] {logging_mixin.py:95} INFO - [2018-11-08 12:13:37,720] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:13:37,722] {logging_mixin.py:95} INFO - [2018-11-08 12:13:37,721] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:08:37.721041+00:00

[2018-11-08 12:13:37,727] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.578 seconds
[2018-11-08 12:13:38,807] {jobs.py:385} INFO - Started process (PID=54441) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:43,819] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:13:43,827] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:13:43,830] {logging_mixin.py:95} INFO - [2018-11-08 12:13:43,829] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:13:44,992] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:45,014] {logging_mixin.py:95} INFO - [2018-11-08 12:13:45,014] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:13:45,016] {logging_mixin.py:95} INFO - [2018-11-08 12:13:45,015] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:08:45.015061+00:00

[2018-11-08 12:13:45,021] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.214 seconds
[2018-11-08 12:13:46,109] {jobs.py:385} INFO - Started process (PID=54447) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:51,117] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:13:51,119] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:13:51,120] {logging_mixin.py:95} INFO - [2018-11-08 12:13:51,120] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:13:51,637] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:51,659] {logging_mixin.py:95} INFO - [2018-11-08 12:13:51,659] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:13:51,661] {logging_mixin.py:95} INFO - [2018-11-08 12:13:51,660] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:08:51.660337+00:00

[2018-11-08 12:13:51,671] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.562 seconds
[2018-11-08 12:13:52,727] {jobs.py:385} INFO - Started process (PID=54455) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:57,734] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:13:57,741] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:13:57,743] {logging_mixin.py:95} INFO - [2018-11-08 12:13:57,742] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:13:58,367] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:13:58,429] {logging_mixin.py:95} INFO - [2018-11-08 12:13:58,428] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:13:58,432] {logging_mixin.py:95} INFO - [2018-11-08 12:13:58,430] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:08:58.430645+00:00

[2018-11-08 12:13:58,447] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.721 seconds
[2018-11-08 12:13:59,561] {jobs.py:385} INFO - Started process (PID=54457) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:04,571] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:14:04,573] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:14:04,574] {logging_mixin.py:95} INFO - [2018-11-08 12:14:04,574] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:14:05,065] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:05,090] {logging_mixin.py:95} INFO - [2018-11-08 12:14:05,089] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:14:05,091] {logging_mixin.py:95} INFO - [2018-11-08 12:14:05,090] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:09:05.090435+00:00

[2018-11-08 12:14:05,096] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.535 seconds
[2018-11-08 12:14:06,240] {jobs.py:385} INFO - Started process (PID=54466) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:11,250] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:14:11,256] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:14:11,258] {logging_mixin.py:95} INFO - [2018-11-08 12:14:11,257] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:14:11,885] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:11,931] {logging_mixin.py:95} INFO - [2018-11-08 12:14:11,931] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:14:11,933] {logging_mixin.py:95} INFO - [2018-11-08 12:14:11,932] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:09:11.932112+00:00

[2018-11-08 12:14:11,941] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.701 seconds
[2018-11-08 12:14:13,036] {jobs.py:385} INFO - Started process (PID=54467) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:18,054] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:14:18,057] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:14:18,058] {logging_mixin.py:95} INFO - [2018-11-08 12:14:18,057] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:14:18,442] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:18,462] {logging_mixin.py:95} INFO - [2018-11-08 12:14:18,462] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:14:18,464] {logging_mixin.py:95} INFO - [2018-11-08 12:14:18,463] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:09:18.463330+00:00

[2018-11-08 12:14:18,472] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.436 seconds
[2018-11-08 12:14:19,621] {jobs.py:385} INFO - Started process (PID=54469) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:24,630] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:14:24,633] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:14:24,635] {logging_mixin.py:95} INFO - [2018-11-08 12:14:24,635] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:14:25,013] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:25,034] {logging_mixin.py:95} INFO - [2018-11-08 12:14:25,034] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:14:25,036] {logging_mixin.py:95} INFO - [2018-11-08 12:14:25,035] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:09:25.035338+00:00

[2018-11-08 12:14:25,042] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.421 seconds
[2018-11-08 12:14:26,099] {jobs.py:385} INFO - Started process (PID=54470) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:31,104] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:14:31,110] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:14:31,112] {logging_mixin.py:95} INFO - [2018-11-08 12:14:31,111] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:14:31,632] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:31,661] {logging_mixin.py:95} INFO - [2018-11-08 12:14:31,661] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:14:31,663] {logging_mixin.py:95} INFO - [2018-11-08 12:14:31,662] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:09:31.662195+00:00

[2018-11-08 12:14:31,670] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.571 seconds
[2018-11-08 12:14:32,753] {jobs.py:385} INFO - Started process (PID=54473) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:37,758] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:14:37,760] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:14:37,761] {logging_mixin.py:95} INFO - [2018-11-08 12:14:37,761] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:14:38,152] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:38,175] {logging_mixin.py:95} INFO - [2018-11-08 12:14:38,175] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:14:38,177] {logging_mixin.py:95} INFO - [2018-11-08 12:14:38,176] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:09:38.176204+00:00

[2018-11-08 12:14:38,183] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.430 seconds
[2018-11-08 12:14:39,325] {jobs.py:385} INFO - Started process (PID=54479) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:44,336] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:14:44,344] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:14:44,346] {logging_mixin.py:95} INFO - [2018-11-08 12:14:44,345] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:14:44,842] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:44,865] {logging_mixin.py:95} INFO - [2018-11-08 12:14:44,865] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:14:44,866] {logging_mixin.py:95} INFO - [2018-11-08 12:14:44,865] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:09:44.865876+00:00

[2018-11-08 12:14:44,871] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.546 seconds
[2018-11-08 12:14:46,016] {jobs.py:385} INFO - Started process (PID=54484) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:51,026] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:14:51,028] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:14:51,029] {logging_mixin.py:95} INFO - [2018-11-08 12:14:51,029] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:14:51,485] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:51,510] {logging_mixin.py:95} INFO - [2018-11-08 12:14:51,509] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:14:51,511] {logging_mixin.py:95} INFO - [2018-11-08 12:14:51,510] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:09:51.510701+00:00

[2018-11-08 12:14:51,516] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.500 seconds
[2018-11-08 12:14:52,577] {jobs.py:385} INFO - Started process (PID=54491) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:57,586] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:14:57,589] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:14:57,591] {logging_mixin.py:95} INFO - [2018-11-08 12:14:57,590] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:14:57,990] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:14:58,013] {logging_mixin.py:95} INFO - [2018-11-08 12:14:58,013] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:14:58,015] {logging_mixin.py:95} INFO - [2018-11-08 12:14:58,013] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:09:58.013879+00:00

[2018-11-08 12:14:58,020] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.443 seconds
[2018-11-08 12:14:59,076] {jobs.py:385} INFO - Started process (PID=54495) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:04,081] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:15:04,085] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:15:04,086] {logging_mixin.py:95} INFO - [2018-11-08 12:15:04,086] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:15:04,511] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:04,537] {logging_mixin.py:95} INFO - [2018-11-08 12:15:04,537] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:15:04,539] {logging_mixin.py:95} INFO - [2018-11-08 12:15:04,538] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:10:04.538030+00:00

[2018-11-08 12:15:04,546] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.470 seconds
[2018-11-08 12:15:05,676] {jobs.py:385} INFO - Started process (PID=54499) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:10,685] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:15:10,689] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:15:10,691] {logging_mixin.py:95} INFO - [2018-11-08 12:15:10,691] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:15:11,276] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:11,297] {logging_mixin.py:95} INFO - [2018-11-08 12:15:11,297] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:15:11,299] {logging_mixin.py:95} INFO - [2018-11-08 12:15:11,298] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:10:11.298363+00:00

[2018-11-08 12:15:11,304] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.628 seconds
[2018-11-08 12:15:12,372] {jobs.py:385} INFO - Started process (PID=54504) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:17,381] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:15:17,383] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:15:17,384] {logging_mixin.py:95} INFO - [2018-11-08 12:15:17,384] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:15:17,916] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:17,938] {logging_mixin.py:95} INFO - [2018-11-08 12:15:17,938] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:15:17,940] {logging_mixin.py:95} INFO - [2018-11-08 12:15:17,939] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:10:17.939149+00:00

[2018-11-08 12:15:17,945] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.574 seconds
[2018-11-08 12:15:19,055] {jobs.py:385} INFO - Started process (PID=54508) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:24,060] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:15:24,062] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:15:24,064] {logging_mixin.py:95} INFO - [2018-11-08 12:15:24,064] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:15:24,475] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:24,498] {logging_mixin.py:95} INFO - [2018-11-08 12:15:24,497] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:15:24,500] {logging_mixin.py:95} INFO - [2018-11-08 12:15:24,499] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:10:24.498997+00:00

[2018-11-08 12:15:24,506] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.452 seconds
[2018-11-08 12:15:25,619] {jobs.py:385} INFO - Started process (PID=54524) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:30,628] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:15:30,632] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:15:30,634] {logging_mixin.py:95} INFO - [2018-11-08 12:15:30,633] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:15:31,057] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:31,086] {logging_mixin.py:95} INFO - [2018-11-08 12:15:31,085] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:15:31,088] {logging_mixin.py:95} INFO - [2018-11-08 12:15:31,086] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:10:31.086555+00:00

[2018-11-08 12:15:31,095] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.476 seconds
[2018-11-08 12:15:32,207] {jobs.py:385} INFO - Started process (PID=54526) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:37,212] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:15:37,219] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:15:37,220] {logging_mixin.py:95} INFO - [2018-11-08 12:15:37,220] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:15:37,604] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:37,624] {logging_mixin.py:95} INFO - [2018-11-08 12:15:37,623] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:15:37,626] {logging_mixin.py:95} INFO - [2018-11-08 12:15:37,625] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:10:37.625178+00:00

[2018-11-08 12:15:37,633] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.426 seconds
[2018-11-08 12:15:38,678] {jobs.py:385} INFO - Started process (PID=54528) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:43,683] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:15:43,687] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:15:43,689] {logging_mixin.py:95} INFO - [2018-11-08 12:15:43,689] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:15:44,157] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:44,180] {logging_mixin.py:95} INFO - [2018-11-08 12:15:44,180] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:15:44,182] {logging_mixin.py:95} INFO - [2018-11-08 12:15:44,181] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:10:44.181267+00:00

[2018-11-08 12:15:44,187] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.510 seconds
[2018-11-08 12:15:45,271] {jobs.py:385} INFO - Started process (PID=54533) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:50,279] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:15:50,296] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:15:50,298] {logging_mixin.py:95} INFO - [2018-11-08 12:15:50,297] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:15:50,804] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:50,832] {logging_mixin.py:95} INFO - [2018-11-08 12:15:50,832] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:15:50,834] {logging_mixin.py:95} INFO - [2018-11-08 12:15:50,832] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:10:50.832877+00:00

[2018-11-08 12:15:50,842] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.571 seconds
[2018-11-08 12:15:51,928] {jobs.py:385} INFO - Started process (PID=54534) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:56,939] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:15:56,943] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:15:56,945] {logging_mixin.py:95} INFO - [2018-11-08 12:15:56,944] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:15:57,440] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:15:57,463] {logging_mixin.py:95} INFO - [2018-11-08 12:15:57,463] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:15:57,464] {logging_mixin.py:95} INFO - [2018-11-08 12:15:57,463] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:10:57.463890+00:00

[2018-11-08 12:15:57,469] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.541 seconds
[2018-11-08 12:15:58,616] {jobs.py:385} INFO - Started process (PID=54536) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:03,627] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:16:03,630] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:16:03,632] {logging_mixin.py:95} INFO - [2018-11-08 12:16:03,631] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:16:04,206] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:04,232] {logging_mixin.py:95} INFO - [2018-11-08 12:16:04,231] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:16:04,234] {logging_mixin.py:95} INFO - [2018-11-08 12:16:04,233] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:11:04.233420+00:00

[2018-11-08 12:16:04,240] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.624 seconds
[2018-11-08 12:16:05,378] {jobs.py:385} INFO - Started process (PID=54538) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:10,397] {jobs.py:396} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 569, in __init__
    super(SchedulerJob, self).__init__(*args, **kwargs)
  File "<string>", line 6, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 107, in __init__
    self.hostname = get_hostname()
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/utils/net.py", line 45, in get_hostname
    return callable()
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 386, in helper
    scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)
  File "<string>", line 4, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0
[2018-11-08 12:16:19,364] {jobs.py:385} INFO - Started process (PID=54562) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:24,373] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:16:24,376] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:16:24,378] {logging_mixin.py:95} INFO - [2018-11-08 12:16:24,378] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:16:24,839] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:24,860] {logging_mixin.py:95} INFO - [2018-11-08 12:16:24,860] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:16:24,862] {logging_mixin.py:95} INFO - [2018-11-08 12:16:24,860] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:11:24.860958+00:00

[2018-11-08 12:16:24,867] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.503 seconds
[2018-11-08 12:16:25,930] {jobs.py:385} INFO - Started process (PID=54563) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:30,938] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:16:30,942] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:16:30,944] {logging_mixin.py:95} INFO - [2018-11-08 12:16:30,943] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:16:31,823] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:31,854] {logging_mixin.py:95} INFO - [2018-11-08 12:16:31,854] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:16:31,856] {logging_mixin.py:95} INFO - [2018-11-08 12:16:31,855] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:11:31.855414+00:00

[2018-11-08 12:16:31,862] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.932 seconds
[2018-11-08 12:16:32,963] {jobs.py:385} INFO - Started process (PID=54565) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:37,972] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:16:37,973] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:16:37,976] {logging_mixin.py:95} INFO - [2018-11-08 12:16:37,975] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:16:38,608] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:38,780] {logging_mixin.py:95} INFO - [2018-11-08 12:16:38,779] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:16:38,784] {logging_mixin.py:95} INFO - [2018-11-08 12:16:38,781] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:11:38.781744+00:00

[2018-11-08 12:16:38,805] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.842 seconds
[2018-11-08 12:16:39,953] {jobs.py:385} INFO - Started process (PID=54574) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:44,964] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:16:44,965] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:16:44,966] {logging_mixin.py:95} INFO - [2018-11-08 12:16:44,966] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:16:45,456] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:45,483] {logging_mixin.py:95} INFO - [2018-11-08 12:16:45,482] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:16:45,484] {logging_mixin.py:95} INFO - [2018-11-08 12:16:45,483] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:11:45.483681+00:00

[2018-11-08 12:16:45,491] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.538 seconds
[2018-11-08 12:16:46,622] {jobs.py:385} INFO - Started process (PID=54593) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:51,630] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:16:51,634] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:16:51,636] {logging_mixin.py:95} INFO - [2018-11-08 12:16:51,636] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:16:52,170] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:52,195] {logging_mixin.py:95} INFO - [2018-11-08 12:16:52,195] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:16:52,197] {logging_mixin.py:95} INFO - [2018-11-08 12:16:52,196] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:11:52.195976+00:00

[2018-11-08 12:16:52,203] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.580 seconds
[2018-11-08 12:16:53,307] {jobs.py:385} INFO - Started process (PID=54603) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:16:58,312] {jobs.py:396} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 569, in __init__
    super(SchedulerJob, self).__init__(*args, **kwargs)
  File "<string>", line 6, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 107, in __init__
    self.hostname = get_hostname()
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/utils/net.py", line 45, in get_hostname
    return callable()
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 386, in helper
    scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)
  File "<string>", line 4, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0
[2018-11-08 12:17:06,055] {jobs.py:385} INFO - Started process (PID=54626) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:11,061] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:17:11,063] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:17:11,064] {logging_mixin.py:95} INFO - [2018-11-08 12:17:11,064] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:17:12,201] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:12,234] {logging_mixin.py:95} INFO - [2018-11-08 12:17:12,233] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:17:12,237] {logging_mixin.py:95} INFO - [2018-11-08 12:17:12,235] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:12:12.235230+00:00

[2018-11-08 12:17:12,250] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.195 seconds
[2018-11-08 12:17:13,422] {jobs.py:385} INFO - Started process (PID=54627) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:18,433] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:17:18,437] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:17:18,440] {logging_mixin.py:95} INFO - [2018-11-08 12:17:18,439] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:17:19,269] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:19,321] {logging_mixin.py:95} INFO - [2018-11-08 12:17:19,321] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:17:19,326] {logging_mixin.py:95} INFO - [2018-11-08 12:17:19,322] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:12:19.322220+00:00

[2018-11-08 12:17:19,337] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.915 seconds
[2018-11-08 12:17:20,459] {jobs.py:385} INFO - Started process (PID=54629) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:25,465] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:17:25,468] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:17:25,469] {logging_mixin.py:95} INFO - [2018-11-08 12:17:25,469] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:17:26,033] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:26,063] {logging_mixin.py:95} INFO - [2018-11-08 12:17:26,063] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:17:26,065] {logging_mixin.py:95} INFO - [2018-11-08 12:17:26,064] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:12:26.064477+00:00

[2018-11-08 12:17:26,072] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.613 seconds
[2018-11-08 12:17:27,159] {jobs.py:385} INFO - Started process (PID=54632) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:32,174] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:17:32,177] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:17:32,179] {logging_mixin.py:95} INFO - [2018-11-08 12:17:32,178] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:17:32,652] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:32,693] {logging_mixin.py:95} INFO - [2018-11-08 12:17:32,693] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:17:32,695] {logging_mixin.py:95} INFO - [2018-11-08 12:17:32,694] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:12:32.694247+00:00

[2018-11-08 12:17:32,702] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.543 seconds
[2018-11-08 12:17:33,866] {jobs.py:385} INFO - Started process (PID=54641) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:38,876] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:17:38,880] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:17:38,881] {logging_mixin.py:95} INFO - [2018-11-08 12:17:38,881] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:17:39,558] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:39,636] {logging_mixin.py:95} INFO - [2018-11-08 12:17:39,635] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:17:39,638] {logging_mixin.py:95} INFO - [2018-11-08 12:17:39,636] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:12:39.636544+00:00

[2018-11-08 12:17:39,657] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.792 seconds
[2018-11-08 12:17:40,782] {jobs.py:385} INFO - Started process (PID=54644) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:45,789] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:17:45,792] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:17:45,797] {logging_mixin.py:95} INFO - [2018-11-08 12:17:45,796] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:17:46,244] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:46,264] {logging_mixin.py:95} INFO - [2018-11-08 12:17:46,264] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:17:46,265] {logging_mixin.py:95} INFO - [2018-11-08 12:17:46,264] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:12:46.264787+00:00

[2018-11-08 12:17:46,270] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.489 seconds
[2018-11-08 12:17:47,387] {jobs.py:385} INFO - Started process (PID=54651) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:52,395] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:17:52,398] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:17:52,400] {logging_mixin.py:95} INFO - [2018-11-08 12:17:52,399] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:17:52,901] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:52,927] {logging_mixin.py:95} INFO - [2018-11-08 12:17:52,927] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:17:52,929] {logging_mixin.py:95} INFO - [2018-11-08 12:17:52,928] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:12:52.928287+00:00

[2018-11-08 12:17:52,934] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.547 seconds
[2018-11-08 12:17:54,079] {jobs.py:385} INFO - Started process (PID=54657) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:17:59,082] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:17:59,084] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:17:59,086] {logging_mixin.py:95} INFO - [2018-11-08 12:17:59,085] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:18:00,040] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:00,070] {logging_mixin.py:95} INFO - [2018-11-08 12:18:00,070] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:18:00,072] {logging_mixin.py:95} INFO - [2018-11-08 12:18:00,071] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:13:00.070997+00:00

[2018-11-08 12:18:00,080] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.002 seconds
[2018-11-08 12:18:01,181] {jobs.py:385} INFO - Started process (PID=54665) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:06,189] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:18:06,191] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:18:06,196] {logging_mixin.py:95} INFO - [2018-11-08 12:18:06,192] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:18:06,739] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:06,786] {logging_mixin.py:95} INFO - [2018-11-08 12:18:06,785] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:18:06,788] {logging_mixin.py:95} INFO - [2018-11-08 12:18:06,787] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:13:06.786809+00:00

[2018-11-08 12:18:06,807] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.626 seconds
[2018-11-08 12:18:07,878] {jobs.py:385} INFO - Started process (PID=54686) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:12,886] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:18:12,890] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:18:12,891] {logging_mixin.py:95} INFO - [2018-11-08 12:18:12,890] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:18:13,458] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:13,481] {logging_mixin.py:95} INFO - [2018-11-08 12:18:13,480] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:18:13,482] {logging_mixin.py:95} INFO - [2018-11-08 12:18:13,481] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:13:13.481816+00:00

[2018-11-08 12:18:13,487] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.610 seconds
[2018-11-08 12:18:14,568] {jobs.py:385} INFO - Started process (PID=54694) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:19,577] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:18:19,581] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:18:19,584] {logging_mixin.py:95} INFO - [2018-11-08 12:18:19,583] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:18:20,153] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:20,175] {logging_mixin.py:95} INFO - [2018-11-08 12:18:20,175] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:18:20,177] {logging_mixin.py:95} INFO - [2018-11-08 12:18:20,176] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:13:20.176451+00:00

[2018-11-08 12:18:20,182] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.614 seconds
[2018-11-08 12:18:21,255] {jobs.py:385} INFO - Started process (PID=54702) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:26,264] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:18:26,268] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:18:26,278] {logging_mixin.py:95} INFO - [2018-11-08 12:18:26,276] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:18:27,417] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:27,454] {logging_mixin.py:95} INFO - [2018-11-08 12:18:27,453] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:18:27,456] {logging_mixin.py:95} INFO - [2018-11-08 12:18:27,455] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:13:27.455293+00:00

[2018-11-08 12:18:27,464] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.209 seconds
[2018-11-08 12:18:28,623] {jobs.py:385} INFO - Started process (PID=54705) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:33,634] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:18:33,638] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:18:33,640] {logging_mixin.py:95} INFO - [2018-11-08 12:18:33,639] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:18:35,124] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:35,190] {logging_mixin.py:95} INFO - [2018-11-08 12:18:35,186] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:18:35,193] {logging_mixin.py:95} INFO - [2018-11-08 12:18:35,191] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:13:35.191308+00:00

[2018-11-08 12:18:35,237] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.614 seconds
[2018-11-08 12:18:36,307] {jobs.py:385} INFO - Started process (PID=54711) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:41,317] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:18:41,321] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:18:41,323] {logging_mixin.py:95} INFO - [2018-11-08 12:18:41,322] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:18:41,918] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:41,939] {logging_mixin.py:95} INFO - [2018-11-08 12:18:41,938] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:18:41,940] {logging_mixin.py:95} INFO - [2018-11-08 12:18:41,939] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:13:41.939409+00:00

[2018-11-08 12:18:41,946] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.639 seconds
[2018-11-08 12:18:43,074] {jobs.py:385} INFO - Started process (PID=54714) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:48,086] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:18:48,089] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:18:48,091] {logging_mixin.py:95} INFO - [2018-11-08 12:18:48,090] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:18:48,819] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:48,854] {logging_mixin.py:95} INFO - [2018-11-08 12:18:48,853] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:18:48,857] {logging_mixin.py:95} INFO - [2018-11-08 12:18:48,855] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:13:48.855283+00:00

[2018-11-08 12:18:48,868] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.794 seconds
[2018-11-08 12:18:49,960] {jobs.py:385} INFO - Started process (PID=54718) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:54,977] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:18:54,980] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:18:54,982] {logging_mixin.py:95} INFO - [2018-11-08 12:18:54,981] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:18:55,417] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:18:55,437] {logging_mixin.py:95} INFO - [2018-11-08 12:18:55,437] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:18:55,439] {logging_mixin.py:95} INFO - [2018-11-08 12:18:55,438] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:13:55.438150+00:00

[2018-11-08 12:18:55,445] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.487 seconds
[2018-11-08 12:18:56,548] {jobs.py:385} INFO - Started process (PID=54720) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:01,555] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:19:01,558] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:19:01,560] {logging_mixin.py:95} INFO - [2018-11-08 12:19:01,559] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:19:02,371] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:02,712] {logging_mixin.py:95} INFO - [2018-11-08 12:19:02,712] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:19:02,717] {logging_mixin.py:95} INFO - [2018-11-08 12:19:02,713] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:14:02.713775+00:00

[2018-11-08 12:19:02,729] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.181 seconds
[2018-11-08 12:19:03,885] {jobs.py:385} INFO - Started process (PID=54722) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:08,895] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:19:08,896] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:19:08,897] {logging_mixin.py:95} INFO - [2018-11-08 12:19:08,897] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:19:09,413] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:09,439] {logging_mixin.py:95} INFO - [2018-11-08 12:19:09,439] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:19:09,441] {logging_mixin.py:95} INFO - [2018-11-08 12:19:09,440] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:14:09.440192+00:00

[2018-11-08 12:19:09,448] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.564 seconds
[2018-11-08 12:19:10,554] {jobs.py:385} INFO - Started process (PID=54723) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:15,562] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:19:15,565] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:19:15,566] {logging_mixin.py:95} INFO - [2018-11-08 12:19:15,566] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:19:16,094] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:16,118] {logging_mixin.py:95} INFO - [2018-11-08 12:19:16,118] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:19:16,120] {logging_mixin.py:95} INFO - [2018-11-08 12:19:16,119] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:14:16.119070+00:00

[2018-11-08 12:19:16,125] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.571 seconds
[2018-11-08 12:19:17,267] {jobs.py:385} INFO - Started process (PID=54726) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:22,273] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:19:22,275] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:19:22,276] {logging_mixin.py:95} INFO - [2018-11-08 12:19:22,275] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:19:22,855] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:22,881] {logging_mixin.py:95} INFO - [2018-11-08 12:19:22,880] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:19:22,882] {logging_mixin.py:95} INFO - [2018-11-08 12:19:22,881] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:14:22.881499+00:00

[2018-11-08 12:19:22,889] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.621 seconds
[2018-11-08 12:19:23,953] {jobs.py:385} INFO - Started process (PID=54727) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:28,959] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:19:28,964] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:19:28,967] {logging_mixin.py:95} INFO - [2018-11-08 12:19:28,966] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:19:30,077] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:30,130] {logging_mixin.py:95} INFO - [2018-11-08 12:19:30,130] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:19:30,132] {logging_mixin.py:95} INFO - [2018-11-08 12:19:30,131] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:14:30.131371+00:00

[2018-11-08 12:19:30,147] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.193 seconds
[2018-11-08 12:19:31,297] {jobs.py:385} INFO - Started process (PID=54729) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:36,307] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:19:36,310] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:19:36,314] {logging_mixin.py:95} INFO - [2018-11-08 12:19:36,312] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:19:36,809] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:36,832] {logging_mixin.py:95} INFO - [2018-11-08 12:19:36,832] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:19:36,834] {logging_mixin.py:95} INFO - [2018-11-08 12:19:36,833] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:14:36.833182+00:00

[2018-11-08 12:19:36,841] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.544 seconds
[2018-11-08 12:19:37,921] {jobs.py:385} INFO - Started process (PID=54730) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:42,934] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:19:42,936] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:19:42,946] {logging_mixin.py:95} INFO - [2018-11-08 12:19:42,940] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:19:43,712] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:43,742] {logging_mixin.py:95} INFO - [2018-11-08 12:19:43,741] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:19:43,744] {logging_mixin.py:95} INFO - [2018-11-08 12:19:43,742] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:14:43.742774+00:00

[2018-11-08 12:19:43,752] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.831 seconds
[2018-11-08 12:19:44,811] {jobs.py:385} INFO - Started process (PID=54733) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:49,818] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:19:49,821] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:19:49,823] {logging_mixin.py:95} INFO - [2018-11-08 12:19:49,822] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:19:50,260] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:50,291] {logging_mixin.py:95} INFO - [2018-11-08 12:19:50,291] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:19:50,293] {logging_mixin.py:95} INFO - [2018-11-08 12:19:50,292] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:14:50.292064+00:00

[2018-11-08 12:19:50,300] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.489 seconds
[2018-11-08 12:19:51,406] {jobs.py:385} INFO - Started process (PID=54737) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:56,414] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:19:56,419] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:19:56,422] {logging_mixin.py:95} INFO - [2018-11-08 12:19:56,421] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:19:57,204] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:19:57,244] {logging_mixin.py:95} INFO - [2018-11-08 12:19:57,243] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:19:57,249] {logging_mixin.py:95} INFO - [2018-11-08 12:19:57,245] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:14:57.245128+00:00

[2018-11-08 12:19:57,262] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.857 seconds
[2018-11-08 12:19:58,400] {jobs.py:385} INFO - Started process (PID=54739) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:03,407] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:20:03,412] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:20:03,414] {logging_mixin.py:95} INFO - [2018-11-08 12:20:03,413] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:20:03,960] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:03,991] {logging_mixin.py:95} INFO - [2018-11-08 12:20:03,989] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:20:03,995] {logging_mixin.py:95} INFO - [2018-11-08 12:20:03,993] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:15:03.993179+00:00

[2018-11-08 12:20:04,004] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.604 seconds
[2018-11-08 12:20:05,110] {jobs.py:385} INFO - Started process (PID=54741) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:10,115] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:20:10,116] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:20:10,117] {logging_mixin.py:95} INFO - [2018-11-08 12:20:10,117] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:20:10,728] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:10,772] {logging_mixin.py:95} INFO - [2018-11-08 12:20:10,771] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:20:10,774] {logging_mixin.py:95} INFO - [2018-11-08 12:20:10,772] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:15:10.772820+00:00

[2018-11-08 12:20:10,782] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.672 seconds
[2018-11-08 12:20:11,914] {jobs.py:385} INFO - Started process (PID=54742) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:16,920] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:20:16,922] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:20:16,923] {logging_mixin.py:95} INFO - [2018-11-08 12:20:16,923] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:20:17,404] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:17,428] {logging_mixin.py:95} INFO - [2018-11-08 12:20:17,428] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:20:17,430] {logging_mixin.py:95} INFO - [2018-11-08 12:20:17,429] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:15:17.429236+00:00

[2018-11-08 12:20:17,436] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.522 seconds
[2018-11-08 12:20:18,498] {jobs.py:385} INFO - Started process (PID=54745) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:23,506] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:20:23,510] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:20:23,512] {logging_mixin.py:95} INFO - [2018-11-08 12:20:23,511] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:20:23,901] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:23,925] {logging_mixin.py:95} INFO - [2018-11-08 12:20:23,925] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:20:23,927] {logging_mixin.py:95} INFO - [2018-11-08 12:20:23,926] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:15:23.926088+00:00

[2018-11-08 12:20:23,933] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.435 seconds
[2018-11-08 12:20:24,986] {jobs.py:385} INFO - Started process (PID=54746) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:29,994] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:20:29,998] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:20:30,000] {logging_mixin.py:95} INFO - [2018-11-08 12:20:30,000] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:20:30,518] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:30,542] {logging_mixin.py:95} INFO - [2018-11-08 12:20:30,541] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:20:30,543] {logging_mixin.py:95} INFO - [2018-11-08 12:20:30,542] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:15:30.542432+00:00

[2018-11-08 12:20:30,548] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.562 seconds
[2018-11-08 12:20:31,663] {jobs.py:385} INFO - Started process (PID=54748) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:36,668] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:20:36,670] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:20:36,672] {logging_mixin.py:95} INFO - [2018-11-08 12:20:36,671] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:20:37,146] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:37,175] {logging_mixin.py:95} INFO - [2018-11-08 12:20:37,174] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:20:37,176] {logging_mixin.py:95} INFO - [2018-11-08 12:20:37,175] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:15:37.175687+00:00

[2018-11-08 12:20:37,184] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.521 seconds
[2018-11-08 12:20:38,265] {jobs.py:385} INFO - Started process (PID=54749) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:43,273] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:20:43,278] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:20:43,280] {logging_mixin.py:95} INFO - [2018-11-08 12:20:43,280] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:20:44,050] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:44,077] {logging_mixin.py:95} INFO - [2018-11-08 12:20:44,077] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:20:44,080] {logging_mixin.py:95} INFO - [2018-11-08 12:20:44,078] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:15:44.078077+00:00

[2018-11-08 12:20:44,086] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.821 seconds
[2018-11-08 12:20:45,172] {jobs.py:385} INFO - Started process (PID=54751) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:50,179] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:20:50,184] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:20:50,186] {logging_mixin.py:95} INFO - [2018-11-08 12:20:50,186] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:20:52,184] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:52,229] {logging_mixin.py:95} INFO - [2018-11-08 12:20:52,228] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:20:52,232] {logging_mixin.py:95} INFO - [2018-11-08 12:20:52,230] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:15:52.230204+00:00

[2018-11-08 12:20:52,239] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.067 seconds
[2018-11-08 12:20:53,311] {jobs.py:385} INFO - Started process (PID=54757) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:58,319] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:20:58,324] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:20:58,326] {logging_mixin.py:95} INFO - [2018-11-08 12:20:58,326] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:20:59,444] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:20:59,471] {logging_mixin.py:95} INFO - [2018-11-08 12:20:59,471] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:20:59,473] {logging_mixin.py:95} INFO - [2018-11-08 12:20:59,472] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:15:59.472144+00:00

[2018-11-08 12:20:59,478] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.167 seconds
[2018-11-08 12:21:00,572] {jobs.py:385} INFO - Started process (PID=54758) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:05,578] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:21:05,581] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:21:05,582] {logging_mixin.py:95} INFO - [2018-11-08 12:21:05,581] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:21:06,321] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:06,355] {logging_mixin.py:95} INFO - [2018-11-08 12:21:06,353] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:21:06,361] {logging_mixin.py:95} INFO - [2018-11-08 12:21:06,358] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:16:06.358078+00:00

[2018-11-08 12:21:06,381] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.808 seconds
[2018-11-08 12:21:07,495] {jobs.py:385} INFO - Started process (PID=54765) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:12,500] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:21:12,503] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:21:12,505] {logging_mixin.py:95} INFO - [2018-11-08 12:21:12,504] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:21:12,945] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:12,969] {logging_mixin.py:95} INFO - [2018-11-08 12:21:12,968] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:21:12,971] {logging_mixin.py:95} INFO - [2018-11-08 12:21:12,969] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:16:12.969845+00:00

[2018-11-08 12:21:12,977] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.482 seconds
[2018-11-08 12:21:14,078] {jobs.py:385} INFO - Started process (PID=54766) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:19,089] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:21:19,091] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:21:19,092] {logging_mixin.py:95} INFO - [2018-11-08 12:21:19,092] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:21:19,604] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:19,633] {logging_mixin.py:95} INFO - [2018-11-08 12:21:19,632] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:21:19,635] {logging_mixin.py:95} INFO - [2018-11-08 12:21:19,634] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:16:19.634127+00:00

[2018-11-08 12:21:19,642] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.565 seconds
[2018-11-08 12:21:20,772] {jobs.py:385} INFO - Started process (PID=54768) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:25,786] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:21:25,790] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:21:25,791] {logging_mixin.py:95} INFO - [2018-11-08 12:21:25,791] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:21:26,235] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:26,257] {logging_mixin.py:95} INFO - [2018-11-08 12:21:26,257] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:21:26,259] {logging_mixin.py:95} INFO - [2018-11-08 12:21:26,258] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:16:26.258391+00:00

[2018-11-08 12:21:26,264] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.492 seconds
[2018-11-08 12:21:37,737] {jobs.py:385} INFO - Started process (PID=54797) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:42,748] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:21:42,751] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:21:42,753] {logging_mixin.py:95} INFO - [2018-11-08 12:21:42,752] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:21:43,230] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:43,255] {logging_mixin.py:95} INFO - [2018-11-08 12:21:43,254] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:21:43,257] {logging_mixin.py:95} INFO - [2018-11-08 12:21:43,256] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:16:43.256052+00:00

[2018-11-08 12:21:43,262] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.525 seconds
[2018-11-08 12:21:44,341] {jobs.py:385} INFO - Started process (PID=54799) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:49,350] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:21:49,356] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:21:49,358] {logging_mixin.py:95} INFO - [2018-11-08 12:21:49,357] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:21:50,156] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:50,188] {logging_mixin.py:95} INFO - [2018-11-08 12:21:50,188] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:21:50,190] {logging_mixin.py:95} INFO - [2018-11-08 12:21:50,189] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:16:50.189375+00:00

[2018-11-08 12:21:50,195] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.855 seconds
[2018-11-08 12:21:51,284] {jobs.py:385} INFO - Started process (PID=54803) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:56,292] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:21:56,295] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:21:56,296] {logging_mixin.py:95} INFO - [2018-11-08 12:21:56,296] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:21:56,756] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:21:56,783] {logging_mixin.py:95} INFO - [2018-11-08 12:21:56,782] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:21:56,785] {logging_mixin.py:95} INFO - [2018-11-08 12:21:56,783] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:16:56.783756+00:00

[2018-11-08 12:21:56,790] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.506 seconds
[2018-11-08 12:21:57,898] {jobs.py:385} INFO - Started process (PID=54805) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:02,907] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:22:02,909] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:22:02,910] {logging_mixin.py:95} INFO - [2018-11-08 12:22:02,909] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:22:03,303] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:03,330] {logging_mixin.py:95} INFO - [2018-11-08 12:22:03,330] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:22:03,332] {logging_mixin.py:95} INFO - [2018-11-08 12:22:03,331] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:17:03.331284+00:00

[2018-11-08 12:22:03,340] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.442 seconds
[2018-11-08 12:22:04,459] {jobs.py:385} INFO - Started process (PID=54807) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:09,467] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:22:09,471] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:22:09,473] {logging_mixin.py:95} INFO - [2018-11-08 12:22:09,472] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:22:10,042] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:10,065] {logging_mixin.py:95} INFO - [2018-11-08 12:22:10,065] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:22:10,067] {logging_mixin.py:95} INFO - [2018-11-08 12:22:10,066] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:17:10.066143+00:00

[2018-11-08 12:22:10,074] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.615 seconds
[2018-11-08 12:22:11,148] {jobs.py:385} INFO - Started process (PID=54809) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:16,156] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:22:16,159] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:22:16,163] {logging_mixin.py:95} INFO - [2018-11-08 12:22:16,161] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:22:16,558] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:16,578] {logging_mixin.py:95} INFO - [2018-11-08 12:22:16,578] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:22:16,580] {logging_mixin.py:95} INFO - [2018-11-08 12:22:16,579] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:17:16.579135+00:00

[2018-11-08 12:22:16,589] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.440 seconds
[2018-11-08 12:22:17,673] {jobs.py:385} INFO - Started process (PID=54811) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:22,683] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:22:22,689] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:22:22,690] {logging_mixin.py:95} INFO - [2018-11-08 12:22:22,689] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:22:23,319] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:23,343] {logging_mixin.py:95} INFO - [2018-11-08 12:22:23,343] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:22:23,344] {logging_mixin.py:95} INFO - [2018-11-08 12:22:23,343] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:17:23.343828+00:00

[2018-11-08 12:22:23,353] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.680 seconds
[2018-11-08 12:22:24,477] {jobs.py:385} INFO - Started process (PID=54812) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:29,483] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:22:29,485] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:22:29,486] {logging_mixin.py:95} INFO - [2018-11-08 12:22:29,485] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:22:29,958] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:29,980] {logging_mixin.py:95} INFO - [2018-11-08 12:22:29,979] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:22:29,982] {logging_mixin.py:95} INFO - [2018-11-08 12:22:29,981] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:17:29.981421+00:00

[2018-11-08 12:22:29,988] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.511 seconds
[2018-11-08 12:22:31,048] {jobs.py:385} INFO - Started process (PID=54814) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:36,058] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:22:36,064] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:22:36,066] {logging_mixin.py:95} INFO - [2018-11-08 12:22:36,065] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:22:36,576] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:36,601] {logging_mixin.py:95} INFO - [2018-11-08 12:22:36,601] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:22:36,603] {logging_mixin.py:95} INFO - [2018-11-08 12:22:36,602] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:17:36.602213+00:00

[2018-11-08 12:22:36,610] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.563 seconds
[2018-11-08 12:22:37,741] {jobs.py:385} INFO - Started process (PID=54815) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:42,748] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:22:42,752] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:22:42,755] {logging_mixin.py:95} INFO - [2018-11-08 12:22:42,754] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:22:43,584] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:43,627] {logging_mixin.py:95} INFO - [2018-11-08 12:22:43,627] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:22:43,630] {logging_mixin.py:95} INFO - [2018-11-08 12:22:43,628] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:17:43.628559+00:00

[2018-11-08 12:22:43,637] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.897 seconds
[2018-11-08 12:22:44,751] {jobs.py:385} INFO - Started process (PID=54831) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:49,764] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:22:49,768] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:22:49,771] {logging_mixin.py:95} INFO - [2018-11-08 12:22:49,770] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:22:50,625] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:50,655] {logging_mixin.py:95} INFO - [2018-11-08 12:22:50,654] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:22:50,656] {logging_mixin.py:95} INFO - [2018-11-08 12:22:50,655] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:17:50.655526+00:00

[2018-11-08 12:22:50,667] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.916 seconds
[2018-11-08 12:22:51,752] {jobs.py:385} INFO - Started process (PID=54835) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:22:56,760] {jobs.py:396} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 569, in __init__
    super(SchedulerJob, self).__init__(*args, **kwargs)
  File "<string>", line 6, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 107, in __init__
    self.hostname = get_hostname()
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/utils/net.py", line 45, in get_hostname
    return callable()
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 386, in helper
    scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)
  File "<string>", line 4, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0
[2018-11-08 12:23:04,231] {jobs.py:385} INFO - Started process (PID=54859) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:09,239] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:23:09,242] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:23:09,243] {logging_mixin.py:95} INFO - [2018-11-08 12:23:09,243] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:23:09,702] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:09,724] {logging_mixin.py:95} INFO - [2018-11-08 12:23:09,724] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:23:09,726] {logging_mixin.py:95} INFO - [2018-11-08 12:23:09,725] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:18:09.725317+00:00

[2018-11-08 12:23:09,731] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.500 seconds
[2018-11-08 12:23:10,787] {jobs.py:385} INFO - Started process (PID=54862) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:15,795] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:23:15,798] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:23:15,800] {logging_mixin.py:95} INFO - [2018-11-08 12:23:15,799] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:23:16,203] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:16,227] {logging_mixin.py:95} INFO - [2018-11-08 12:23:16,226] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:23:16,228] {logging_mixin.py:95} INFO - [2018-11-08 12:23:16,227] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:18:16.227489+00:00

[2018-11-08 12:23:16,234] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.448 seconds
[2018-11-08 12:23:17,392] {jobs.py:385} INFO - Started process (PID=54867) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:22,399] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:23:22,403] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:23:22,405] {logging_mixin.py:95} INFO - [2018-11-08 12:23:22,404] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:23:22,845] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:22,874] {logging_mixin.py:95} INFO - [2018-11-08 12:23:22,874] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:23:22,877] {logging_mixin.py:95} INFO - [2018-11-08 12:23:22,876] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:18:22.875980+00:00

[2018-11-08 12:23:22,883] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.492 seconds
[2018-11-08 12:23:23,955] {jobs.py:385} INFO - Started process (PID=54870) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:28,963] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:23:28,966] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:23:28,967] {logging_mixin.py:95} INFO - [2018-11-08 12:23:28,966] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:23:29,634] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:29,667] {logging_mixin.py:95} INFO - [2018-11-08 12:23:29,667] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:23:29,671] {logging_mixin.py:95} INFO - [2018-11-08 12:23:29,670] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:18:29.670045+00:00

[2018-11-08 12:23:29,680] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.726 seconds
[2018-11-08 12:23:30,735] {jobs.py:385} INFO - Started process (PID=54883) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:35,744] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:23:35,749] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:23:35,752] {logging_mixin.py:95} INFO - [2018-11-08 12:23:35,751] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:23:36,208] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:36,246] {logging_mixin.py:95} INFO - [2018-11-08 12:23:36,245] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:23:36,247] {logging_mixin.py:95} INFO - [2018-11-08 12:23:36,246] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:18:36.246722+00:00

[2018-11-08 12:23:36,253] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.518 seconds
[2018-11-08 12:23:37,344] {jobs.py:385} INFO - Started process (PID=54892) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:42,349] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:23:42,351] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:23:42,353] {logging_mixin.py:95} INFO - [2018-11-08 12:23:42,353] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:23:42,931] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:42,958] {logging_mixin.py:95} INFO - [2018-11-08 12:23:42,957] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:23:42,960] {logging_mixin.py:95} INFO - [2018-11-08 12:23:42,958] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:18:42.958947+00:00

[2018-11-08 12:23:42,968] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.624 seconds
[2018-11-08 12:23:44,040] {jobs.py:385} INFO - Started process (PID=54895) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:49,048] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:23:49,049] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:23:49,050] {logging_mixin.py:95} INFO - [2018-11-08 12:23:49,050] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:23:49,548] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:49,574] {logging_mixin.py:95} INFO - [2018-11-08 12:23:49,573] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:23:49,575] {logging_mixin.py:95} INFO - [2018-11-08 12:23:49,574] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:18:49.574519+00:00

[2018-11-08 12:23:49,582] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.543 seconds
[2018-11-08 12:23:50,648] {jobs.py:385} INFO - Started process (PID=54899) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:55,660] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:23:55,662] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:23:55,666] {logging_mixin.py:95} INFO - [2018-11-08 12:23:55,665] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:23:56,152] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:23:56,176] {logging_mixin.py:95} INFO - [2018-11-08 12:23:56,175] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:23:56,177] {logging_mixin.py:95} INFO - [2018-11-08 12:23:56,176] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:18:56.176723+00:00

[2018-11-08 12:23:56,184] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.536 seconds
[2018-11-08 12:23:57,232] {jobs.py:385} INFO - Started process (PID=54901) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:02,238] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:24:02,239] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:24:02,240] {logging_mixin.py:95} INFO - [2018-11-08 12:24:02,240] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:24:02,645] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:02,671] {logging_mixin.py:95} INFO - [2018-11-08 12:24:02,671] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:24:02,675] {logging_mixin.py:95} INFO - [2018-11-08 12:24:02,672] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:19:02.672441+00:00

[2018-11-08 12:24:02,684] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.452 seconds
[2018-11-08 12:24:03,816] {jobs.py:385} INFO - Started process (PID=54903) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:08,826] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:24:08,828] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:24:08,829] {logging_mixin.py:95} INFO - [2018-11-08 12:24:08,829] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:24:09,262] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:09,284] {logging_mixin.py:95} INFO - [2018-11-08 12:24:09,284] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:24:09,286] {logging_mixin.py:95} INFO - [2018-11-08 12:24:09,284] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:19:09.284813+00:00

[2018-11-08 12:24:09,293] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.477 seconds
[2018-11-08 12:24:10,389] {jobs.py:385} INFO - Started process (PID=54904) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:15,398] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:24:15,400] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:24:15,402] {logging_mixin.py:95} INFO - [2018-11-08 12:24:15,401] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:24:16,106] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:16,144] {logging_mixin.py:95} INFO - [2018-11-08 12:24:16,144] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:24:16,147] {logging_mixin.py:95} INFO - [2018-11-08 12:24:16,145] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:19:16.145387+00:00

[2018-11-08 12:24:16,160] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.771 seconds
[2018-11-08 12:24:17,291] {jobs.py:385} INFO - Started process (PID=54907) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:22,304] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:24:22,309] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:24:22,313] {logging_mixin.py:95} INFO - [2018-11-08 12:24:22,312] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:24:23,039] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:23,101] {logging_mixin.py:95} INFO - [2018-11-08 12:24:23,100] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:24:23,106] {logging_mixin.py:95} INFO - [2018-11-08 12:24:23,102] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:19:23.102680+00:00

[2018-11-08 12:24:23,120] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.830 seconds
[2018-11-08 12:24:24,184] {jobs.py:385} INFO - Started process (PID=54908) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:29,193] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:24:29,195] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:24:29,196] {logging_mixin.py:95} INFO - [2018-11-08 12:24:29,195] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:24:29,620] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:29,647] {logging_mixin.py:95} INFO - [2018-11-08 12:24:29,646] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:24:29,649] {logging_mixin.py:95} INFO - [2018-11-08 12:24:29,647] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:19:29.647796+00:00

[2018-11-08 12:24:29,657] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.473 seconds
[2018-11-08 12:24:30,763] {jobs.py:385} INFO - Started process (PID=54910) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:35,770] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:24:35,775] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:24:35,782] {logging_mixin.py:95} INFO - [2018-11-08 12:24:35,780] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:24:37,240] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:37,310] {logging_mixin.py:95} INFO - [2018-11-08 12:24:37,307] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:24:37,314] {logging_mixin.py:95} INFO - [2018-11-08 12:24:37,311] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:19:37.311771+00:00

[2018-11-08 12:24:37,347] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.584 seconds
[2018-11-08 12:24:38,448] {jobs.py:385} INFO - Started process (PID=54911) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:43,454] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:24:43,457] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:24:43,460] {logging_mixin.py:95} INFO - [2018-11-08 12:24:43,458] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:24:44,224] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:44,248] {logging_mixin.py:95} INFO - [2018-11-08 12:24:44,248] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:24:44,250] {logging_mixin.py:95} INFO - [2018-11-08 12:24:44,249] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:19:44.249214+00:00

[2018-11-08 12:24:44,257] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.809 seconds
[2018-11-08 12:24:45,356] {jobs.py:385} INFO - Started process (PID=54913) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:50,366] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:24:50,373] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:24:50,374] {logging_mixin.py:95} INFO - [2018-11-08 12:24:50,374] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:24:50,906] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:50,927] {logging_mixin.py:95} INFO - [2018-11-08 12:24:50,927] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:24:50,929] {logging_mixin.py:95} INFO - [2018-11-08 12:24:50,928] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:19:50.928462+00:00

[2018-11-08 12:24:50,935] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.579 seconds
[2018-11-08 12:24:51,987] {jobs.py:385} INFO - Started process (PID=54918) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:56,997] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:24:57,000] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:24:57,003] {logging_mixin.py:95} INFO - [2018-11-08 12:24:57,001] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:24:57,537] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:24:57,565] {logging_mixin.py:95} INFO - [2018-11-08 12:24:57,565] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:24:57,567] {logging_mixin.py:95} INFO - [2018-11-08 12:24:57,566] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:19:57.566601+00:00

[2018-11-08 12:24:57,573] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.586 seconds
[2018-11-08 12:24:58,667] {jobs.py:385} INFO - Started process (PID=54920) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:03,676] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:25:03,678] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:25:03,684] {logging_mixin.py:95} INFO - [2018-11-08 12:25:03,682] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:25:04,209] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:04,232] {logging_mixin.py:95} INFO - [2018-11-08 12:25:04,232] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:25:04,234] {logging_mixin.py:95} INFO - [2018-11-08 12:25:04,233] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:20:04.233324+00:00

[2018-11-08 12:25:04,239] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.572 seconds
[2018-11-08 12:25:05,368] {jobs.py:385} INFO - Started process (PID=54922) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:10,376] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:25:10,382] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:25:10,383] {logging_mixin.py:95} INFO - [2018-11-08 12:25:10,383] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:25:11,260] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:11,296] {logging_mixin.py:95} INFO - [2018-11-08 12:25:11,295] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:25:11,299] {logging_mixin.py:95} INFO - [2018-11-08 12:25:11,297] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:20:11.297543+00:00

[2018-11-08 12:25:11,310] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.942 seconds
[2018-11-08 12:25:12,409] {jobs.py:385} INFO - Started process (PID=54926) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:17,421] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:25:17,425] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:25:17,427] {logging_mixin.py:95} INFO - [2018-11-08 12:25:17,426] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:25:17,854] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:17,878] {logging_mixin.py:95} INFO - [2018-11-08 12:25:17,878] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:25:17,880] {logging_mixin.py:95} INFO - [2018-11-08 12:25:17,879] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:20:17.879278+00:00

[2018-11-08 12:25:17,886] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.477 seconds
[2018-11-08 12:25:19,025] {jobs.py:385} INFO - Started process (PID=54929) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:24,041] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:25:24,042] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:25:24,044] {logging_mixin.py:95} INFO - [2018-11-08 12:25:24,043] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:25:24,491] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:24,520] {logging_mixin.py:95} INFO - [2018-11-08 12:25:24,519] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:25:24,521] {logging_mixin.py:95} INFO - [2018-11-08 12:25:24,520] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:20:24.520478+00:00

[2018-11-08 12:25:24,528] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.503 seconds
[2018-11-08 12:25:25,620] {jobs.py:385} INFO - Started process (PID=54934) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:30,628] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:25:30,633] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:25:30,635] {logging_mixin.py:95} INFO - [2018-11-08 12:25:30,634] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:25:31,217] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:31,240] {logging_mixin.py:95} INFO - [2018-11-08 12:25:31,239] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:25:31,241] {logging_mixin.py:95} INFO - [2018-11-08 12:25:31,240] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:20:31.240549+00:00

[2018-11-08 12:25:31,246] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.625 seconds
[2018-11-08 12:25:32,321] {jobs.py:385} INFO - Started process (PID=54936) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:37,327] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:25:37,329] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:25:37,331] {logging_mixin.py:95} INFO - [2018-11-08 12:25:37,330] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:25:37,741] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:37,763] {logging_mixin.py:95} INFO - [2018-11-08 12:25:37,763] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:25:37,765] {logging_mixin.py:95} INFO - [2018-11-08 12:25:37,764] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:20:37.764377+00:00

[2018-11-08 12:25:37,773] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.452 seconds
[2018-11-08 12:25:38,911] {jobs.py:385} INFO - Started process (PID=54937) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:43,919] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:25:43,928] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:25:43,929] {logging_mixin.py:95} INFO - [2018-11-08 12:25:43,928] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:25:44,833] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:44,854] {logging_mixin.py:95} INFO - [2018-11-08 12:25:44,854] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:25:44,856] {logging_mixin.py:95} INFO - [2018-11-08 12:25:44,855] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:20:44.855245+00:00

[2018-11-08 12:25:44,861] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.949 seconds
[2018-11-08 12:25:45,938] {jobs.py:385} INFO - Started process (PID=54951) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:50,947] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:25:50,951] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:25:50,952] {logging_mixin.py:95} INFO - [2018-11-08 12:25:50,952] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:25:51,454] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:51,474] {logging_mixin.py:95} INFO - [2018-11-08 12:25:51,474] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:25:51,476] {logging_mixin.py:95} INFO - [2018-11-08 12:25:51,475] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:20:51.475125+00:00

[2018-11-08 12:25:51,481] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.543 seconds
[2018-11-08 12:25:52,540] {jobs.py:385} INFO - Started process (PID=54953) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:57,546] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:25:57,547] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:25:57,548] {logging_mixin.py:95} INFO - [2018-11-08 12:25:57,548] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:25:58,176] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:25:58,222] {logging_mixin.py:95} INFO - [2018-11-08 12:25:58,221] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:25:58,224] {logging_mixin.py:95} INFO - [2018-11-08 12:25:58,223] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:20:58.223164+00:00

[2018-11-08 12:25:58,230] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.690 seconds
[2018-11-08 12:25:59,343] {jobs.py:385} INFO - Started process (PID=54964) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:04,350] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:26:04,356] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:26:04,358] {logging_mixin.py:95} INFO - [2018-11-08 12:26:04,357] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:26:04,810] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:04,831] {logging_mixin.py:95} INFO - [2018-11-08 12:26:04,830] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:26:04,832] {logging_mixin.py:95} INFO - [2018-11-08 12:26:04,831] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:21:04.831777+00:00

[2018-11-08 12:26:04,838] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.495 seconds
[2018-11-08 12:26:05,906] {jobs.py:385} INFO - Started process (PID=54966) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:10,911] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:26:10,914] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:26:10,916] {logging_mixin.py:95} INFO - [2018-11-08 12:26:10,915] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:26:11,350] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:11,371] {logging_mixin.py:95} INFO - [2018-11-08 12:26:11,371] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:26:11,373] {logging_mixin.py:95} INFO - [2018-11-08 12:26:11,372] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:21:11.372199+00:00

[2018-11-08 12:26:11,378] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.473 seconds
[2018-11-08 12:26:12,470] {jobs.py:385} INFO - Started process (PID=54967) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:17,513] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:26:17,563] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:26:17,568] {logging_mixin.py:95} INFO - [2018-11-08 12:26:17,566] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:26:22,694] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:22,738] {logging_mixin.py:95} INFO - [2018-11-08 12:26:22,738] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:26:22,743] {logging_mixin.py:95} INFO - [2018-11-08 12:26:22,739] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:21:22.739805+00:00

[2018-11-08 12:26:22,754] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 10.284 seconds
[2018-11-08 12:26:24,053] {jobs.py:385} INFO - Started process (PID=54972) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:29,062] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:26:29,076] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:26:29,078] {logging_mixin.py:95} INFO - [2018-11-08 12:26:29,077] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:26:29,608] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:29,630] {logging_mixin.py:95} INFO - [2018-11-08 12:26:29,629] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:26:29,632] {logging_mixin.py:95} INFO - [2018-11-08 12:26:29,631] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:21:29.631026+00:00

[2018-11-08 12:26:29,637] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.584 seconds
[2018-11-08 12:26:30,720] {jobs.py:385} INFO - Started process (PID=54976) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:35,729] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:26:35,732] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:26:35,733] {logging_mixin.py:95} INFO - [2018-11-08 12:26:35,733] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:26:36,122] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:36,144] {logging_mixin.py:95} INFO - [2018-11-08 12:26:36,143] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:26:36,146] {logging_mixin.py:95} INFO - [2018-11-08 12:26:36,145] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:21:36.144988+00:00

[2018-11-08 12:26:36,151] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.431 seconds
[2018-11-08 12:26:37,291] {jobs.py:385} INFO - Started process (PID=54978) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:42,302] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:26:42,305] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:26:42,307] {logging_mixin.py:95} INFO - [2018-11-08 12:26:42,306] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:26:42,738] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:42,760] {logging_mixin.py:95} INFO - [2018-11-08 12:26:42,759] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:26:42,761] {logging_mixin.py:95} INFO - [2018-11-08 12:26:42,760] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:21:42.760619+00:00

[2018-11-08 12:26:42,766] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.475 seconds
[2018-11-08 12:26:43,916] {jobs.py:385} INFO - Started process (PID=54995) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:48,926] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:26:48,931] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:26:48,932] {logging_mixin.py:95} INFO - [2018-11-08 12:26:48,932] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:26:49,302] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:49,328] {logging_mixin.py:95} INFO - [2018-11-08 12:26:49,328] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:26:49,330] {logging_mixin.py:95} INFO - [2018-11-08 12:26:49,329] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:21:49.329194+00:00

[2018-11-08 12:26:49,336] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.420 seconds
[2018-11-08 12:26:50,448] {jobs.py:385} INFO - Started process (PID=55007) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:55,457] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 12:26:55,459] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 12:26:55,460] {logging_mixin.py:95} INFO - [2018-11-08 12:26:55,460] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 12:26:55,958] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 12:26:55,978] {logging_mixin.py:95} INFO - [2018-11-08 12:26:55,978] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 12:26:55,980] {logging_mixin.py:95} INFO - [2018-11-08 12:26:55,979] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 14:21:55.979136+00:00

[2018-11-08 12:26:55,985] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.537 seconds
[2018-11-08 12:26:57,041] {jobs.py:385} INFO - Started process (PID=55011) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:05:58,228] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 13:05:58,308] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 13:05:58,323] {logging_mixin.py:95} INFO - [2018-11-08 13:05:58,309] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 13:06:00,286] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:00,334] {logging_mixin.py:95} INFO - [2018-11-08 13:06:00,334] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 13:06:00,337] {logging_mixin.py:95} INFO - [2018-11-08 13:06:00,335] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 15:01:00.335299+00:00

[2018-11-08 13:06:00,345] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 2343.304 seconds
[2018-11-08 13:06:01,442] {jobs.py:385} INFO - Started process (PID=55021) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:06,447] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 13:06:06,452] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 13:06:06,453] {logging_mixin.py:95} INFO - [2018-11-08 13:06:06,453] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 13:06:06,882] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:06,900] {logging_mixin.py:95} INFO - [2018-11-08 13:06:06,900] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 13:06:06,903] {logging_mixin.py:95} INFO - [2018-11-08 13:06:06,901] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 15:01:06.901314+00:00

[2018-11-08 13:06:06,908] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.466 seconds
[2018-11-08 13:06:08,035] {jobs.py:385} INFO - Started process (PID=55023) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:12,939] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 13:06:12,945] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 13:06:12,948] {logging_mixin.py:95} INFO - [2018-11-08 13:06:12,947] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 13:06:13,388] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:13,412] {logging_mixin.py:95} INFO - [2018-11-08 13:06:13,412] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 13:06:13,414] {logging_mixin.py:95} INFO - [2018-11-08 13:06:13,413] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 15:01:13.413194+00:00

[2018-11-08 13:06:13,418] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.384 seconds
[2018-11-08 13:06:14,543] {jobs.py:385} INFO - Started process (PID=55028) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:19,558] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 13:06:19,561] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 13:06:19,563] {logging_mixin.py:95} INFO - [2018-11-08 13:06:19,562] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 13:06:20,068] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:20,087] {logging_mixin.py:95} INFO - [2018-11-08 13:06:20,087] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 13:06:20,090] {logging_mixin.py:95} INFO - [2018-11-08 13:06:20,088] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 15:01:20.088579+00:00

[2018-11-08 13:06:20,096] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.553 seconds
[2018-11-08 13:06:21,186] {jobs.py:385} INFO - Started process (PID=55031) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:26,196] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 13:06:26,198] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 13:06:26,199] {logging_mixin.py:95} INFO - [2018-11-08 13:06:26,199] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 13:06:26,610] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:26,633] {logging_mixin.py:95} INFO - [2018-11-08 13:06:26,633] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 13:06:26,635] {logging_mixin.py:95} INFO - [2018-11-08 13:06:26,634] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 15:01:26.634011+00:00

[2018-11-08 13:06:26,641] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.455 seconds
[2018-11-08 13:06:27,755] {jobs.py:385} INFO - Started process (PID=55034) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:32,768] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 13:06:32,771] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 13:06:32,773] {logging_mixin.py:95} INFO - [2018-11-08 13:06:32,772] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 13:06:33,152] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:33,182] {logging_mixin.py:95} INFO - [2018-11-08 13:06:33,182] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 13:06:33,184] {logging_mixin.py:95} INFO - [2018-11-08 13:06:33,183] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 15:01:33.183166+00:00

[2018-11-08 13:06:33,189] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.434 seconds
[2018-11-08 13:06:34,318] {jobs.py:385} INFO - Started process (PID=55037) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:39,327] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 13:06:39,331] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 13:06:39,333] {logging_mixin.py:95} INFO - [2018-11-08 13:06:39,333] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 13:06:39,764] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:39,784] {logging_mixin.py:95} INFO - [2018-11-08 13:06:39,784] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 13:06:39,786] {logging_mixin.py:95} INFO - [2018-11-08 13:06:39,785] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 15:01:39.785200+00:00

[2018-11-08 13:06:39,791] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.473 seconds
[2018-11-08 13:06:40,924] {jobs.py:385} INFO - Started process (PID=55042) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:45,934] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 13:06:45,936] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 13:06:45,939] {logging_mixin.py:95} INFO - [2018-11-08 13:06:45,938] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 13:06:46,314] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:46,337] {logging_mixin.py:95} INFO - [2018-11-08 13:06:46,336] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 13:06:46,339] {logging_mixin.py:95} INFO - [2018-11-08 13:06:46,338] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 15:01:46.338001+00:00

[2018-11-08 13:06:46,343] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.419 seconds
[2018-11-08 13:06:47,439] {jobs.py:385} INFO - Started process (PID=55043) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:52,453] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 13:06:52,458] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 13:06:52,461] {logging_mixin.py:95} INFO - [2018-11-08 13:06:52,459] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 13:06:52,904] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 13:06:52,925] {logging_mixin.py:95} INFO - [2018-11-08 13:06:52,924] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 13:06:52,928] {logging_mixin.py:95} INFO - [2018-11-08 13:06:52,926] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 15:01:52.926132+00:00

[2018-11-08 13:06:52,934] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.495 seconds
[2018-11-08 13:06:54,015] {jobs.py:385} INFO - Started process (PID=55045) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:40:14,890] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:40:14,893] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:40:14,895] {logging_mixin.py:95} INFO - [2018-11-08 14:40:14,894] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:40:21,256] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:40:21,610] {logging_mixin.py:95} INFO - [2018-11-08 14:40:21,609] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:40:21,612] {logging_mixin.py:95} INFO - [2018-11-08 14:40:21,610] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:35:21.610788+00:00

[2018-11-08 14:40:21,908] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5607.893 seconds
[2018-11-08 14:40:23,184] {jobs.py:385} INFO - Started process (PID=55060) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:40:28,231] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:40:28,241] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:40:28,243] {logging_mixin.py:95} INFO - [2018-11-08 14:40:28,242] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:40:29,848] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:40:29,886] {logging_mixin.py:95} INFO - [2018-11-08 14:40:29,886] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:40:29,889] {logging_mixin.py:95} INFO - [2018-11-08 14:40:29,887] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:35:29.887685+00:00

[2018-11-08 14:40:29,899] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.715 seconds
[2018-11-08 14:40:31,101] {jobs.py:385} INFO - Started process (PID=55073) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:40:36,115] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:40:36,127] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:40:36,133] {logging_mixin.py:95} INFO - [2018-11-08 14:40:36,131] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:40:38,287] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:40:38,348] {logging_mixin.py:95} INFO - [2018-11-08 14:40:38,348] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:40:38,351] {logging_mixin.py:95} INFO - [2018-11-08 14:40:38,349] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:35:38.349497+00:00

[2018-11-08 14:40:38,365] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.264 seconds
[2018-11-08 14:40:39,566] {jobs.py:385} INFO - Started process (PID=55090) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:40:44,578] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:40:44,581] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:40:44,583] {logging_mixin.py:95} INFO - [2018-11-08 14:40:44,582] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:40:45,036] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:40:45,057] {logging_mixin.py:95} INFO - [2018-11-08 14:40:45,057] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:40:45,059] {logging_mixin.py:95} INFO - [2018-11-08 14:40:45,058] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:35:45.058139+00:00

[2018-11-08 14:40:45,064] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.497 seconds
[2018-11-08 14:40:46,217] {jobs.py:385} INFO - Started process (PID=55133) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:40:51,249] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:40:51,254] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:40:51,258] {logging_mixin.py:95} INFO - [2018-11-08 14:40:51,256] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:40:52,186] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:40:52,213] {logging_mixin.py:95} INFO - [2018-11-08 14:40:52,212] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:40:52,214] {logging_mixin.py:95} INFO - [2018-11-08 14:40:52,213] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:35:52.213772+00:00

[2018-11-08 14:40:52,223] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.006 seconds
[2018-11-08 14:40:53,322] {jobs.py:385} INFO - Started process (PID=55141) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:40:58,334] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:40:58,346] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:40:58,348] {logging_mixin.py:95} INFO - [2018-11-08 14:40:58,347] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:41:00,583] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:00,892] {logging_mixin.py:95} INFO - [2018-11-08 14:41:00,892] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:41:00,908] {logging_mixin.py:95} INFO - [2018-11-08 14:41:00,893] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:36:00.893470+00:00

[2018-11-08 14:41:00,937] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.616 seconds
[2018-11-08 14:41:02,209] {jobs.py:385} INFO - Started process (PID=55148) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:07,225] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:41:07,230] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:41:07,232] {logging_mixin.py:95} INFO - [2018-11-08 14:41:07,231] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:41:07,928] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:07,961] {logging_mixin.py:95} INFO - [2018-11-08 14:41:07,960] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:41:07,962] {logging_mixin.py:95} INFO - [2018-11-08 14:41:07,961] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:36:07.961609+00:00

[2018-11-08 14:41:07,975] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.766 seconds
[2018-11-08 14:41:09,079] {jobs.py:385} INFO - Started process (PID=55149) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:14,088] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:41:14,095] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:41:14,097] {logging_mixin.py:95} INFO - [2018-11-08 14:41:14,096] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:41:15,237] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:15,285] {logging_mixin.py:95} INFO - [2018-11-08 14:41:15,284] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:41:15,286] {logging_mixin.py:95} INFO - [2018-11-08 14:41:15,285] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:36:15.285714+00:00

[2018-11-08 14:41:15,293] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.215 seconds
[2018-11-08 14:41:16,434] {jobs.py:385} INFO - Started process (PID=55156) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:21,443] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:41:21,447] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:41:21,449] {logging_mixin.py:95} INFO - [2018-11-08 14:41:21,449] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:41:22,074] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:22,144] {logging_mixin.py:95} INFO - [2018-11-08 14:41:22,144] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:41:22,146] {logging_mixin.py:95} INFO - [2018-11-08 14:41:22,145] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:36:22.145530+00:00

[2018-11-08 14:41:22,161] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.727 seconds
[2018-11-08 14:41:23,316] {jobs.py:385} INFO - Started process (PID=55165) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:28,325] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:41:28,328] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:41:28,330] {logging_mixin.py:95} INFO - [2018-11-08 14:41:28,329] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:41:28,724] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:28,743] {logging_mixin.py:95} INFO - [2018-11-08 14:41:28,742] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:41:28,744] {logging_mixin.py:95} INFO - [2018-11-08 14:41:28,743] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:36:28.743431+00:00

[2018-11-08 14:41:28,749] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.433 seconds
[2018-11-08 14:41:29,880] {jobs.py:385} INFO - Started process (PID=55174) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:34,890] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:41:34,898] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:41:34,901] {logging_mixin.py:95} INFO - [2018-11-08 14:41:34,900] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:41:35,758] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:35,813] {logging_mixin.py:95} INFO - [2018-11-08 14:41:35,810] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:41:35,826] {logging_mixin.py:95} INFO - [2018-11-08 14:41:35,814] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:36:35.814805+00:00

[2018-11-08 14:41:35,836] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.956 seconds
[2018-11-08 14:41:36,965] {jobs.py:385} INFO - Started process (PID=55183) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:41,985] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:41:42,009] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:41:42,020] {logging_mixin.py:95} INFO - [2018-11-08 14:41:42,017] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:41:43,279] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:43,319] {logging_mixin.py:95} INFO - [2018-11-08 14:41:43,318] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:41:43,321] {logging_mixin.py:95} INFO - [2018-11-08 14:41:43,319] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:36:43.319761+00:00

[2018-11-08 14:41:43,345] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.380 seconds
[2018-11-08 14:41:44,472] {jobs.py:385} INFO - Started process (PID=55186) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:49,482] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:41:49,485] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:41:49,487] {logging_mixin.py:95} INFO - [2018-11-08 14:41:49,487] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:41:50,148] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:50,169] {logging_mixin.py:95} INFO - [2018-11-08 14:41:50,168] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:41:50,170] {logging_mixin.py:95} INFO - [2018-11-08 14:41:50,169] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:36:50.169560+00:00

[2018-11-08 14:41:50,175] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.703 seconds
[2018-11-08 14:41:51,225] {jobs.py:385} INFO - Started process (PID=55190) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:56,235] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:41:56,238] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:41:56,241] {logging_mixin.py:95} INFO - [2018-11-08 14:41:56,239] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:41:56,767] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:41:56,789] {logging_mixin.py:95} INFO - [2018-11-08 14:41:56,789] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:41:56,791] {logging_mixin.py:95} INFO - [2018-11-08 14:41:56,790] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:36:56.790180+00:00

[2018-11-08 14:41:56,796] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.571 seconds
[2018-11-08 14:41:57,895] {jobs.py:385} INFO - Started process (PID=55191) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:02,906] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:42:02,909] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:42:02,911] {logging_mixin.py:95} INFO - [2018-11-08 14:42:02,910] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:42:03,517] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:03,548] {logging_mixin.py:95} INFO - [2018-11-08 14:42:03,547] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:42:03,550] {logging_mixin.py:95} INFO - [2018-11-08 14:42:03,548] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:37:03.548916+00:00

[2018-11-08 14:42:03,559] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.664 seconds
[2018-11-08 14:42:04,678] {jobs.py:385} INFO - Started process (PID=55197) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:09,691] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:42:09,698] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:42:09,704] {logging_mixin.py:95} INFO - [2018-11-08 14:42:09,703] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:42:10,333] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:10,361] {logging_mixin.py:95} INFO - [2018-11-08 14:42:10,361] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:42:10,363] {logging_mixin.py:95} INFO - [2018-11-08 14:42:10,362] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:37:10.362280+00:00

[2018-11-08 14:42:10,371] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.693 seconds
[2018-11-08 14:42:11,444] {jobs.py:385} INFO - Started process (PID=55204) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:16,458] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:42:16,460] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:42:16,461] {logging_mixin.py:95} INFO - [2018-11-08 14:42:16,461] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:42:16,904] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:16,928] {logging_mixin.py:95} INFO - [2018-11-08 14:42:16,927] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:42:16,930] {logging_mixin.py:95} INFO - [2018-11-08 14:42:16,928] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:37:16.928808+00:00

[2018-11-08 14:42:16,936] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.492 seconds
[2018-11-08 14:42:18,021] {jobs.py:385} INFO - Started process (PID=55207) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:23,029] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:42:23,032] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:42:23,034] {logging_mixin.py:95} INFO - [2018-11-08 14:42:23,033] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:42:23,500] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:23,519] {logging_mixin.py:95} INFO - [2018-11-08 14:42:23,518] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:42:23,521] {logging_mixin.py:95} INFO - [2018-11-08 14:42:23,519] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:37:23.519886+00:00

[2018-11-08 14:42:23,526] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.504 seconds
[2018-11-08 14:42:24,571] {jobs.py:385} INFO - Started process (PID=55217) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:29,585] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:42:29,592] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:42:29,602] {logging_mixin.py:95} INFO - [2018-11-08 14:42:29,596] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:42:31,606] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:31,646] {logging_mixin.py:95} INFO - [2018-11-08 14:42:31,646] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:42:31,648] {logging_mixin.py:95} INFO - [2018-11-08 14:42:31,647] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:37:31.647184+00:00

[2018-11-08 14:42:31,656] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.085 seconds
[2018-11-08 14:42:32,732] {jobs.py:385} INFO - Started process (PID=55223) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:37,741] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:42:37,746] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:42:37,748] {logging_mixin.py:95} INFO - [2018-11-08 14:42:37,747] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:42:38,269] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:38,295] {logging_mixin.py:95} INFO - [2018-11-08 14:42:38,294] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:42:38,297] {logging_mixin.py:95} INFO - [2018-11-08 14:42:38,296] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:37:38.296519+00:00

[2018-11-08 14:42:38,302] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.571 seconds
[2018-11-08 14:42:39,407] {jobs.py:385} INFO - Started process (PID=55227) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:44,413] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:42:44,415] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:42:44,416] {logging_mixin.py:95} INFO - [2018-11-08 14:42:44,416] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:42:44,874] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:44,900] {logging_mixin.py:95} INFO - [2018-11-08 14:42:44,899] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:42:44,902] {logging_mixin.py:95} INFO - [2018-11-08 14:42:44,901] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:37:44.901005+00:00

[2018-11-08 14:42:44,909] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.503 seconds
[2018-11-08 14:42:45,992] {jobs.py:385} INFO - Started process (PID=55233) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:51,004] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:42:51,012] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:42:51,014] {logging_mixin.py:95} INFO - [2018-11-08 14:42:51,013] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:42:51,427] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:51,452] {logging_mixin.py:95} INFO - [2018-11-08 14:42:51,452] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:42:51,454] {logging_mixin.py:95} INFO - [2018-11-08 14:42:51,453] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:37:51.453378+00:00

[2018-11-08 14:42:51,460] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.468 seconds
[2018-11-08 14:42:52,597] {jobs.py:385} INFO - Started process (PID=55238) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:57,605] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:42:57,608] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:42:57,610] {logging_mixin.py:95} INFO - [2018-11-08 14:42:57,609] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:42:58,048] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:42:58,067] {logging_mixin.py:95} INFO - [2018-11-08 14:42:58,067] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:42:58,068] {logging_mixin.py:95} INFO - [2018-11-08 14:42:58,067] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:37:58.067774+00:00

[2018-11-08 14:42:58,073] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.476 seconds
[2018-11-08 14:42:59,187] {jobs.py:385} INFO - Started process (PID=55245) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:04,197] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:43:04,199] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:43:04,200] {logging_mixin.py:95} INFO - [2018-11-08 14:43:04,200] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:43:04,697] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:04,728] {logging_mixin.py:95} INFO - [2018-11-08 14:43:04,727] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:43:04,729] {logging_mixin.py:95} INFO - [2018-11-08 14:43:04,728] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:38:04.728725+00:00

[2018-11-08 14:43:04,739] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.552 seconds
[2018-11-08 14:43:05,873] {jobs.py:385} INFO - Started process (PID=55248) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:10,887] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:43:10,895] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:43:10,897] {logging_mixin.py:95} INFO - [2018-11-08 14:43:10,896] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:43:11,304] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:11,329] {logging_mixin.py:95} INFO - [2018-11-08 14:43:11,329] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:43:11,331] {logging_mixin.py:95} INFO - [2018-11-08 14:43:11,330] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:38:11.330348+00:00

[2018-11-08 14:43:11,338] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.465 seconds
[2018-11-08 14:43:12,453] {jobs.py:385} INFO - Started process (PID=55251) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:17,461] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:43:17,464] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:43:17,466] {logging_mixin.py:95} INFO - [2018-11-08 14:43:17,465] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:43:18,048] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:18,075] {logging_mixin.py:95} INFO - [2018-11-08 14:43:18,075] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:43:18,077] {logging_mixin.py:95} INFO - [2018-11-08 14:43:18,076] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:38:18.076229+00:00

[2018-11-08 14:43:18,083] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.631 seconds
[2018-11-08 14:43:19,198] {jobs.py:385} INFO - Started process (PID=55253) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:24,204] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:43:24,209] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:43:24,211] {logging_mixin.py:95} INFO - [2018-11-08 14:43:24,210] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:43:24,605] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:24,626] {logging_mixin.py:95} INFO - [2018-11-08 14:43:24,626] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:43:24,628] {logging_mixin.py:95} INFO - [2018-11-08 14:43:24,627] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:38:24.627188+00:00

[2018-11-08 14:43:24,633] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.434 seconds
[2018-11-08 14:43:25,767] {jobs.py:385} INFO - Started process (PID=55255) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:30,773] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:43:30,775] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:43:30,778] {logging_mixin.py:95} INFO - [2018-11-08 14:43:30,777] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:43:31,571] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:31,603] {logging_mixin.py:95} INFO - [2018-11-08 14:43:31,603] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:43:31,605] {logging_mixin.py:95} INFO - [2018-11-08 14:43:31,604] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:38:31.604137+00:00

[2018-11-08 14:43:31,612] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.845 seconds
[2018-11-08 14:43:32,753] {jobs.py:385} INFO - Started process (PID=55258) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:37,763] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:43:37,770] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:43:37,771] {logging_mixin.py:95} INFO - [2018-11-08 14:43:37,770] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:43:38,280] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:38,305] {logging_mixin.py:95} INFO - [2018-11-08 14:43:38,304] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:43:38,307] {logging_mixin.py:95} INFO - [2018-11-08 14:43:38,306] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:38:38.306288+00:00

[2018-11-08 14:43:38,312] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.560 seconds
[2018-11-08 14:43:39,446] {jobs.py:385} INFO - Started process (PID=55260) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:44,457] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:43:44,466] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:43:44,467] {logging_mixin.py:95} INFO - [2018-11-08 14:43:44,467] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:43:45,499] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:45,543] {logging_mixin.py:95} INFO - [2018-11-08 14:43:45,542] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:43:45,546] {logging_mixin.py:95} INFO - [2018-11-08 14:43:45,544] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:38:45.544504+00:00

[2018-11-08 14:43:45,557] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.111 seconds
[2018-11-08 14:43:46,642] {jobs.py:385} INFO - Started process (PID=55261) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:51,654] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:43:51,661] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:43:51,664] {logging_mixin.py:95} INFO - [2018-11-08 14:43:51,663] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:43:52,199] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:52,237] {logging_mixin.py:95} INFO - [2018-11-08 14:43:52,236] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:43:52,239] {logging_mixin.py:95} INFO - [2018-11-08 14:43:52,237] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:38:52.237621+00:00

[2018-11-08 14:43:52,248] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.606 seconds
[2018-11-08 14:43:53,383] {jobs.py:385} INFO - Started process (PID=55264) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:58,390] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:43:58,395] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:43:58,397] {logging_mixin.py:95} INFO - [2018-11-08 14:43:58,396] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:43:58,960] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:43:58,983] {logging_mixin.py:95} INFO - [2018-11-08 14:43:58,983] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:43:58,985] {logging_mixin.py:95} INFO - [2018-11-08 14:43:58,984] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:38:58.984044+00:00

[2018-11-08 14:43:58,989] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.607 seconds
[2018-11-08 14:44:00,036] {jobs.py:385} INFO - Started process (PID=55270) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:05,046] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:44:05,063] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:44:05,064] {logging_mixin.py:95} INFO - [2018-11-08 14:44:05,064] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:44:05,468] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:05,495] {logging_mixin.py:95} INFO - [2018-11-08 14:44:05,494] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:44:05,496] {logging_mixin.py:95} INFO - [2018-11-08 14:44:05,495] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:39:05.495436+00:00

[2018-11-08 14:44:05,501] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.466 seconds
[2018-11-08 14:44:06,614] {jobs.py:385} INFO - Started process (PID=55271) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:11,621] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:44:11,626] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:44:11,627] {logging_mixin.py:95} INFO - [2018-11-08 14:44:11,627] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:44:12,050] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:12,071] {logging_mixin.py:95} INFO - [2018-11-08 14:44:12,071] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:44:12,073] {logging_mixin.py:95} INFO - [2018-11-08 14:44:12,072] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:39:12.072214+00:00

[2018-11-08 14:44:12,078] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.465 seconds
[2018-11-08 14:44:13,158] {jobs.py:385} INFO - Started process (PID=55273) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:18,166] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:44:18,168] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:44:18,170] {logging_mixin.py:95} INFO - [2018-11-08 14:44:18,169] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:44:18,783] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:18,806] {logging_mixin.py:95} INFO - [2018-11-08 14:44:18,806] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:44:18,807] {logging_mixin.py:95} INFO - [2018-11-08 14:44:18,806] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:39:18.806750+00:00

[2018-11-08 14:44:18,817] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.659 seconds
[2018-11-08 14:44:19,930] {jobs.py:385} INFO - Started process (PID=55274) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:24,939] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:44:24,943] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:44:24,944] {logging_mixin.py:95} INFO - [2018-11-08 14:44:24,944] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:44:25,589] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:25,611] {logging_mixin.py:95} INFO - [2018-11-08 14:44:25,610] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:44:25,612] {logging_mixin.py:95} INFO - [2018-11-08 14:44:25,611] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:39:25.611733+00:00

[2018-11-08 14:44:25,617] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.688 seconds
[2018-11-08 14:44:26,707] {jobs.py:385} INFO - Started process (PID=55278) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:31,712] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:44:31,717] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:44:31,723] {logging_mixin.py:95} INFO - [2018-11-08 14:44:31,719] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:44:32,338] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:32,366] {logging_mixin.py:95} INFO - [2018-11-08 14:44:32,366] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:44:32,368] {logging_mixin.py:95} INFO - [2018-11-08 14:44:32,367] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:39:32.367438+00:00

[2018-11-08 14:44:32,375] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.667 seconds
[2018-11-08 14:44:33,486] {jobs.py:385} INFO - Started process (PID=55279) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:38,494] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:44:38,504] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:44:38,506] {logging_mixin.py:95} INFO - [2018-11-08 14:44:38,505] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:44:39,761] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:39,790] {logging_mixin.py:95} INFO - [2018-11-08 14:44:39,790] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:44:39,792] {logging_mixin.py:95} INFO - [2018-11-08 14:44:39,791] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:39:39.791373+00:00

[2018-11-08 14:44:39,798] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.313 seconds
[2018-11-08 14:44:40,924] {jobs.py:385} INFO - Started process (PID=55281) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:45,945] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:44:45,948] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:44:45,981] {logging_mixin.py:95} INFO - [2018-11-08 14:44:45,966] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:44:46,641] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:46,674] {logging_mixin.py:95} INFO - [2018-11-08 14:44:46,673] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:44:46,676] {logging_mixin.py:95} INFO - [2018-11-08 14:44:46,674] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:39:46.674677+00:00

[2018-11-08 14:44:46,687] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.763 seconds
[2018-11-08 14:44:47,813] {jobs.py:385} INFO - Started process (PID=55283) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:52,820] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:44:52,824] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:44:52,827] {logging_mixin.py:95} INFO - [2018-11-08 14:44:52,825] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:44:53,981] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:44:54,038] {logging_mixin.py:95} INFO - [2018-11-08 14:44:54,037] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:44:54,040] {logging_mixin.py:95} INFO - [2018-11-08 14:44:54,039] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:39:54.039513+00:00

[2018-11-08 14:44:54,060] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.247 seconds
[2018-11-08 14:44:55,166] {jobs.py:385} INFO - Started process (PID=55285) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:00,177] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:45:00,181] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:45:00,185] {logging_mixin.py:95} INFO - [2018-11-08 14:45:00,184] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:45:01,864] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:01,890] {logging_mixin.py:95} INFO - [2018-11-08 14:45:01,890] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:45:01,891] {logging_mixin.py:95} INFO - [2018-11-08 14:45:01,890] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:40:01.890939+00:00

[2018-11-08 14:45:01,897] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.731 seconds
[2018-11-08 14:45:03,001] {jobs.py:385} INFO - Started process (PID=55290) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:08,009] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:45:08,011] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:45:08,012] {logging_mixin.py:95} INFO - [2018-11-08 14:45:08,012] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:45:08,477] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:08,505] {logging_mixin.py:95} INFO - [2018-11-08 14:45:08,504] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:45:08,506] {logging_mixin.py:95} INFO - [2018-11-08 14:45:08,505] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:40:08.505728+00:00

[2018-11-08 14:45:08,514] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.513 seconds
[2018-11-08 14:45:09,572] {jobs.py:385} INFO - Started process (PID=55291) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:14,577] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:45:14,585] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:45:14,588] {logging_mixin.py:95} INFO - [2018-11-08 14:45:14,586] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:45:16,103] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:16,158] {logging_mixin.py:95} INFO - [2018-11-08 14:45:16,158] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:45:16,160] {logging_mixin.py:95} INFO - [2018-11-08 14:45:16,159] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:40:16.159010+00:00

[2018-11-08 14:45:16,172] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.599 seconds
[2018-11-08 14:45:17,314] {jobs.py:385} INFO - Started process (PID=55293) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:22,325] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:45:22,327] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:45:22,330] {logging_mixin.py:95} INFO - [2018-11-08 14:45:22,329] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:45:23,513] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:23,535] {logging_mixin.py:95} INFO - [2018-11-08 14:45:23,534] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:45:23,536] {logging_mixin.py:95} INFO - [2018-11-08 14:45:23,535] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:40:23.535614+00:00

[2018-11-08 14:45:23,542] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.229 seconds
[2018-11-08 14:45:24,659] {jobs.py:385} INFO - Started process (PID=55302) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:29,666] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:45:29,668] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:45:29,669] {logging_mixin.py:95} INFO - [2018-11-08 14:45:29,669] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:45:30,442] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:30,465] {logging_mixin.py:95} INFO - [2018-11-08 14:45:30,465] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:45:30,469] {logging_mixin.py:95} INFO - [2018-11-08 14:45:30,466] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:40:30.466739+00:00

[2018-11-08 14:45:30,483] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.824 seconds
[2018-11-08 14:45:31,572] {jobs.py:385} INFO - Started process (PID=55304) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:36,583] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:45:36,586] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:45:36,590] {logging_mixin.py:95} INFO - [2018-11-08 14:45:36,587] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:45:37,291] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:37,316] {logging_mixin.py:95} INFO - [2018-11-08 14:45:37,316] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:45:37,318] {logging_mixin.py:95} INFO - [2018-11-08 14:45:37,317] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:40:37.317427+00:00

[2018-11-08 14:45:37,336] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.764 seconds
[2018-11-08 14:45:38,456] {jobs.py:385} INFO - Started process (PID=55312) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:43,462] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:45:43,468] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:45:43,471] {logging_mixin.py:95} INFO - [2018-11-08 14:45:43,470] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:45:44,108] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:44,140] {logging_mixin.py:95} INFO - [2018-11-08 14:45:44,139] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:45:44,141] {logging_mixin.py:95} INFO - [2018-11-08 14:45:44,140] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:40:44.140773+00:00

[2018-11-08 14:45:44,152] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.696 seconds
[2018-11-08 14:45:45,256] {jobs.py:385} INFO - Started process (PID=55313) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:50,264] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:45:50,266] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:45:50,268] {logging_mixin.py:95} INFO - [2018-11-08 14:45:50,267] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:45:50,969] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:50,997] {logging_mixin.py:95} INFO - [2018-11-08 14:45:50,996] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:45:50,998] {logging_mixin.py:95} INFO - [2018-11-08 14:45:50,997] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:40:50.997585+00:00

[2018-11-08 14:45:51,006] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.750 seconds
[2018-11-08 14:45:52,088] {jobs.py:385} INFO - Started process (PID=55315) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:57,093] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:45:57,095] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:45:57,096] {logging_mixin.py:95} INFO - [2018-11-08 14:45:57,095] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:45:57,860] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:45:57,901] {logging_mixin.py:95} INFO - [2018-11-08 14:45:57,899] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:45:57,904] {logging_mixin.py:95} INFO - [2018-11-08 14:45:57,902] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:40:57.902236+00:00

[2018-11-08 14:45:57,914] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.826 seconds
[2018-11-08 14:45:58,983] {jobs.py:385} INFO - Started process (PID=55321) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:03,989] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:46:03,992] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:46:03,994] {logging_mixin.py:95} INFO - [2018-11-08 14:46:03,993] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:46:04,418] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:04,438] {logging_mixin.py:95} INFO - [2018-11-08 14:46:04,438] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:46:04,439] {logging_mixin.py:95} INFO - [2018-11-08 14:46:04,438] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:41:04.438848+00:00

[2018-11-08 14:46:04,444] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.461 seconds
[2018-11-08 14:46:05,561] {jobs.py:385} INFO - Started process (PID=55324) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:10,571] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:46:10,580] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:46:10,582] {logging_mixin.py:95} INFO - [2018-11-08 14:46:10,581] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:46:11,666] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:11,712] {logging_mixin.py:95} INFO - [2018-11-08 14:46:11,712] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:46:11,715] {logging_mixin.py:95} INFO - [2018-11-08 14:46:11,713] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:41:11.713409+00:00

[2018-11-08 14:46:11,725] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.164 seconds
[2018-11-08 14:46:12,777] {jobs.py:385} INFO - Started process (PID=55328) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:17,781] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:46:17,782] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:46:17,783] {logging_mixin.py:95} INFO - [2018-11-08 14:46:17,783] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:46:18,203] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:18,234] {logging_mixin.py:95} INFO - [2018-11-08 14:46:18,233] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:46:18,236] {logging_mixin.py:95} INFO - [2018-11-08 14:46:18,234] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:41:18.234857+00:00

[2018-11-08 14:46:18,242] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.466 seconds
[2018-11-08 14:46:19,396] {jobs.py:385} INFO - Started process (PID=55343) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:24,406] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:46:24,410] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:46:24,412] {logging_mixin.py:95} INFO - [2018-11-08 14:46:24,412] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:46:25,399] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:25,433] {logging_mixin.py:95} INFO - [2018-11-08 14:46:25,432] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:46:25,435] {logging_mixin.py:95} INFO - [2018-11-08 14:46:25,433] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:41:25.433752+00:00

[2018-11-08 14:46:25,445] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.049 seconds
[2018-11-08 14:46:26,586] {jobs.py:385} INFO - Started process (PID=55345) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:31,593] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:46:31,594] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:46:31,595] {logging_mixin.py:95} INFO - [2018-11-08 14:46:31,595] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:46:32,072] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:32,098] {logging_mixin.py:95} INFO - [2018-11-08 14:46:32,097] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:46:32,099] {logging_mixin.py:95} INFO - [2018-11-08 14:46:32,098] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:41:32.098769+00:00

[2018-11-08 14:46:32,110] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.525 seconds
[2018-11-08 14:46:33,202] {jobs.py:385} INFO - Started process (PID=55347) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:38,210] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:46:38,214] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:46:38,215] {logging_mixin.py:95} INFO - [2018-11-08 14:46:38,215] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:46:38,806] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:38,839] {logging_mixin.py:95} INFO - [2018-11-08 14:46:38,839] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:46:38,841] {logging_mixin.py:95} INFO - [2018-11-08 14:46:38,840] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:41:38.840092+00:00

[2018-11-08 14:46:38,848] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.646 seconds
[2018-11-08 14:46:39,967] {jobs.py:385} INFO - Started process (PID=55350) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:44,975] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:46:44,979] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:46:44,981] {logging_mixin.py:95} INFO - [2018-11-08 14:46:44,980] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:46:45,479] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:45,501] {logging_mixin.py:95} INFO - [2018-11-08 14:46:45,501] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:46:45,503] {logging_mixin.py:95} INFO - [2018-11-08 14:46:45,502] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:41:45.502190+00:00

[2018-11-08 14:46:45,509] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.542 seconds
[2018-11-08 14:46:46,637] {jobs.py:385} INFO - Started process (PID=55354) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:51,644] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:46:51,647] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:46:51,649] {logging_mixin.py:95} INFO - [2018-11-08 14:46:51,648] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:46:52,106] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:52,138] {logging_mixin.py:95} INFO - [2018-11-08 14:46:52,138] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:46:52,140] {logging_mixin.py:95} INFO - [2018-11-08 14:46:52,138] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:41:52.138951+00:00

[2018-11-08 14:46:52,146] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.510 seconds
[2018-11-08 14:46:53,275] {jobs.py:385} INFO - Started process (PID=55356) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:58,283] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:46:58,286] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:46:58,288] {logging_mixin.py:95} INFO - [2018-11-08 14:46:58,288] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:46:58,754] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:46:58,779] {logging_mixin.py:95} INFO - [2018-11-08 14:46:58,778] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:46:58,781] {logging_mixin.py:95} INFO - [2018-11-08 14:46:58,779] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:41:58.779675+00:00

[2018-11-08 14:46:58,789] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.514 seconds
[2018-11-08 14:46:59,858] {jobs.py:385} INFO - Started process (PID=55361) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:04,865] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:47:04,868] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:47:04,870] {logging_mixin.py:95} INFO - [2018-11-08 14:47:04,870] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:47:05,347] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:05,368] {logging_mixin.py:95} INFO - [2018-11-08 14:47:05,368] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:47:05,370] {logging_mixin.py:95} INFO - [2018-11-08 14:47:05,369] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:42:05.369377+00:00

[2018-11-08 14:47:05,375] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.517 seconds
[2018-11-08 14:47:06,441] {jobs.py:385} INFO - Started process (PID=55367) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:11,449] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:47:11,454] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:47:11,456] {logging_mixin.py:95} INFO - [2018-11-08 14:47:11,456] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:47:12,477] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:12,515] {logging_mixin.py:95} INFO - [2018-11-08 14:47:12,514] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:47:12,518] {logging_mixin.py:95} INFO - [2018-11-08 14:47:12,517] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:42:12.517027+00:00

[2018-11-08 14:47:12,529] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.087 seconds
[2018-11-08 14:47:13,636] {jobs.py:385} INFO - Started process (PID=55370) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:18,645] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:47:18,648] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:47:18,649] {logging_mixin.py:95} INFO - [2018-11-08 14:47:18,649] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:47:19,193] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:19,215] {logging_mixin.py:95} INFO - [2018-11-08 14:47:19,214] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:47:19,216] {logging_mixin.py:95} INFO - [2018-11-08 14:47:19,215] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:42:19.215842+00:00

[2018-11-08 14:47:19,222] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.586 seconds
[2018-11-08 14:47:20,286] {jobs.py:385} INFO - Started process (PID=55374) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:25,292] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:47:25,294] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:47:25,295] {logging_mixin.py:95} INFO - [2018-11-08 14:47:25,295] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:47:26,355] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:26,382] {logging_mixin.py:95} INFO - [2018-11-08 14:47:26,381] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:47:26,383] {logging_mixin.py:95} INFO - [2018-11-08 14:47:26,382] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:42:26.382592+00:00

[2018-11-08 14:47:26,389] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.104 seconds
[2018-11-08 14:47:27,498] {jobs.py:385} INFO - Started process (PID=55386) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:32,505] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:47:32,507] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:47:32,508] {logging_mixin.py:95} INFO - [2018-11-08 14:47:32,508] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:47:32,941] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:32,963] {logging_mixin.py:95} INFO - [2018-11-08 14:47:32,963] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:47:32,964] {logging_mixin.py:95} INFO - [2018-11-08 14:47:32,963] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:42:32.963864+00:00

[2018-11-08 14:47:32,970] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.472 seconds
[2018-11-08 14:47:34,049] {jobs.py:385} INFO - Started process (PID=55389) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:39,060] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:47:39,067] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:47:39,068] {logging_mixin.py:95} INFO - [2018-11-08 14:47:39,068] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:47:39,567] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:39,592] {logging_mixin.py:95} INFO - [2018-11-08 14:47:39,592] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:47:39,595] {logging_mixin.py:95} INFO - [2018-11-08 14:47:39,593] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:42:39.593597+00:00

[2018-11-08 14:47:39,600] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.551 seconds
[2018-11-08 14:47:40,740] {jobs.py:385} INFO - Started process (PID=55392) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:45,749] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:47:45,755] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:47:45,757] {logging_mixin.py:95} INFO - [2018-11-08 14:47:45,756] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:47:46,139] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:46,162] {logging_mixin.py:95} INFO - [2018-11-08 14:47:46,162] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:47:46,164] {logging_mixin.py:95} INFO - [2018-11-08 14:47:46,163] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:42:46.163222+00:00

[2018-11-08 14:47:46,169] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.429 seconds
[2018-11-08 14:47:47,227] {jobs.py:385} INFO - Started process (PID=55394) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:52,233] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:47:52,237] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:47:52,239] {logging_mixin.py:95} INFO - [2018-11-08 14:47:52,238] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:47:52,672] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:52,697] {logging_mixin.py:95} INFO - [2018-11-08 14:47:52,696] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:47:52,699] {logging_mixin.py:95} INFO - [2018-11-08 14:47:52,697] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:42:52.697703+00:00

[2018-11-08 14:47:52,705] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.478 seconds
[2018-11-08 14:47:53,809] {jobs.py:385} INFO - Started process (PID=55396) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:58,819] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:47:58,823] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:47:58,828] {logging_mixin.py:95} INFO - [2018-11-08 14:47:58,825] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:47:59,253] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:47:59,272] {logging_mixin.py:95} INFO - [2018-11-08 14:47:59,272] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:47:59,274] {logging_mixin.py:95} INFO - [2018-11-08 14:47:59,273] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:42:59.273178+00:00

[2018-11-08 14:47:59,279] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.470 seconds
[2018-11-08 14:48:00,411] {jobs.py:385} INFO - Started process (PID=55401) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:05,417] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:48:05,419] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:48:05,420] {logging_mixin.py:95} INFO - [2018-11-08 14:48:05,420] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:48:06,065] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:06,091] {logging_mixin.py:95} INFO - [2018-11-08 14:48:06,090] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:48:06,092] {logging_mixin.py:95} INFO - [2018-11-08 14:48:06,091] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:43:06.091621+00:00

[2018-11-08 14:48:06,097] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.686 seconds
[2018-11-08 14:48:07,228] {jobs.py:385} INFO - Started process (PID=55402) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:12,235] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:48:12,239] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:48:12,241] {logging_mixin.py:95} INFO - [2018-11-08 14:48:12,241] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:48:12,619] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:12,640] {logging_mixin.py:95} INFO - [2018-11-08 14:48:12,640] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:48:12,642] {logging_mixin.py:95} INFO - [2018-11-08 14:48:12,641] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:43:12.641253+00:00

[2018-11-08 14:48:12,647] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.419 seconds
[2018-11-08 14:48:13,731] {jobs.py:385} INFO - Started process (PID=55404) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:18,737] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:48:18,742] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:48:18,744] {logging_mixin.py:95} INFO - [2018-11-08 14:48:18,743] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:48:19,561] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:19,593] {logging_mixin.py:95} INFO - [2018-11-08 14:48:19,593] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:48:19,595] {logging_mixin.py:95} INFO - [2018-11-08 14:48:19,594] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:43:19.594365+00:00

[2018-11-08 14:48:19,603] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.873 seconds
[2018-11-08 14:48:20,734] {jobs.py:385} INFO - Started process (PID=55406) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:25,739] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:48:25,741] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:48:25,742] {logging_mixin.py:95} INFO - [2018-11-08 14:48:25,742] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:48:26,211] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:26,237] {logging_mixin.py:95} INFO - [2018-11-08 14:48:26,236] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:48:26,239] {logging_mixin.py:95} INFO - [2018-11-08 14:48:26,237] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:43:26.237793+00:00

[2018-11-08 14:48:26,246] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.512 seconds
[2018-11-08 14:48:27,333] {jobs.py:385} INFO - Started process (PID=55408) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:32,341] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:48:32,343] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:48:32,345] {logging_mixin.py:95} INFO - [2018-11-08 14:48:32,344] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:48:32,815] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:32,842] {logging_mixin.py:95} INFO - [2018-11-08 14:48:32,841] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:48:32,844] {logging_mixin.py:95} INFO - [2018-11-08 14:48:32,843] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:43:32.843171+00:00

[2018-11-08 14:48:32,849] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.516 seconds
[2018-11-08 14:48:33,928] {jobs.py:385} INFO - Started process (PID=55411) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:38,934] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:48:38,937] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:48:38,938] {logging_mixin.py:95} INFO - [2018-11-08 14:48:38,937] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:48:39,491] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:39,513] {logging_mixin.py:95} INFO - [2018-11-08 14:48:39,513] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:48:39,514] {logging_mixin.py:95} INFO - [2018-11-08 14:48:39,513] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:43:39.513762+00:00

[2018-11-08 14:48:39,520] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.592 seconds
[2018-11-08 14:48:40,646] {jobs.py:385} INFO - Started process (PID=55413) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:45,652] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:48:45,665] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:48:45,667] {logging_mixin.py:95} INFO - [2018-11-08 14:48:45,666] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:48:46,087] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:46,108] {logging_mixin.py:95} INFO - [2018-11-08 14:48:46,107] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:48:46,109] {logging_mixin.py:95} INFO - [2018-11-08 14:48:46,108] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:43:46.108894+00:00

[2018-11-08 14:48:46,115] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.469 seconds
[2018-11-08 14:48:47,217] {jobs.py:385} INFO - Started process (PID=55414) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:52,222] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:48:52,223] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:48:52,224] {logging_mixin.py:95} INFO - [2018-11-08 14:48:52,224] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:48:52,610] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:52,630] {logging_mixin.py:95} INFO - [2018-11-08 14:48:52,629] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:48:52,632] {logging_mixin.py:95} INFO - [2018-11-08 14:48:52,630] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:43:52.630615+00:00

[2018-11-08 14:48:52,637] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.420 seconds
[2018-11-08 14:48:53,682] {jobs.py:385} INFO - Started process (PID=55417) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:58,694] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:48:58,696] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:48:58,698] {logging_mixin.py:95} INFO - [2018-11-08 14:48:58,697] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:48:59,292] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:48:59,325] {logging_mixin.py:95} INFO - [2018-11-08 14:48:59,324] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:48:59,326] {logging_mixin.py:95} INFO - [2018-11-08 14:48:59,325] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:43:59.325462+00:00

[2018-11-08 14:48:59,336] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.653 seconds
[2018-11-08 14:49:00,413] {jobs.py:385} INFO - Started process (PID=55422) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:05,419] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:49:05,430] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:49:05,432] {logging_mixin.py:95} INFO - [2018-11-08 14:49:05,432] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:49:06,064] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:06,090] {logging_mixin.py:95} INFO - [2018-11-08 14:49:06,090] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:49:06,091] {logging_mixin.py:95} INFO - [2018-11-08 14:49:06,090] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:44:06.090794+00:00

[2018-11-08 14:49:06,097] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.684 seconds
[2018-11-08 14:49:07,260] {jobs.py:385} INFO - Started process (PID=55423) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:12,270] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:49:12,277] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:49:12,281] {logging_mixin.py:95} INFO - [2018-11-08 14:49:12,279] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:49:12,726] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:12,749] {logging_mixin.py:95} INFO - [2018-11-08 14:49:12,749] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:49:12,750] {logging_mixin.py:95} INFO - [2018-11-08 14:49:12,749] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:44:12.749815+00:00

[2018-11-08 14:49:12,755] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.495 seconds
[2018-11-08 14:49:13,824] {jobs.py:385} INFO - Started process (PID=55429) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:18,830] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:49:18,833] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:49:18,836] {logging_mixin.py:95} INFO - [2018-11-08 14:49:18,835] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:49:19,335] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:19,360] {logging_mixin.py:95} INFO - [2018-11-08 14:49:19,359] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:49:19,361] {logging_mixin.py:95} INFO - [2018-11-08 14:49:19,360] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:44:19.360651+00:00

[2018-11-08 14:49:19,369] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.545 seconds
[2018-11-08 14:49:20,491] {jobs.py:385} INFO - Started process (PID=55430) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:25,508] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:49:25,510] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:49:25,514] {logging_mixin.py:95} INFO - [2018-11-08 14:49:25,512] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:49:26,942] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:26,994] {logging_mixin.py:95} INFO - [2018-11-08 14:49:26,994] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:49:27,000] {logging_mixin.py:95} INFO - [2018-11-08 14:49:26,995] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:44:26.995427+00:00

[2018-11-08 14:49:27,009] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.518 seconds
[2018-11-08 14:49:28,089] {jobs.py:385} INFO - Started process (PID=55433) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:33,100] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:49:33,105] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:49:33,109] {logging_mixin.py:95} INFO - [2018-11-08 14:49:33,108] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:49:33,650] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:33,677] {logging_mixin.py:95} INFO - [2018-11-08 14:49:33,677] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:49:33,680] {logging_mixin.py:95} INFO - [2018-11-08 14:49:33,678] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:44:33.678338+00:00

[2018-11-08 14:49:33,685] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.597 seconds
[2018-11-08 14:49:34,756] {jobs.py:385} INFO - Started process (PID=55434) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:39,765] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:49:39,769] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:49:39,771] {logging_mixin.py:95} INFO - [2018-11-08 14:49:39,770] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:49:40,699] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:40,725] {logging_mixin.py:95} INFO - [2018-11-08 14:49:40,724] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:49:40,726] {logging_mixin.py:95} INFO - [2018-11-08 14:49:40,725] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:44:40.725790+00:00

[2018-11-08 14:49:40,732] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.977 seconds
[2018-11-08 14:49:41,824] {jobs.py:385} INFO - Started process (PID=55436) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:46,833] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:49:46,841] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:49:46,842] {logging_mixin.py:95} INFO - [2018-11-08 14:49:46,842] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:49:47,507] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:47,557] {logging_mixin.py:95} INFO - [2018-11-08 14:49:47,557] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:49:47,558] {logging_mixin.py:95} INFO - [2018-11-08 14:49:47,557] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:44:47.557833+00:00

[2018-11-08 14:49:47,564] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.740 seconds
[2018-11-08 14:49:48,750] {jobs.py:385} INFO - Started process (PID=55438) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:53,761] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:49:53,766] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:49:53,768] {logging_mixin.py:95} INFO - [2018-11-08 14:49:53,767] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:49:54,254] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:49:54,275] {logging_mixin.py:95} INFO - [2018-11-08 14:49:54,275] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:49:54,278] {logging_mixin.py:95} INFO - [2018-11-08 14:49:54,276] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:44:54.276386+00:00

[2018-11-08 14:49:54,286] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.537 seconds
[2018-11-08 14:49:55,393] {jobs.py:385} INFO - Started process (PID=55444) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:00,401] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:50:00,404] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:50:00,406] {logging_mixin.py:95} INFO - [2018-11-08 14:50:00,405] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:50:00,906] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:00,933] {logging_mixin.py:95} INFO - [2018-11-08 14:50:00,933] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:50:00,935] {logging_mixin.py:95} INFO - [2018-11-08 14:50:00,934] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:45:00.934299+00:00

[2018-11-08 14:50:00,940] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.548 seconds
[2018-11-08 14:50:02,052] {jobs.py:385} INFO - Started process (PID=55455) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:07,060] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:50:07,062] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:50:07,064] {logging_mixin.py:95} INFO - [2018-11-08 14:50:07,064] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:50:07,638] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:07,664] {logging_mixin.py:95} INFO - [2018-11-08 14:50:07,664] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:50:07,666] {logging_mixin.py:95} INFO - [2018-11-08 14:50:07,665] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:45:07.665231+00:00

[2018-11-08 14:50:07,671] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.619 seconds
[2018-11-08 14:50:08,741] {jobs.py:385} INFO - Started process (PID=55457) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:13,746] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:50:13,755] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:50:13,757] {logging_mixin.py:95} INFO - [2018-11-08 14:50:13,756] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:50:14,277] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:14,298] {logging_mixin.py:95} INFO - [2018-11-08 14:50:14,297] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:50:14,299] {logging_mixin.py:95} INFO - [2018-11-08 14:50:14,298] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:45:14.298663+00:00

[2018-11-08 14:50:14,305] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.564 seconds
[2018-11-08 14:50:15,423] {jobs.py:385} INFO - Started process (PID=55463) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:20,435] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:50:20,438] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:50:20,440] {logging_mixin.py:95} INFO - [2018-11-08 14:50:20,439] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:50:21,157] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:21,184] {logging_mixin.py:95} INFO - [2018-11-08 14:50:21,184] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:50:21,186] {logging_mixin.py:95} INFO - [2018-11-08 14:50:21,185] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:45:21.185676+00:00

[2018-11-08 14:50:21,193] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.770 seconds
[2018-11-08 14:50:22,706] {jobs.py:385} INFO - Started process (PID=55467) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:27,717] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:50:27,720] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:50:27,721] {logging_mixin.py:95} INFO - [2018-11-08 14:50:27,721] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:50:28,248] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:28,270] {logging_mixin.py:95} INFO - [2018-11-08 14:50:28,269] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:50:28,271] {logging_mixin.py:95} INFO - [2018-11-08 14:50:28,270] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:45:28.270633+00:00

[2018-11-08 14:50:28,276] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.570 seconds
[2018-11-08 14:50:29,366] {jobs.py:385} INFO - Started process (PID=55472) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:34,374] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:50:34,377] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:50:34,379] {logging_mixin.py:95} INFO - [2018-11-08 14:50:34,378] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:50:34,759] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:34,799] {logging_mixin.py:95} INFO - [2018-11-08 14:50:34,799] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:50:34,802] {logging_mixin.py:95} INFO - [2018-11-08 14:50:34,800] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:45:34.800271+00:00

[2018-11-08 14:50:34,811] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.445 seconds
[2018-11-08 14:50:35,941] {jobs.py:385} INFO - Started process (PID=55474) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:40,946] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:50:40,950] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:50:40,951] {logging_mixin.py:95} INFO - [2018-11-08 14:50:40,950] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:50:41,441] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:41,467] {logging_mixin.py:95} INFO - [2018-11-08 14:50:41,466] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:50:41,468] {logging_mixin.py:95} INFO - [2018-11-08 14:50:41,467] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:45:41.467500+00:00

[2018-11-08 14:50:41,475] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.534 seconds
[2018-11-08 14:50:42,595] {jobs.py:385} INFO - Started process (PID=55475) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:47,600] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:50:47,603] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:50:47,605] {logging_mixin.py:95} INFO - [2018-11-08 14:50:47,604] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:50:48,056] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:48,083] {logging_mixin.py:95} INFO - [2018-11-08 14:50:48,082] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:50:48,084] {logging_mixin.py:95} INFO - [2018-11-08 14:50:48,083] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:45:48.083539+00:00

[2018-11-08 14:50:48,091] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.496 seconds
[2018-11-08 14:50:49,140] {jobs.py:385} INFO - Started process (PID=55480) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:54,150] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:50:54,155] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:50:54,157] {logging_mixin.py:95} INFO - [2018-11-08 14:50:54,156] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:50:54,563] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:50:54,599] {logging_mixin.py:95} INFO - [2018-11-08 14:50:54,599] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:50:54,602] {logging_mixin.py:95} INFO - [2018-11-08 14:50:54,600] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:45:54.600568+00:00

[2018-11-08 14:50:54,612] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.472 seconds
[2018-11-08 14:50:55,704] {jobs.py:385} INFO - Started process (PID=55481) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:00,712] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:51:00,714] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:51:00,715] {logging_mixin.py:95} INFO - [2018-11-08 14:51:00,715] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:51:01,351] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:01,384] {logging_mixin.py:95} INFO - [2018-11-08 14:51:01,384] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:51:01,387] {logging_mixin.py:95} INFO - [2018-11-08 14:51:01,385] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:46:01.385509+00:00

[2018-11-08 14:51:01,396] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.692 seconds
[2018-11-08 14:51:02,454] {jobs.py:385} INFO - Started process (PID=55487) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:07,458] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:51:07,461] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:51:07,463] {logging_mixin.py:95} INFO - [2018-11-08 14:51:07,462] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:51:07,897] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:07,921] {logging_mixin.py:95} INFO - [2018-11-08 14:51:07,921] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:51:07,924] {logging_mixin.py:95} INFO - [2018-11-08 14:51:07,922] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:46:07.922396+00:00

[2018-11-08 14:51:07,929] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.475 seconds
[2018-11-08 14:51:08,992] {jobs.py:385} INFO - Started process (PID=55488) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:14,000] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:51:14,007] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:51:14,017] {logging_mixin.py:95} INFO - [2018-11-08 14:51:14,013] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:51:15,082] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:15,116] {logging_mixin.py:95} INFO - [2018-11-08 14:51:15,114] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:51:15,119] {logging_mixin.py:95} INFO - [2018-11-08 14:51:15,117] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:46:15.117690+00:00

[2018-11-08 14:51:15,127] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.135 seconds
[2018-11-08 14:51:16,203] {jobs.py:385} INFO - Started process (PID=55490) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:21,208] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:51:21,211] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:51:21,214] {logging_mixin.py:95} INFO - [2018-11-08 14:51:21,213] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:51:21,687] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:21,712] {logging_mixin.py:95} INFO - [2018-11-08 14:51:21,712] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:51:21,714] {logging_mixin.py:95} INFO - [2018-11-08 14:51:21,713] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:46:21.713305+00:00

[2018-11-08 14:51:21,719] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.516 seconds
[2018-11-08 14:51:22,771] {jobs.py:385} INFO - Started process (PID=55492) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:27,782] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:51:27,785] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:51:27,787] {logging_mixin.py:95} INFO - [2018-11-08 14:51:27,786] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:51:28,191] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:28,212] {logging_mixin.py:95} INFO - [2018-11-08 14:51:28,212] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:51:28,215] {logging_mixin.py:95} INFO - [2018-11-08 14:51:28,213] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:46:28.213466+00:00

[2018-11-08 14:51:28,223] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.452 seconds
[2018-11-08 14:51:29,331] {jobs.py:385} INFO - Started process (PID=55494) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:34,336] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:51:34,338] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:51:34,339] {logging_mixin.py:95} INFO - [2018-11-08 14:51:34,339] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:51:34,726] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:34,745] {logging_mixin.py:95} INFO - [2018-11-08 14:51:34,745] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:51:34,747] {logging_mixin.py:95} INFO - [2018-11-08 14:51:34,746] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:46:34.746176+00:00

[2018-11-08 14:51:34,752] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.421 seconds
[2018-11-08 14:51:35,893] {jobs.py:385} INFO - Started process (PID=55497) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:40,898] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:51:40,901] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:51:40,902] {logging_mixin.py:95} INFO - [2018-11-08 14:51:40,901] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:51:41,349] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:41,369] {logging_mixin.py:95} INFO - [2018-11-08 14:51:41,369] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:51:41,371] {logging_mixin.py:95} INFO - [2018-11-08 14:51:41,370] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:46:41.370344+00:00

[2018-11-08 14:51:41,376] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.483 seconds
[2018-11-08 14:51:42,452] {jobs.py:385} INFO - Started process (PID=55498) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:47,458] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:51:47,462] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:51:47,464] {logging_mixin.py:95} INFO - [2018-11-08 14:51:47,463] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:51:47,832] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:47,852] {logging_mixin.py:95} INFO - [2018-11-08 14:51:47,851] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:51:47,853] {logging_mixin.py:95} INFO - [2018-11-08 14:51:47,852] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:46:47.852623+00:00

[2018-11-08 14:51:47,858] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.407 seconds
[2018-11-08 14:51:48,929] {jobs.py:385} INFO - Started process (PID=55500) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:53,936] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:51:53,940] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:51:53,942] {logging_mixin.py:95} INFO - [2018-11-08 14:51:53,941] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:51:55,050] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:51:55,082] {logging_mixin.py:95} INFO - [2018-11-08 14:51:55,081] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:51:55,084] {logging_mixin.py:95} INFO - [2018-11-08 14:51:55,082] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:46:55.082744+00:00

[2018-11-08 14:51:55,096] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.166 seconds
[2018-11-08 14:51:56,230] {jobs.py:385} INFO - Started process (PID=55501) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:01,234] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:52:01,236] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:52:01,237] {logging_mixin.py:95} INFO - [2018-11-08 14:52:01,236] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:52:01,667] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:01,687] {logging_mixin.py:95} INFO - [2018-11-08 14:52:01,686] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:52:01,688] {logging_mixin.py:95} INFO - [2018-11-08 14:52:01,687] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:47:01.687623+00:00

[2018-11-08 14:52:01,693] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.463 seconds
[2018-11-08 14:52:02,820] {jobs.py:385} INFO - Started process (PID=55506) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:07,828] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:52:07,831] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:52:07,833] {logging_mixin.py:95} INFO - [2018-11-08 14:52:07,833] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:52:08,288] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:08,308] {logging_mixin.py:95} INFO - [2018-11-08 14:52:08,308] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:52:08,310] {logging_mixin.py:95} INFO - [2018-11-08 14:52:08,309] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:47:08.309041+00:00

[2018-11-08 14:52:08,315] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.495 seconds
[2018-11-08 14:52:09,365] {jobs.py:385} INFO - Started process (PID=55508) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:14,372] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:52:14,377] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:52:14,380] {logging_mixin.py:95} INFO - [2018-11-08 14:52:14,379] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:52:14,738] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:14,758] {logging_mixin.py:95} INFO - [2018-11-08 14:52:14,757] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:52:14,760] {logging_mixin.py:95} INFO - [2018-11-08 14:52:14,758] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:47:14.758829+00:00

[2018-11-08 14:52:14,765] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.400 seconds
[2018-11-08 14:52:15,874] {jobs.py:385} INFO - Started process (PID=55510) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:20,878] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:52:20,881] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:52:20,884] {logging_mixin.py:95} INFO - [2018-11-08 14:52:20,883] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:52:21,477] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:21,532] {logging_mixin.py:95} INFO - [2018-11-08 14:52:21,531] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:52:21,534] {logging_mixin.py:95} INFO - [2018-11-08 14:52:21,533] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:47:21.533261+00:00

[2018-11-08 14:52:21,546] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.672 seconds
[2018-11-08 14:52:22,669] {jobs.py:385} INFO - Started process (PID=55512) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:27,687] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:52:27,691] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:52:27,694] {logging_mixin.py:95} INFO - [2018-11-08 14:52:27,693] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:52:28,218] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:28,238] {logging_mixin.py:95} INFO - [2018-11-08 14:52:28,238] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:52:28,240] {logging_mixin.py:95} INFO - [2018-11-08 14:52:28,239] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:47:28.239371+00:00

[2018-11-08 14:52:28,245] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.576 seconds
[2018-11-08 14:52:29,313] {jobs.py:385} INFO - Started process (PID=55514) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:34,321] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:52:34,324] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:52:34,330] {logging_mixin.py:95} INFO - [2018-11-08 14:52:34,329] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:52:34,797] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:34,823] {logging_mixin.py:95} INFO - [2018-11-08 14:52:34,823] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:52:34,825] {logging_mixin.py:95} INFO - [2018-11-08 14:52:34,823] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:47:34.823809+00:00

[2018-11-08 14:52:34,831] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.519 seconds
[2018-11-08 14:52:35,927] {jobs.py:385} INFO - Started process (PID=55516) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:40,936] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:52:40,941] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:52:40,943] {logging_mixin.py:95} INFO - [2018-11-08 14:52:40,942] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:52:41,325] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:41,349] {logging_mixin.py:95} INFO - [2018-11-08 14:52:41,349] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:52:41,350] {logging_mixin.py:95} INFO - [2018-11-08 14:52:41,350] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:47:41.349980+00:00

[2018-11-08 14:52:41,355] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.428 seconds
[2018-11-08 14:52:42,491] {jobs.py:385} INFO - Started process (PID=55518) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:47,496] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:52:47,500] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:52:47,503] {logging_mixin.py:95} INFO - [2018-11-08 14:52:47,502] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:52:48,001] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:48,022] {logging_mixin.py:95} INFO - [2018-11-08 14:52:48,022] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:52:48,026] {logging_mixin.py:95} INFO - [2018-11-08 14:52:48,023] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:47:48.023421+00:00

[2018-11-08 14:52:48,032] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.541 seconds
[2018-11-08 14:52:49,155] {jobs.py:385} INFO - Started process (PID=55520) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:54,163] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:52:54,167] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:52:54,172] {logging_mixin.py:95} INFO - [2018-11-08 14:52:54,169] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:52:55,157] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:52:55,194] {logging_mixin.py:95} INFO - [2018-11-08 14:52:55,193] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:52:55,200] {logging_mixin.py:95} INFO - [2018-11-08 14:52:55,196] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:47:55.196368+00:00

[2018-11-08 14:52:55,265] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.111 seconds
[2018-11-08 14:52:56,366] {jobs.py:385} INFO - Started process (PID=55524) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:01,376] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:53:01,394] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:53:01,396] {logging_mixin.py:95} INFO - [2018-11-08 14:53:01,395] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:53:01,803] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:01,823] {logging_mixin.py:95} INFO - [2018-11-08 14:53:01,823] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:53:01,825] {logging_mixin.py:95} INFO - [2018-11-08 14:53:01,824] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:48:01.824252+00:00

[2018-11-08 14:53:01,830] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.464 seconds
[2018-11-08 14:53:02,923] {jobs.py:385} INFO - Started process (PID=55529) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:07,929] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:53:07,932] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:53:07,934] {logging_mixin.py:95} INFO - [2018-11-08 14:53:07,933] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:53:08,384] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:08,404] {logging_mixin.py:95} INFO - [2018-11-08 14:53:08,404] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:53:08,405] {logging_mixin.py:95} INFO - [2018-11-08 14:53:08,404] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:48:08.404904+00:00

[2018-11-08 14:53:08,410] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.488 seconds
[2018-11-08 14:53:09,499] {jobs.py:385} INFO - Started process (PID=55531) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:14,506] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:53:14,509] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:53:14,511] {logging_mixin.py:95} INFO - [2018-11-08 14:53:14,510] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:53:14,942] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:14,971] {logging_mixin.py:95} INFO - [2018-11-08 14:53:14,970] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:53:14,972] {logging_mixin.py:95} INFO - [2018-11-08 14:53:14,971] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:48:14.971646+00:00

[2018-11-08 14:53:14,978] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.479 seconds
[2018-11-08 14:53:16,041] {jobs.py:385} INFO - Started process (PID=55536) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:21,051] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:53:21,054] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:53:21,055] {logging_mixin.py:95} INFO - [2018-11-08 14:53:21,055] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:53:21,480] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:21,504] {logging_mixin.py:95} INFO - [2018-11-08 14:53:21,503] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:53:21,506] {logging_mixin.py:95} INFO - [2018-11-08 14:53:21,505] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:48:21.505039+00:00

[2018-11-08 14:53:21,512] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.470 seconds
[2018-11-08 14:53:22,640] {jobs.py:385} INFO - Started process (PID=55537) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:27,647] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:53:27,653] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:53:27,655] {logging_mixin.py:95} INFO - [2018-11-08 14:53:27,655] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:53:28,393] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:28,433] {logging_mixin.py:95} INFO - [2018-11-08 14:53:28,433] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:53:28,436] {logging_mixin.py:95} INFO - [2018-11-08 14:53:28,434] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:48:28.434372+00:00

[2018-11-08 14:53:28,442] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.802 seconds
[2018-11-08 14:53:29,546] {jobs.py:385} INFO - Started process (PID=55539) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:34,555] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:53:34,557] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:53:34,558] {logging_mixin.py:95} INFO - [2018-11-08 14:53:34,558] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:53:35,046] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:35,074] {logging_mixin.py:95} INFO - [2018-11-08 14:53:35,073] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:53:35,076] {logging_mixin.py:95} INFO - [2018-11-08 14:53:35,074] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:48:35.074639+00:00

[2018-11-08 14:53:35,082] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.536 seconds
[2018-11-08 14:53:36,211] {jobs.py:385} INFO - Started process (PID=55541) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:41,217] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:53:41,223] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:53:41,226] {logging_mixin.py:95} INFO - [2018-11-08 14:53:41,224] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:53:41,937] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:41,959] {logging_mixin.py:95} INFO - [2018-11-08 14:53:41,959] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:53:41,960] {logging_mixin.py:95} INFO - [2018-11-08 14:53:41,959] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:48:41.959802+00:00

[2018-11-08 14:53:41,965] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.754 seconds
[2018-11-08 14:53:43,024] {jobs.py:385} INFO - Started process (PID=55542) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:48,032] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:53:48,036] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:53:48,037] {logging_mixin.py:95} INFO - [2018-11-08 14:53:48,036] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:53:49,127] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:49,155] {logging_mixin.py:95} INFO - [2018-11-08 14:53:49,154] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:53:49,157] {logging_mixin.py:95} INFO - [2018-11-08 14:53:49,155] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:48:49.155887+00:00

[2018-11-08 14:53:49,164] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.140 seconds
[2018-11-08 14:53:50,318] {jobs.py:385} INFO - Started process (PID=55545) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:55,326] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:53:55,329] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:53:55,332] {logging_mixin.py:95} INFO - [2018-11-08 14:53:55,331] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:53:55,836] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:53:55,865] {logging_mixin.py:95} INFO - [2018-11-08 14:53:55,865] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:53:55,867] {logging_mixin.py:95} INFO - [2018-11-08 14:53:55,866] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:48:55.865983+00:00

[2018-11-08 14:53:55,874] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.556 seconds
[2018-11-08 14:53:56,969] {jobs.py:385} INFO - Started process (PID=55546) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:01,978] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:54:01,981] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:54:01,983] {logging_mixin.py:95} INFO - [2018-11-08 14:54:01,982] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:54:02,465] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:02,485] {logging_mixin.py:95} INFO - [2018-11-08 14:54:02,484] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:54:02,487] {logging_mixin.py:95} INFO - [2018-11-08 14:54:02,486] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:49:02.486074+00:00

[2018-11-08 14:54:02,493] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.525 seconds
[2018-11-08 14:54:03,565] {jobs.py:385} INFO - Started process (PID=55551) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:08,574] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:54:08,578] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:54:08,580] {logging_mixin.py:95} INFO - [2018-11-08 14:54:08,579] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:54:09,163] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:09,188] {logging_mixin.py:95} INFO - [2018-11-08 14:54:09,188] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:54:09,191] {logging_mixin.py:95} INFO - [2018-11-08 14:54:09,189] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:49:09.189307+00:00

[2018-11-08 14:54:09,203] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.637 seconds
[2018-11-08 14:54:10,368] {jobs.py:385} INFO - Started process (PID=55552) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:15,374] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:54:15,378] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:54:15,379] {logging_mixin.py:95} INFO - [2018-11-08 14:54:15,379] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:54:16,490] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:16,527] {logging_mixin.py:95} INFO - [2018-11-08 14:54:16,527] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:54:16,532] {logging_mixin.py:95} INFO - [2018-11-08 14:54:16,528] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:49:16.528485+00:00

[2018-11-08 14:54:16,544] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.176 seconds
[2018-11-08 14:54:17,700] {jobs.py:385} INFO - Started process (PID=55554) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:22,722] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:54:22,727] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:54:22,729] {logging_mixin.py:95} INFO - [2018-11-08 14:54:22,728] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:54:23,416] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:23,445] {logging_mixin.py:95} INFO - [2018-11-08 14:54:23,445] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:54:23,446] {logging_mixin.py:95} INFO - [2018-11-08 14:54:23,445] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:49:23.445816+00:00

[2018-11-08 14:54:23,453] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.753 seconds
[2018-11-08 14:54:24,597] {jobs.py:385} INFO - Started process (PID=55557) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:29,614] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:54:29,619] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:54:29,621] {logging_mixin.py:95} INFO - [2018-11-08 14:54:29,621] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:54:30,176] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:30,202] {logging_mixin.py:95} INFO - [2018-11-08 14:54:30,202] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:54:30,204] {logging_mixin.py:95} INFO - [2018-11-08 14:54:30,203] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:49:30.203140+00:00

[2018-11-08 14:54:30,210] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.613 seconds
[2018-11-08 14:54:31,337] {jobs.py:385} INFO - Started process (PID=55558) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:36,345] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:54:36,347] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:54:36,349] {logging_mixin.py:95} INFO - [2018-11-08 14:54:36,348] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:54:36,909] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:36,937] {logging_mixin.py:95} INFO - [2018-11-08 14:54:36,937] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:54:36,939] {logging_mixin.py:95} INFO - [2018-11-08 14:54:36,938] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:49:36.938153+00:00

[2018-11-08 14:54:36,944] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.607 seconds
[2018-11-08 14:54:38,039] {jobs.py:385} INFO - Started process (PID=55560) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:43,044] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:54:43,048] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:54:43,049] {logging_mixin.py:95} INFO - [2018-11-08 14:54:43,049] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:54:43,689] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:43,718] {logging_mixin.py:95} INFO - [2018-11-08 14:54:43,718] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:54:43,722] {logging_mixin.py:95} INFO - [2018-11-08 14:54:43,720] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:49:43.719919+00:00

[2018-11-08 14:54:43,729] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.690 seconds
[2018-11-08 14:54:44,788] {jobs.py:385} INFO - Started process (PID=55561) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:49,794] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:54:49,798] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:54:49,799] {logging_mixin.py:95} INFO - [2018-11-08 14:54:49,799] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:54:50,420] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:50,448] {logging_mixin.py:95} INFO - [2018-11-08 14:54:50,447] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:54:50,450] {logging_mixin.py:95} INFO - [2018-11-08 14:54:50,448] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:49:50.448775+00:00

[2018-11-08 14:54:50,456] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.669 seconds
[2018-11-08 14:54:51,569] {jobs.py:385} INFO - Started process (PID=55563) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:56,581] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:54:56,596] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:54:56,597] {logging_mixin.py:95} INFO - [2018-11-08 14:54:56,597] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:54:57,161] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:54:57,191] {logging_mixin.py:95} INFO - [2018-11-08 14:54:57,190] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:54:57,193] {logging_mixin.py:95} INFO - [2018-11-08 14:54:57,192] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:49:57.192075+00:00

[2018-11-08 14:54:57,202] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.634 seconds
[2018-11-08 14:54:58,271] {jobs.py:385} INFO - Started process (PID=55564) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:03,280] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:55:03,281] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:55:03,283] {logging_mixin.py:95} INFO - [2018-11-08 14:55:03,282] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:55:03,745] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:03,766] {logging_mixin.py:95} INFO - [2018-11-08 14:55:03,765] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:55:03,767] {logging_mixin.py:95} INFO - [2018-11-08 14:55:03,766] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:50:03.766679+00:00

[2018-11-08 14:55:03,772] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.501 seconds
[2018-11-08 14:55:04,872] {jobs.py:385} INFO - Started process (PID=55570) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:09,880] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:55:09,882] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:55:09,883] {logging_mixin.py:95} INFO - [2018-11-08 14:55:09,883] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:55:10,420] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:10,448] {logging_mixin.py:95} INFO - [2018-11-08 14:55:10,448] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:55:10,450] {logging_mixin.py:95} INFO - [2018-11-08 14:55:10,449] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:50:10.449229+00:00

[2018-11-08 14:55:10,455] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.583 seconds
[2018-11-08 14:55:11,588] {jobs.py:385} INFO - Started process (PID=55572) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:16,602] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:55:16,609] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:55:16,612] {logging_mixin.py:95} INFO - [2018-11-08 14:55:16,611] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:55:17,152] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:17,177] {logging_mixin.py:95} INFO - [2018-11-08 14:55:17,177] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:55:17,180] {logging_mixin.py:95} INFO - [2018-11-08 14:55:17,178] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:50:17.178519+00:00

[2018-11-08 14:55:17,193] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.605 seconds
[2018-11-08 14:55:18,302] {jobs.py:385} INFO - Started process (PID=55573) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:23,312] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:55:23,314] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:55:23,315] {logging_mixin.py:95} INFO - [2018-11-08 14:55:23,315] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:55:23,734] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:23,757] {logging_mixin.py:95} INFO - [2018-11-08 14:55:23,757] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:55:23,759] {logging_mixin.py:95} INFO - [2018-11-08 14:55:23,758] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:50:23.758015+00:00

[2018-11-08 14:55:23,765] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.463 seconds
[2018-11-08 14:55:24,876] {jobs.py:385} INFO - Started process (PID=55575) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:29,884] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:55:29,887] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:55:29,889] {logging_mixin.py:95} INFO - [2018-11-08 14:55:29,888] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:55:30,506] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:30,528] {logging_mixin.py:95} INFO - [2018-11-08 14:55:30,528] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:55:30,530] {logging_mixin.py:95} INFO - [2018-11-08 14:55:30,529] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:50:30.529043+00:00

[2018-11-08 14:55:30,537] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.662 seconds
[2018-11-08 14:55:31,646] {jobs.py:385} INFO - Started process (PID=55576) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:36,651] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:55:36,653] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:55:36,654] {logging_mixin.py:95} INFO - [2018-11-08 14:55:36,653] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:55:37,052] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:37,073] {logging_mixin.py:95} INFO - [2018-11-08 14:55:37,073] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:55:37,075] {logging_mixin.py:95} INFO - [2018-11-08 14:55:37,074] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:50:37.073996+00:00

[2018-11-08 14:55:37,080] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.434 seconds
[2018-11-08 14:55:38,234] {jobs.py:385} INFO - Started process (PID=55579) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:43,241] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:55:43,247] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:55:43,248] {logging_mixin.py:95} INFO - [2018-11-08 14:55:43,248] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:55:43,618] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:43,640] {logging_mixin.py:95} INFO - [2018-11-08 14:55:43,640] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:55:43,642] {logging_mixin.py:95} INFO - [2018-11-08 14:55:43,641] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:50:43.641358+00:00

[2018-11-08 14:55:43,647] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.414 seconds
[2018-11-08 14:55:44,723] {jobs.py:385} INFO - Started process (PID=55580) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:49,728] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:55:49,731] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:55:49,732] {logging_mixin.py:95} INFO - [2018-11-08 14:55:49,732] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:55:50,205] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:50,254] {logging_mixin.py:95} INFO - [2018-11-08 14:55:50,253] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:55:50,257] {logging_mixin.py:95} INFO - [2018-11-08 14:55:50,255] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:50:50.255327+00:00

[2018-11-08 14:55:50,271] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.548 seconds
[2018-11-08 14:55:51,342] {jobs.py:385} INFO - Started process (PID=55582) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:56,349] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:55:56,352] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:55:56,353] {logging_mixin.py:95} INFO - [2018-11-08 14:55:56,353] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:55:56,773] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:55:56,798] {logging_mixin.py:95} INFO - [2018-11-08 14:55:56,798] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:55:56,801] {logging_mixin.py:95} INFO - [2018-11-08 14:55:56,799] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:50:56.799562+00:00

[2018-11-08 14:55:56,806] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.464 seconds
[2018-11-08 14:55:57,917] {jobs.py:385} INFO - Started process (PID=55583) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:02,922] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:56:02,924] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:56:02,925] {logging_mixin.py:95} INFO - [2018-11-08 14:56:02,925] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:56:03,345] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:03,369] {logging_mixin.py:95} INFO - [2018-11-08 14:56:03,369] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:56:03,370] {logging_mixin.py:95} INFO - [2018-11-08 14:56:03,369] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:51:03.369943+00:00

[2018-11-08 14:56:03,375] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.458 seconds
[2018-11-08 14:56:04,503] {jobs.py:385} INFO - Started process (PID=55588) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:09,509] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:56:09,516] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:56:09,518] {logging_mixin.py:95} INFO - [2018-11-08 14:56:09,517] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:56:09,988] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:10,011] {logging_mixin.py:95} INFO - [2018-11-08 14:56:10,011] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:56:10,013] {logging_mixin.py:95} INFO - [2018-11-08 14:56:10,012] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:51:10.012004+00:00

[2018-11-08 14:56:10,018] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.515 seconds
[2018-11-08 14:56:11,073] {jobs.py:385} INFO - Started process (PID=55590) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:16,081] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:56:16,086] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:56:16,087] {logging_mixin.py:95} INFO - [2018-11-08 14:56:16,087] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:56:16,541] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:16,562] {logging_mixin.py:95} INFO - [2018-11-08 14:56:16,561] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:56:16,564] {logging_mixin.py:95} INFO - [2018-11-08 14:56:16,562] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:51:16.562908+00:00

[2018-11-08 14:56:16,569] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.495 seconds
[2018-11-08 14:56:17,659] {jobs.py:385} INFO - Started process (PID=55592) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:22,664] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:56:22,667] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:56:22,668] {logging_mixin.py:95} INFO - [2018-11-08 14:56:22,668] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:56:23,274] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:23,297] {logging_mixin.py:95} INFO - [2018-11-08 14:56:23,297] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:56:23,299] {logging_mixin.py:95} INFO - [2018-11-08 14:56:23,298] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:51:23.298345+00:00

[2018-11-08 14:56:23,304] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.646 seconds
[2018-11-08 14:56:24,368] {jobs.py:385} INFO - Started process (PID=55594) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:29,378] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:56:29,381] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:56:29,383] {logging_mixin.py:95} INFO - [2018-11-08 14:56:29,383] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:56:29,928] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:29,951] {logging_mixin.py:95} INFO - [2018-11-08 14:56:29,950] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:56:29,954] {logging_mixin.py:95} INFO - [2018-11-08 14:56:29,952] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:51:29.951989+00:00

[2018-11-08 14:56:29,961] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.593 seconds
[2018-11-08 14:56:31,063] {jobs.py:385} INFO - Started process (PID=55595) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:36,075] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:56:36,077] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:56:36,078] {logging_mixin.py:95} INFO - [2018-11-08 14:56:36,078] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:56:36,492] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:36,515] {logging_mixin.py:95} INFO - [2018-11-08 14:56:36,515] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:56:36,516] {logging_mixin.py:95} INFO - [2018-11-08 14:56:36,515] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:51:36.515740+00:00

[2018-11-08 14:56:36,523] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.460 seconds
[2018-11-08 14:56:37,663] {jobs.py:385} INFO - Started process (PID=55597) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:42,672] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:56:42,675] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:56:42,676] {logging_mixin.py:95} INFO - [2018-11-08 14:56:42,676] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:56:43,060] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:43,082] {logging_mixin.py:95} INFO - [2018-11-08 14:56:43,082] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:56:43,084] {logging_mixin.py:95} INFO - [2018-11-08 14:56:43,082] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:51:43.082931+00:00

[2018-11-08 14:56:43,089] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.426 seconds
[2018-11-08 14:56:44,230] {jobs.py:385} INFO - Started process (PID=55599) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:49,264] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:56:49,268] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:56:49,271] {logging_mixin.py:95} INFO - [2018-11-08 14:56:49,269] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:56:50,947] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:51,016] {logging_mixin.py:95} INFO - [2018-11-08 14:56:51,015] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:56:51,017] {logging_mixin.py:95} INFO - [2018-11-08 14:56:51,016] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:51:51.016440+00:00

[2018-11-08 14:56:51,023] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.793 seconds
[2018-11-08 14:56:52,148] {jobs.py:385} INFO - Started process (PID=55601) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:57,154] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:56:57,159] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:56:57,161] {logging_mixin.py:95} INFO - [2018-11-08 14:56:57,160] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:56:57,635] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:56:57,660] {logging_mixin.py:95} INFO - [2018-11-08 14:56:57,660] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:56:57,662] {logging_mixin.py:95} INFO - [2018-11-08 14:56:57,661] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:51:57.661319+00:00

[2018-11-08 14:56:57,669] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.521 seconds
[2018-11-08 14:56:58,716] {jobs.py:385} INFO - Started process (PID=55602) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:03,726] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:57:03,729] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:57:03,730] {logging_mixin.py:95} INFO - [2018-11-08 14:57:03,730] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:57:04,135] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:04,155] {logging_mixin.py:95} INFO - [2018-11-08 14:57:04,154] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:57:04,157] {logging_mixin.py:95} INFO - [2018-11-08 14:57:04,155] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:52:04.155866+00:00

[2018-11-08 14:57:04,162] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.446 seconds
[2018-11-08 14:57:05,270] {jobs.py:385} INFO - Started process (PID=55607) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:10,278] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:57:10,281] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:57:10,282] {logging_mixin.py:95} INFO - [2018-11-08 14:57:10,282] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:57:10,652] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:10,681] {logging_mixin.py:95} INFO - [2018-11-08 14:57:10,680] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:57:10,682] {logging_mixin.py:95} INFO - [2018-11-08 14:57:10,681] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:52:10.681489+00:00

[2018-11-08 14:57:10,689] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.419 seconds
[2018-11-08 14:57:11,742] {jobs.py:385} INFO - Started process (PID=55611) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:16,750] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:57:16,755] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:57:16,756] {logging_mixin.py:95} INFO - [2018-11-08 14:57:16,756] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:57:17,846] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:17,895] {logging_mixin.py:95} INFO - [2018-11-08 14:57:17,894] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:57:17,898] {logging_mixin.py:95} INFO - [2018-11-08 14:57:17,896] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:52:17.896250+00:00

[2018-11-08 14:57:17,909] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.168 seconds
[2018-11-08 14:57:19,065] {jobs.py:385} INFO - Started process (PID=55612) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:24,072] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:57:24,077] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:57:24,078] {logging_mixin.py:95} INFO - [2018-11-08 14:57:24,078] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:57:24,529] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:24,554] {logging_mixin.py:95} INFO - [2018-11-08 14:57:24,553] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:57:24,556] {logging_mixin.py:95} INFO - [2018-11-08 14:57:24,554] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:52:24.554708+00:00

[2018-11-08 14:57:24,567] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.502 seconds
[2018-11-08 14:57:25,710] {jobs.py:385} INFO - Started process (PID=55614) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:30,717] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:57:30,723] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:57:30,725] {logging_mixin.py:95} INFO - [2018-11-08 14:57:30,724] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:57:31,747] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:31,788] {logging_mixin.py:95} INFO - [2018-11-08 14:57:31,788] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:57:31,791] {logging_mixin.py:95} INFO - [2018-11-08 14:57:31,789] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:52:31.789434+00:00

[2018-11-08 14:57:31,800] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.090 seconds
[2018-11-08 14:57:32,904] {jobs.py:385} INFO - Started process (PID=55616) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:37,910] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:57:37,913] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:57:37,917] {logging_mixin.py:95} INFO - [2018-11-08 14:57:37,915] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:57:38,452] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:38,474] {logging_mixin.py:95} INFO - [2018-11-08 14:57:38,474] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:57:38,476] {logging_mixin.py:95} INFO - [2018-11-08 14:57:38,475] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:52:38.475250+00:00

[2018-11-08 14:57:38,483] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.578 seconds
[2018-11-08 14:57:39,569] {jobs.py:385} INFO - Started process (PID=55619) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:44,574] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:57:44,579] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:57:44,581] {logging_mixin.py:95} INFO - [2018-11-08 14:57:44,580] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:57:45,215] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:45,251] {logging_mixin.py:95} INFO - [2018-11-08 14:57:45,250] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:57:45,253] {logging_mixin.py:95} INFO - [2018-11-08 14:57:45,251] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:52:45.251939+00:00

[2018-11-08 14:57:45,262] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.693 seconds
[2018-11-08 14:57:46,383] {jobs.py:385} INFO - Started process (PID=55621) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:51,391] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:57:51,397] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:57:51,399] {logging_mixin.py:95} INFO - [2018-11-08 14:57:51,398] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:57:51,819] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:51,839] {logging_mixin.py:95} INFO - [2018-11-08 14:57:51,839] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:57:51,841] {logging_mixin.py:95} INFO - [2018-11-08 14:57:51,840] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:52:51.840152+00:00

[2018-11-08 14:57:51,847] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.464 seconds
[2018-11-08 14:57:53,006] {jobs.py:385} INFO - Started process (PID=55623) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:58,017] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:57:58,023] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:57:58,030] {logging_mixin.py:95} INFO - [2018-11-08 14:57:58,028] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:57:58,532] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:57:58,574] {logging_mixin.py:95} INFO - [2018-11-08 14:57:58,574] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:57:58,576] {logging_mixin.py:95} INFO - [2018-11-08 14:57:58,575] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:52:58.575328+00:00

[2018-11-08 14:57:58,582] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.576 seconds
[2018-11-08 14:57:59,655] {jobs.py:385} INFO - Started process (PID=55624) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:04,664] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:58:04,668] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:58:04,672] {logging_mixin.py:95} INFO - [2018-11-08 14:58:04,671] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:58:05,173] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:05,193] {logging_mixin.py:95} INFO - [2018-11-08 14:58:05,192] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:58:05,194] {logging_mixin.py:95} INFO - [2018-11-08 14:58:05,193] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:53:05.193837+00:00

[2018-11-08 14:58:05,200] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.545 seconds
[2018-11-08 14:58:06,320] {jobs.py:385} INFO - Started process (PID=55629) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:11,330] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:58:11,334] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:58:11,335] {logging_mixin.py:95} INFO - [2018-11-08 14:58:11,334] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:58:11,701] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:11,721] {logging_mixin.py:95} INFO - [2018-11-08 14:58:11,720] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:58:11,722] {logging_mixin.py:95} INFO - [2018-11-08 14:58:11,721] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:53:11.721630+00:00

[2018-11-08 14:58:11,727] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.407 seconds
[2018-11-08 14:58:12,788] {jobs.py:385} INFO - Started process (PID=55631) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:17,794] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:58:17,795] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:58:17,796] {logging_mixin.py:95} INFO - [2018-11-08 14:58:17,796] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:58:18,237] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:18,261] {logging_mixin.py:95} INFO - [2018-11-08 14:58:18,261] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:58:18,265] {logging_mixin.py:95} INFO - [2018-11-08 14:58:18,262] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:53:18.262319+00:00

[2018-11-08 14:58:18,270] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.483 seconds
[2018-11-08 14:58:19,388] {jobs.py:385} INFO - Started process (PID=55633) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:24,398] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:58:24,400] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:58:24,401] {logging_mixin.py:95} INFO - [2018-11-08 14:58:24,401] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:58:24,866] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:24,889] {logging_mixin.py:95} INFO - [2018-11-08 14:58:24,888] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:58:24,892] {logging_mixin.py:95} INFO - [2018-11-08 14:58:24,890] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:53:24.890404+00:00

[2018-11-08 14:58:24,897] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.509 seconds
[2018-11-08 14:58:25,968] {jobs.py:385} INFO - Started process (PID=55635) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:30,977] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:58:30,990] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:58:30,992] {logging_mixin.py:95} INFO - [2018-11-08 14:58:30,991] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:58:31,378] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:31,413] {logging_mixin.py:95} INFO - [2018-11-08 14:58:31,413] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:58:31,415] {logging_mixin.py:95} INFO - [2018-11-08 14:58:31,414] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:53:31.414232+00:00

[2018-11-08 14:58:31,420] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.452 seconds
[2018-11-08 14:58:32,561] {jobs.py:385} INFO - Started process (PID=55636) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:37,570] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:58:37,575] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:58:37,577] {logging_mixin.py:95} INFO - [2018-11-08 14:58:37,576] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:58:38,102] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:38,130] {logging_mixin.py:95} INFO - [2018-11-08 14:58:38,130] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:58:38,133] {logging_mixin.py:95} INFO - [2018-11-08 14:58:38,131] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:53:38.131655+00:00

[2018-11-08 14:58:38,141] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.581 seconds
[2018-11-08 14:58:39,294] {jobs.py:385} INFO - Started process (PID=55638) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:44,305] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:58:44,312] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:58:44,313] {logging_mixin.py:95} INFO - [2018-11-08 14:58:44,312] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:58:44,700] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:44,729] {logging_mixin.py:95} INFO - [2018-11-08 14:58:44,728] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:58:44,733] {logging_mixin.py:95} INFO - [2018-11-08 14:58:44,730] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:53:44.730943+00:00

[2018-11-08 14:58:44,741] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.446 seconds
[2018-11-08 14:58:45,835] {jobs.py:385} INFO - Started process (PID=55639) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:50,841] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:58:50,844] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:58:50,845] {logging_mixin.py:95} INFO - [2018-11-08 14:58:50,845] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:58:51,282] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:51,307] {logging_mixin.py:95} INFO - [2018-11-08 14:58:51,306] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:58:51,309] {logging_mixin.py:95} INFO - [2018-11-08 14:58:51,308] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:53:51.308209+00:00

[2018-11-08 14:58:51,316] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.481 seconds
[2018-11-08 14:58:52,376] {jobs.py:385} INFO - Started process (PID=55642) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:57,382] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:58:57,385] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:58:57,387] {logging_mixin.py:95} INFO - [2018-11-08 14:58:57,387] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:58:57,854] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:58:57,875] {logging_mixin.py:95} INFO - [2018-11-08 14:58:57,875] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:58:57,878] {logging_mixin.py:95} INFO - [2018-11-08 14:58:57,876] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:53:57.876734+00:00

[2018-11-08 14:58:57,885] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.508 seconds
[2018-11-08 14:58:58,957] {jobs.py:385} INFO - Started process (PID=55643) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:03,965] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:59:03,970] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:59:03,974] {logging_mixin.py:95} INFO - [2018-11-08 14:59:03,973] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:59:04,640] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:04,661] {logging_mixin.py:95} INFO - [2018-11-08 14:59:04,661] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:59:04,663] {logging_mixin.py:95} INFO - [2018-11-08 14:59:04,662] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:54:04.662455+00:00

[2018-11-08 14:59:04,669] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.711 seconds
[2018-11-08 14:59:05,845] {jobs.py:385} INFO - Started process (PID=55648) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:10,853] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:59:10,859] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:59:10,861] {logging_mixin.py:95} INFO - [2018-11-08 14:59:10,860] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:59:11,278] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:11,301] {logging_mixin.py:95} INFO - [2018-11-08 14:59:11,301] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:59:11,303] {logging_mixin.py:95} INFO - [2018-11-08 14:59:11,302] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:54:11.302347+00:00

[2018-11-08 14:59:11,311] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.466 seconds
[2018-11-08 14:59:12,366] {jobs.py:385} INFO - Started process (PID=55655) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:17,370] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:59:17,372] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:59:17,374] {logging_mixin.py:95} INFO - [2018-11-08 14:59:17,373] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:59:18,084] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:18,111] {logging_mixin.py:95} INFO - [2018-11-08 14:59:18,110] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:59:18,112] {logging_mixin.py:95} INFO - [2018-11-08 14:59:18,111] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:54:18.111503+00:00

[2018-11-08 14:59:18,119] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.753 seconds
[2018-11-08 14:59:19,201] {jobs.py:385} INFO - Started process (PID=55656) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:24,207] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:59:24,212] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:59:24,214] {logging_mixin.py:95} INFO - [2018-11-08 14:59:24,213] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:59:24,728] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:24,751] {logging_mixin.py:95} INFO - [2018-11-08 14:59:24,750] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:59:24,752] {logging_mixin.py:95} INFO - [2018-11-08 14:59:24,751] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:54:24.751688+00:00

[2018-11-08 14:59:24,757] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.556 seconds
[2018-11-08 14:59:25,896] {jobs.py:385} INFO - Started process (PID=55659) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:30,907] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:59:30,917] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:59:30,919] {logging_mixin.py:95} INFO - [2018-11-08 14:59:30,918] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:59:31,840] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:31,878] {logging_mixin.py:95} INFO - [2018-11-08 14:59:31,877] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:59:31,880] {logging_mixin.py:95} INFO - [2018-11-08 14:59:31,878] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:54:31.878863+00:00

[2018-11-08 14:59:31,891] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.996 seconds
[2018-11-08 14:59:32,981] {jobs.py:385} INFO - Started process (PID=55660) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:37,993] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:59:37,997] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:59:37,999] {logging_mixin.py:95} INFO - [2018-11-08 14:59:37,998] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:59:38,537] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:38,567] {logging_mixin.py:95} INFO - [2018-11-08 14:59:38,566] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:59:38,569] {logging_mixin.py:95} INFO - [2018-11-08 14:59:38,568] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:54:38.568260+00:00

[2018-11-08 14:59:38,575] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.595 seconds
[2018-11-08 14:59:39,800] {jobs.py:385} INFO - Started process (PID=55662) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:44,816] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:59:44,821] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:59:44,823] {logging_mixin.py:95} INFO - [2018-11-08 14:59:44,822] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:59:45,325] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:45,351] {logging_mixin.py:95} INFO - [2018-11-08 14:59:45,351] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:59:45,354] {logging_mixin.py:95} INFO - [2018-11-08 14:59:45,352] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:54:45.352471+00:00

[2018-11-08 14:59:45,360] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.560 seconds
[2018-11-08 14:59:46,465] {jobs.py:385} INFO - Started process (PID=55663) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:51,475] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:59:51,480] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:59:51,482] {logging_mixin.py:95} INFO - [2018-11-08 14:59:51,481] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:59:51,986] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:52,032] {logging_mixin.py:95} INFO - [2018-11-08 14:59:52,031] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:59:52,034] {logging_mixin.py:95} INFO - [2018-11-08 14:59:52,033] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:54:52.033554+00:00

[2018-11-08 14:59:52,040] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.575 seconds
[2018-11-08 14:59:53,107] {jobs.py:385} INFO - Started process (PID=55665) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:58,115] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 14:59:58,118] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 14:59:58,119] {logging_mixin.py:95} INFO - [2018-11-08 14:59:58,119] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 14:59:58,780] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 14:59:58,815] {logging_mixin.py:95} INFO - [2018-11-08 14:59:58,814] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 14:59:58,817] {logging_mixin.py:95} INFO - [2018-11-08 14:59:58,815] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:54:58.815663+00:00

[2018-11-08 14:59:58,825] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.718 seconds
[2018-11-08 14:59:59,936] {jobs.py:385} INFO - Started process (PID=55667) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:04,945] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:00:04,948] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:00:04,950] {logging_mixin.py:95} INFO - [2018-11-08 15:00:04,949] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:00:05,311] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:05,332] {logging_mixin.py:95} INFO - [2018-11-08 15:00:05,331] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:00:05,333] {logging_mixin.py:95} INFO - [2018-11-08 15:00:05,332] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:55:05.332542+00:00

[2018-11-08 15:00:05,338] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.402 seconds
[2018-11-08 15:00:06,458] {jobs.py:385} INFO - Started process (PID=55672) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:11,466] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:00:11,472] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:00:11,473] {logging_mixin.py:95} INFO - [2018-11-08 15:00:11,473] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:00:11,923] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:11,946] {logging_mixin.py:95} INFO - [2018-11-08 15:00:11,945] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:00:11,947] {logging_mixin.py:95} INFO - [2018-11-08 15:00:11,946] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:55:11.946624+00:00

[2018-11-08 15:00:11,952] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.493 seconds
[2018-11-08 15:00:13,105] {jobs.py:385} INFO - Started process (PID=55681) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:18,113] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:00:18,119] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:00:18,120] {logging_mixin.py:95} INFO - [2018-11-08 15:00:18,120] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:00:18,597] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:18,616] {logging_mixin.py:95} INFO - [2018-11-08 15:00:18,615] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:00:18,617] {logging_mixin.py:95} INFO - [2018-11-08 15:00:18,616] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:55:18.616625+00:00

[2018-11-08 15:00:18,623] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.518 seconds
[2018-11-08 15:00:19,761] {jobs.py:385} INFO - Started process (PID=55682) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:24,773] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:00:24,776] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:00:24,777] {logging_mixin.py:95} INFO - [2018-11-08 15:00:24,777] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:00:25,695] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:25,730] {logging_mixin.py:95} INFO - [2018-11-08 15:00:25,729] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:00:25,732] {logging_mixin.py:95} INFO - [2018-11-08 15:00:25,730] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:55:25.730912+00:00

[2018-11-08 15:00:25,738] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.977 seconds
[2018-11-08 15:00:26,818] {jobs.py:385} INFO - Started process (PID=55684) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:31,831] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:00:31,834] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:00:31,838] {logging_mixin.py:95} INFO - [2018-11-08 15:00:31,835] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:00:32,322] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:32,353] {logging_mixin.py:95} INFO - [2018-11-08 15:00:32,353] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:00:32,356] {logging_mixin.py:95} INFO - [2018-11-08 15:00:32,355] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:55:32.354937+00:00

[2018-11-08 15:00:32,362] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.544 seconds
[2018-11-08 15:00:33,459] {jobs.py:385} INFO - Started process (PID=55686) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:38,472] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:00:38,476] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:00:38,477] {logging_mixin.py:95} INFO - [2018-11-08 15:00:38,476] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:00:38,944] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:38,972] {logging_mixin.py:95} INFO - [2018-11-08 15:00:38,972] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:00:38,974] {logging_mixin.py:95} INFO - [2018-11-08 15:00:38,973] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:55:38.973272+00:00

[2018-11-08 15:00:38,979] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.520 seconds
[2018-11-08 15:00:40,087] {jobs.py:385} INFO - Started process (PID=55688) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:45,097] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:00:45,101] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:00:45,103] {logging_mixin.py:95} INFO - [2018-11-08 15:00:45,102] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:00:45,641] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:45,667] {logging_mixin.py:95} INFO - [2018-11-08 15:00:45,667] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:00:45,669] {logging_mixin.py:95} INFO - [2018-11-08 15:00:45,668] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:55:45.668242+00:00

[2018-11-08 15:00:45,676] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.589 seconds
[2018-11-08 15:00:46,744] {jobs.py:385} INFO - Started process (PID=55689) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:51,750] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:00:51,753] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:00:51,758] {logging_mixin.py:95} INFO - [2018-11-08 15:00:51,755] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:00:52,334] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:52,359] {logging_mixin.py:95} INFO - [2018-11-08 15:00:52,359] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:00:52,361] {logging_mixin.py:95} INFO - [2018-11-08 15:00:52,360] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:55:52.360077+00:00

[2018-11-08 15:00:52,368] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.624 seconds
[2018-11-08 15:00:53,526] {jobs.py:385} INFO - Started process (PID=55691) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:58,536] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:00:58,538] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:00:58,539] {logging_mixin.py:95} INFO - [2018-11-08 15:00:58,538] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:00:59,057] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:00:59,085] {logging_mixin.py:95} INFO - [2018-11-08 15:00:59,085] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:00:59,087] {logging_mixin.py:95} INFO - [2018-11-08 15:00:59,086] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:55:59.086398+00:00

[2018-11-08 15:00:59,094] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.568 seconds
[2018-11-08 15:01:00,220] {jobs.py:385} INFO - Started process (PID=55693) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:05,228] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:01:05,231] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:01:05,233] {logging_mixin.py:95} INFO - [2018-11-08 15:01:05,232] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:01:05,765] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:05,796] {logging_mixin.py:95} INFO - [2018-11-08 15:01:05,796] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:01:05,798] {logging_mixin.py:95} INFO - [2018-11-08 15:01:05,797] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:56:05.796995+00:00

[2018-11-08 15:01:05,803] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.583 seconds
[2018-11-08 15:01:06,954] {jobs.py:385} INFO - Started process (PID=55698) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:11,965] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:01:11,972] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:01:11,974] {logging_mixin.py:95} INFO - [2018-11-08 15:01:11,973] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:01:12,680] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:12,720] {logging_mixin.py:95} INFO - [2018-11-08 15:01:12,719] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:01:12,725] {logging_mixin.py:95} INFO - [2018-11-08 15:01:12,722] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:56:12.722713+00:00

[2018-11-08 15:01:12,734] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.780 seconds
[2018-11-08 15:01:13,807] {jobs.py:385} INFO - Started process (PID=55700) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:18,812] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:01:18,816] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:01:18,819] {logging_mixin.py:95} INFO - [2018-11-08 15:01:18,818] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:01:19,238] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:19,261] {logging_mixin.py:95} INFO - [2018-11-08 15:01:19,260] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:01:19,263] {logging_mixin.py:95} INFO - [2018-11-08 15:01:19,262] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:56:19.261987+00:00

[2018-11-08 15:01:19,268] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.461 seconds
[2018-11-08 15:01:20,356] {jobs.py:385} INFO - Started process (PID=55701) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:25,363] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:01:25,367] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:01:25,372] {logging_mixin.py:95} INFO - [2018-11-08 15:01:25,371] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:01:25,841] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:25,875] {logging_mixin.py:95} INFO - [2018-11-08 15:01:25,874] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:01:25,878] {logging_mixin.py:95} INFO - [2018-11-08 15:01:25,875] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:56:25.875944+00:00

[2018-11-08 15:01:25,886] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.531 seconds
[2018-11-08 15:01:26,982] {jobs.py:385} INFO - Started process (PID=55703) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:31,990] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:01:31,992] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:01:31,993] {logging_mixin.py:95} INFO - [2018-11-08 15:01:31,993] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:01:32,475] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:32,529] {logging_mixin.py:95} INFO - [2018-11-08 15:01:32,529] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:01:32,531] {logging_mixin.py:95} INFO - [2018-11-08 15:01:32,530] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:56:32.530302+00:00

[2018-11-08 15:01:32,538] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.557 seconds
[2018-11-08 15:01:33,600] {jobs.py:385} INFO - Started process (PID=55705) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:38,612] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:01:38,622] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:01:38,623] {logging_mixin.py:95} INFO - [2018-11-08 15:01:38,623] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:01:39,050] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:39,076] {logging_mixin.py:95} INFO - [2018-11-08 15:01:39,075] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:01:39,077] {logging_mixin.py:95} INFO - [2018-11-08 15:01:39,076] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:56:39.076655+00:00

[2018-11-08 15:01:39,084] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.484 seconds
[2018-11-08 15:01:40,130] {jobs.py:385} INFO - Started process (PID=55707) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:45,142] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:01:45,148] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:01:45,149] {logging_mixin.py:95} INFO - [2018-11-08 15:01:45,149] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:01:45,600] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:45,623] {logging_mixin.py:95} INFO - [2018-11-08 15:01:45,622] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:01:45,624] {logging_mixin.py:95} INFO - [2018-11-08 15:01:45,623] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:56:45.623541+00:00

[2018-11-08 15:01:45,629] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.499 seconds
[2018-11-08 15:01:46,746] {jobs.py:385} INFO - Started process (PID=55708) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:51,756] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:01:51,759] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:01:51,761] {logging_mixin.py:95} INFO - [2018-11-08 15:01:51,760] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:01:52,311] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:52,344] {logging_mixin.py:95} INFO - [2018-11-08 15:01:52,344] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:01:52,349] {logging_mixin.py:95} INFO - [2018-11-08 15:01:52,345] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:56:52.345373+00:00

[2018-11-08 15:01:52,355] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.609 seconds
[2018-11-08 15:01:53,521] {jobs.py:385} INFO - Started process (PID=55710) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:58,529] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:01:58,531] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:01:58,536] {logging_mixin.py:95} INFO - [2018-11-08 15:01:58,534] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:01:59,080] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:01:59,104] {logging_mixin.py:95} INFO - [2018-11-08 15:01:59,104] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:01:59,107] {logging_mixin.py:95} INFO - [2018-11-08 15:01:59,106] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:56:59.106141+00:00

[2018-11-08 15:01:59,112] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.591 seconds
[2018-11-08 15:02:00,236] {jobs.py:385} INFO - Started process (PID=55711) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:05,242] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:02:05,246] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:02:05,249] {logging_mixin.py:95} INFO - [2018-11-08 15:02:05,248] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:02:06,051] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:06,072] {logging_mixin.py:95} INFO - [2018-11-08 15:02:06,071] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:02:06,074] {logging_mixin.py:95} INFO - [2018-11-08 15:02:06,073] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:57:06.073051+00:00

[2018-11-08 15:02:06,080] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.844 seconds
[2018-11-08 15:02:07,194] {jobs.py:385} INFO - Started process (PID=55717) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:12,202] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:02:12,210] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:02:12,211] {logging_mixin.py:95} INFO - [2018-11-08 15:02:12,211] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:02:13,386] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:13,443] {logging_mixin.py:95} INFO - [2018-11-08 15:02:13,442] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:02:13,449] {logging_mixin.py:95} INFO - [2018-11-08 15:02:13,444] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:57:13.444508+00:00

[2018-11-08 15:02:13,459] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.265 seconds
[2018-11-08 15:02:14,564] {jobs.py:385} INFO - Started process (PID=55719) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:19,575] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:02:19,578] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:02:19,580] {logging_mixin.py:95} INFO - [2018-11-08 15:02:19,579] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:02:19,969] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:19,990] {logging_mixin.py:95} INFO - [2018-11-08 15:02:19,990] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:02:19,992] {logging_mixin.py:95} INFO - [2018-11-08 15:02:19,991] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:57:19.990995+00:00

[2018-11-08 15:02:19,996] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.433 seconds
[2018-11-08 15:02:21,080] {jobs.py:385} INFO - Started process (PID=55720) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:26,088] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:02:26,091] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:02:26,093] {logging_mixin.py:95} INFO - [2018-11-08 15:02:26,092] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:02:26,460] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:26,480] {logging_mixin.py:95} INFO - [2018-11-08 15:02:26,480] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:02:26,482] {logging_mixin.py:95} INFO - [2018-11-08 15:02:26,481] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:57:26.481146+00:00

[2018-11-08 15:02:26,488] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.408 seconds
[2018-11-08 15:02:27,564] {jobs.py:385} INFO - Started process (PID=55722) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:32,571] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:02:32,573] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:02:32,574] {logging_mixin.py:95} INFO - [2018-11-08 15:02:32,574] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:02:32,949] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:32,971] {logging_mixin.py:95} INFO - [2018-11-08 15:02:32,970] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:02:32,973] {logging_mixin.py:95} INFO - [2018-11-08 15:02:32,971] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:57:32.971650+00:00

[2018-11-08 15:02:32,979] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.415 seconds
[2018-11-08 15:02:34,108] {jobs.py:385} INFO - Started process (PID=55723) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:39,123] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:02:39,275] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:02:39,277] {logging_mixin.py:95} INFO - [2018-11-08 15:02:39,276] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:02:40,073] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:40,114] {logging_mixin.py:95} INFO - [2018-11-08 15:02:40,113] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:02:40,116] {logging_mixin.py:95} INFO - [2018-11-08 15:02:40,115] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:57:40.115143+00:00

[2018-11-08 15:02:40,122] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.014 seconds
[2018-11-08 15:02:41,215] {jobs.py:385} INFO - Started process (PID=55726) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:46,222] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:02:46,225] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:02:46,227] {logging_mixin.py:95} INFO - [2018-11-08 15:02:46,226] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:02:46,674] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:46,698] {logging_mixin.py:95} INFO - [2018-11-08 15:02:46,698] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:02:46,701] {logging_mixin.py:95} INFO - [2018-11-08 15:02:46,699] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:57:46.699361+00:00

[2018-11-08 15:02:46,709] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.494 seconds
[2018-11-08 15:02:47,843] {jobs.py:385} INFO - Started process (PID=55727) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:52,853] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:02:52,857] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:02:52,858] {logging_mixin.py:95} INFO - [2018-11-08 15:02:52,857] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:02:53,437] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:53,473] {logging_mixin.py:95} INFO - [2018-11-08 15:02:53,472] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:02:53,475] {logging_mixin.py:95} INFO - [2018-11-08 15:02:53,473] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:57:53.473827+00:00

[2018-11-08 15:02:53,482] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.639 seconds
[2018-11-08 15:02:54,591] {jobs.py:385} INFO - Started process (PID=55729) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:59,597] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:02:59,599] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:02:59,600] {logging_mixin.py:95} INFO - [2018-11-08 15:02:59,600] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:02:59,923] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:02:59,947] {logging_mixin.py:95} INFO - [2018-11-08 15:02:59,946] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:02:59,949] {logging_mixin.py:95} INFO - [2018-11-08 15:02:59,947] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:57:59.947682+00:00

[2018-11-08 15:02:59,954] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.364 seconds
[2018-11-08 15:03:01,095] {jobs.py:385} INFO - Started process (PID=55731) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:06,109] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:03:06,112] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:03:06,114] {logging_mixin.py:95} INFO - [2018-11-08 15:03:06,113] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:03:06,457] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:06,482] {logging_mixin.py:95} INFO - [2018-11-08 15:03:06,482] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:03:06,485] {logging_mixin.py:95} INFO - [2018-11-08 15:03:06,483] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:58:06.483130+00:00

[2018-11-08 15:03:06,490] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.395 seconds
[2018-11-08 15:03:07,626] {jobs.py:385} INFO - Started process (PID=55735) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:12,634] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:03:12,636] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:03:12,638] {logging_mixin.py:95} INFO - [2018-11-08 15:03:12,637] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:03:13,022] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:13,046] {logging_mixin.py:95} INFO - [2018-11-08 15:03:13,046] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:03:13,048] {logging_mixin.py:95} INFO - [2018-11-08 15:03:13,047] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:58:13.047023+00:00

[2018-11-08 15:03:13,054] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.428 seconds
[2018-11-08 15:03:14,159] {jobs.py:385} INFO - Started process (PID=55738) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:19,166] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:03:19,169] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:03:19,173] {logging_mixin.py:95} INFO - [2018-11-08 15:03:19,172] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:03:19,556] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:19,583] {logging_mixin.py:95} INFO - [2018-11-08 15:03:19,583] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:03:19,585] {logging_mixin.py:95} INFO - [2018-11-08 15:03:19,584] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:58:19.584013+00:00

[2018-11-08 15:03:19,593] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.434 seconds
[2018-11-08 15:03:20,699] {jobs.py:385} INFO - Started process (PID=55739) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:25,708] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:03:25,712] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:03:25,714] {logging_mixin.py:95} INFO - [2018-11-08 15:03:25,713] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:03:26,199] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:26,230] {logging_mixin.py:95} INFO - [2018-11-08 15:03:26,230] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:03:26,232] {logging_mixin.py:95} INFO - [2018-11-08 15:03:26,230] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:58:26.230854+00:00

[2018-11-08 15:03:26,239] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.540 seconds
[2018-11-08 15:03:27,336] {jobs.py:385} INFO - Started process (PID=55741) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:32,342] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:03:32,344] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:03:32,345] {logging_mixin.py:95} INFO - [2018-11-08 15:03:32,344] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:03:32,918] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:32,945] {logging_mixin.py:95} INFO - [2018-11-08 15:03:32,944] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:03:32,947] {logging_mixin.py:95} INFO - [2018-11-08 15:03:32,945] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:58:32.945735+00:00

[2018-11-08 15:03:32,954] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.618 seconds
[2018-11-08 15:03:34,068] {jobs.py:385} INFO - Started process (PID=55742) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:39,077] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:03:39,082] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:03:39,086] {logging_mixin.py:95} INFO - [2018-11-08 15:03:39,085] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:03:39,984] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:40,030] {logging_mixin.py:95} INFO - [2018-11-08 15:03:40,030] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:03:40,032] {logging_mixin.py:95} INFO - [2018-11-08 15:03:40,031] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:58:40.031071+00:00

[2018-11-08 15:03:40,038] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.969 seconds
[2018-11-08 15:03:41,110] {jobs.py:385} INFO - Started process (PID=55744) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:46,118] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:03:46,125] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:03:46,128] {logging_mixin.py:95} INFO - [2018-11-08 15:03:46,127] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:03:46,626] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:46,658] {logging_mixin.py:95} INFO - [2018-11-08 15:03:46,657] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:03:46,661] {logging_mixin.py:95} INFO - [2018-11-08 15:03:46,658] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:58:46.658659+00:00

[2018-11-08 15:03:46,670] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.560 seconds
[2018-11-08 15:03:47,750] {jobs.py:385} INFO - Started process (PID=55746) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:52,759] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:03:52,763] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:03:52,765] {logging_mixin.py:95} INFO - [2018-11-08 15:03:52,764] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:03:53,197] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:53,226] {logging_mixin.py:95} INFO - [2018-11-08 15:03:53,226] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:03:53,228] {logging_mixin.py:95} INFO - [2018-11-08 15:03:53,227] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:58:53.227125+00:00

[2018-11-08 15:03:53,235] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.485 seconds
[2018-11-08 15:03:54,354] {jobs.py:385} INFO - Started process (PID=55748) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:59,367] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:03:59,369] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:03:59,370] {logging_mixin.py:95} INFO - [2018-11-08 15:03:59,370] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:03:59,801] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:03:59,828] {logging_mixin.py:95} INFO - [2018-11-08 15:03:59,828] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:03:59,830] {logging_mixin.py:95} INFO - [2018-11-08 15:03:59,829] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:58:59.829280+00:00

[2018-11-08 15:03:59,836] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.482 seconds
[2018-11-08 15:04:00,989] {jobs.py:385} INFO - Started process (PID=55750) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:06,002] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:04:06,005] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:04:06,007] {logging_mixin.py:95} INFO - [2018-11-08 15:04:06,006] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:04:06,449] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:06,487] {logging_mixin.py:95} INFO - [2018-11-08 15:04:06,486] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:04:06,490] {logging_mixin.py:95} INFO - [2018-11-08 15:04:06,487] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:59:06.487858+00:00

[2018-11-08 15:04:06,502] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.513 seconds
[2018-11-08 15:04:07,595] {jobs.py:385} INFO - Started process (PID=55754) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:12,602] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:04:12,608] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:04:12,610] {logging_mixin.py:95} INFO - [2018-11-08 15:04:12,609] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:04:12,927] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:12,950] {logging_mixin.py:95} INFO - [2018-11-08 15:04:12,950] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:04:12,953] {logging_mixin.py:95} INFO - [2018-11-08 15:04:12,951] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:59:12.951489+00:00

[2018-11-08 15:04:12,959] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.364 seconds
[2018-11-08 15:04:14,056] {jobs.py:385} INFO - Started process (PID=55756) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:19,062] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:04:19,064] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:04:19,065] {logging_mixin.py:95} INFO - [2018-11-08 15:04:19,065] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:04:19,683] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:19,722] {logging_mixin.py:95} INFO - [2018-11-08 15:04:19,722] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:04:19,724] {logging_mixin.py:95} INFO - [2018-11-08 15:04:19,722] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:59:19.722932+00:00

[2018-11-08 15:04:19,731] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.675 seconds
[2018-11-08 15:04:20,835] {jobs.py:385} INFO - Started process (PID=55758) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:25,842] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:04:25,845] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:04:25,846] {logging_mixin.py:95} INFO - [2018-11-08 15:04:25,845] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:04:26,179] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:26,201] {logging_mixin.py:95} INFO - [2018-11-08 15:04:26,201] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:04:26,203] {logging_mixin.py:95} INFO - [2018-11-08 15:04:26,202] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:59:26.202360+00:00

[2018-11-08 15:04:26,209] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.374 seconds
[2018-11-08 15:04:27,325] {jobs.py:385} INFO - Started process (PID=55764) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:32,333] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:04:32,336] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:04:32,338] {logging_mixin.py:95} INFO - [2018-11-08 15:04:32,337] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:04:32,666] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:32,690] {logging_mixin.py:95} INFO - [2018-11-08 15:04:32,690] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:04:32,692] {logging_mixin.py:95} INFO - [2018-11-08 15:04:32,691] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:59:32.691153+00:00

[2018-11-08 15:04:32,698] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.373 seconds
[2018-11-08 15:04:33,786] {jobs.py:385} INFO - Started process (PID=55782) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:38,793] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:04:38,797] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:04:38,800] {logging_mixin.py:95} INFO - [2018-11-08 15:04:38,799] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:04:39,220] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:39,249] {logging_mixin.py:95} INFO - [2018-11-08 15:04:39,249] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:04:39,251] {logging_mixin.py:95} INFO - [2018-11-08 15:04:39,249] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:59:39.249869+00:00

[2018-11-08 15:04:39,258] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.472 seconds
[2018-11-08 15:04:40,400] {jobs.py:385} INFO - Started process (PID=55784) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:45,412] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:04:45,416] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:04:45,418] {logging_mixin.py:95} INFO - [2018-11-08 15:04:45,417] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:04:45,865] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:45,891] {logging_mixin.py:95} INFO - [2018-11-08 15:04:45,891] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:04:45,893] {logging_mixin.py:95} INFO - [2018-11-08 15:04:45,892] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:59:45.892142+00:00

[2018-11-08 15:04:45,899] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.499 seconds
[2018-11-08 15:04:47,023] {jobs.py:385} INFO - Started process (PID=55785) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:52,033] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:04:52,036] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:04:52,038] {logging_mixin.py:95} INFO - [2018-11-08 15:04:52,037] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:04:52,355] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:52,378] {logging_mixin.py:95} INFO - [2018-11-08 15:04:52,378] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:04:52,380] {logging_mixin.py:95} INFO - [2018-11-08 15:04:52,378] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:59:52.378745+00:00

[2018-11-08 15:04:52,385] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.362 seconds
[2018-11-08 15:04:53,492] {jobs.py:385} INFO - Started process (PID=55788) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:58,500] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:04:58,503] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:04:58,506] {logging_mixin.py:95} INFO - [2018-11-08 15:04:58,505] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:04:58,831] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:04:58,855] {logging_mixin.py:95} INFO - [2018-11-08 15:04:58,855] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:04:58,857] {logging_mixin.py:95} INFO - [2018-11-08 15:04:58,855] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 16:59:58.855880+00:00

[2018-11-08 15:04:58,863] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.371 seconds
[2018-11-08 15:04:59,947] {jobs.py:385} INFO - Started process (PID=55789) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:04,952] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:05:04,955] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:05:04,957] {logging_mixin.py:95} INFO - [2018-11-08 15:05:04,956] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:05:05,286] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:05,311] {logging_mixin.py:95} INFO - [2018-11-08 15:05:05,310] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:05:05,313] {logging_mixin.py:95} INFO - [2018-11-08 15:05:05,311] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:00:05.311548+00:00

[2018-11-08 15:05:05,318] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.371 seconds
[2018-11-08 15:05:06,462] {jobs.py:385} INFO - Started process (PID=55794) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:11,468] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:05:11,473] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:05:11,476] {logging_mixin.py:95} INFO - [2018-11-08 15:05:11,475] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:05:11,835] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:11,858] {logging_mixin.py:95} INFO - [2018-11-08 15:05:11,857] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:05:11,860] {logging_mixin.py:95} INFO - [2018-11-08 15:05:11,859] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:00:11.859277+00:00

[2018-11-08 15:05:11,867] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.405 seconds
[2018-11-08 15:05:12,927] {jobs.py:385} INFO - Started process (PID=55796) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:17,934] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:05:17,939] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:05:17,942] {logging_mixin.py:95} INFO - [2018-11-08 15:05:17,941] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:05:18,707] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:18,757] {logging_mixin.py:95} INFO - [2018-11-08 15:05:18,756] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:05:18,762] {logging_mixin.py:95} INFO - [2018-11-08 15:05:18,758] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:00:18.757875+00:00

[2018-11-08 15:05:18,771] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.843 seconds
[2018-11-08 15:05:19,887] {jobs.py:385} INFO - Started process (PID=55797) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:24,893] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:05:24,894] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:05:24,895] {logging_mixin.py:95} INFO - [2018-11-08 15:05:24,895] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:05:25,277] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:25,307] {logging_mixin.py:95} INFO - [2018-11-08 15:05:25,307] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:05:25,310] {logging_mixin.py:95} INFO - [2018-11-08 15:05:25,307] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:00:25.307923+00:00

[2018-11-08 15:05:25,316] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.429 seconds
[2018-11-08 15:05:26,452] {jobs.py:385} INFO - Started process (PID=55800) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:31,456] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:05:31,457] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:05:31,459] {logging_mixin.py:95} INFO - [2018-11-08 15:05:31,458] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:05:31,929] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:31,968] {logging_mixin.py:95} INFO - [2018-11-08 15:05:31,967] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:05:31,971] {logging_mixin.py:95} INFO - [2018-11-08 15:05:31,969] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:00:31.969013+00:00

[2018-11-08 15:05:31,978] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.526 seconds
[2018-11-08 15:05:33,028] {jobs.py:385} INFO - Started process (PID=55801) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:38,035] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:05:38,037] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:05:38,038] {logging_mixin.py:95} INFO - [2018-11-08 15:05:38,038] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:05:38,938] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:38,999] {logging_mixin.py:95} INFO - [2018-11-08 15:05:38,999] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:05:39,005] {logging_mixin.py:95} INFO - [2018-11-08 15:05:39,000] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:00:39.000059+00:00

[2018-11-08 15:05:39,015] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.987 seconds
[2018-11-08 15:05:40,173] {jobs.py:385} INFO - Started process (PID=55803) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:45,180] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:05:45,182] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:05:45,183] {logging_mixin.py:95} INFO - [2018-11-08 15:05:45,182] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:05:45,718] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:45,755] {logging_mixin.py:95} INFO - [2018-11-08 15:05:45,755] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:05:45,758] {logging_mixin.py:95} INFO - [2018-11-08 15:05:45,756] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:00:45.755976+00:00

[2018-11-08 15:05:45,767] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.594 seconds
[2018-11-08 15:05:46,849] {jobs.py:385} INFO - Started process (PID=55804) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:51,857] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:05:51,860] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:05:51,864] {logging_mixin.py:95} INFO - [2018-11-08 15:05:51,862] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:05:52,414] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:52,450] {logging_mixin.py:95} INFO - [2018-11-08 15:05:52,449] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:05:52,452] {logging_mixin.py:95} INFO - [2018-11-08 15:05:52,450] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:00:52.450778+00:00

[2018-11-08 15:05:52,458] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.609 seconds
[2018-11-08 15:05:53,543] {jobs.py:385} INFO - Started process (PID=55806) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:58,552] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:05:58,558] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:05:58,562] {logging_mixin.py:95} INFO - [2018-11-08 15:05:58,561] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:05:59,104] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:05:59,132] {logging_mixin.py:95} INFO - [2018-11-08 15:05:59,131] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:05:59,134] {logging_mixin.py:95} INFO - [2018-11-08 15:05:59,132] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:00:59.132732+00:00

[2018-11-08 15:05:59,141] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.597 seconds
[2018-11-08 15:06:00,285] {jobs.py:385} INFO - Started process (PID=55808) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:05,358] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:06:05,361] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:06:05,363] {logging_mixin.py:95} INFO - [2018-11-08 15:06:05,362] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:06:06,561] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:06,615] {logging_mixin.py:95} INFO - [2018-11-08 15:06:06,615] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:06:06,617] {logging_mixin.py:95} INFO - [2018-11-08 15:06:06,616] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:01:06.616296+00:00

[2018-11-08 15:06:06,631] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.346 seconds
[2018-11-08 15:06:07,766] {jobs.py:385} INFO - Started process (PID=55813) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:12,796] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:06:12,804] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:06:12,806] {logging_mixin.py:95} INFO - [2018-11-08 15:06:12,805] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:06:13,867] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:13,909] {logging_mixin.py:95} INFO - [2018-11-08 15:06:13,909] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:06:13,916] {logging_mixin.py:95} INFO - [2018-11-08 15:06:13,910] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:01:13.910682+00:00

[2018-11-08 15:06:13,932] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.166 seconds
[2018-11-08 15:06:15,002] {jobs.py:385} INFO - Started process (PID=55815) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:20,013] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:06:20,017] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:06:20,020] {logging_mixin.py:95} INFO - [2018-11-08 15:06:20,019] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:06:22,642] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:22,739] {logging_mixin.py:95} INFO - [2018-11-08 15:06:22,738] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:06:22,746] {logging_mixin.py:95} INFO - [2018-11-08 15:06:22,740] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:01:22.740131+00:00

[2018-11-08 15:06:22,770] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.768 seconds
[2018-11-08 15:06:23,921] {jobs.py:385} INFO - Started process (PID=55816) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:28,927] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:06:28,930] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:06:28,932] {logging_mixin.py:95} INFO - [2018-11-08 15:06:28,931] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:06:30,023] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:30,066] {logging_mixin.py:95} INFO - [2018-11-08 15:06:30,066] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:06:30,073] {logging_mixin.py:95} INFO - [2018-11-08 15:06:30,067] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:01:30.067761+00:00

[2018-11-08 15:06:30,086] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.165 seconds
[2018-11-08 15:06:31,265] {jobs.py:385} INFO - Started process (PID=55819) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:36,280] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:06:36,283] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:06:36,285] {logging_mixin.py:95} INFO - [2018-11-08 15:06:36,284] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:06:36,683] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:36,712] {logging_mixin.py:95} INFO - [2018-11-08 15:06:36,711] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:06:36,715] {logging_mixin.py:95} INFO - [2018-11-08 15:06:36,713] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:01:36.713837+00:00

[2018-11-08 15:06:36,724] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.459 seconds
[2018-11-08 15:06:37,852] {jobs.py:385} INFO - Started process (PID=55821) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:42,857] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:06:42,859] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:06:42,861] {logging_mixin.py:95} INFO - [2018-11-08 15:06:42,860] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:06:43,637] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:43,685] {logging_mixin.py:95} INFO - [2018-11-08 15:06:43,684] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:06:43,693] {logging_mixin.py:95} INFO - [2018-11-08 15:06:43,686] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:01:43.686810+00:00

[2018-11-08 15:06:43,725] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.872 seconds
[2018-11-08 15:06:44,903] {jobs.py:385} INFO - Started process (PID=55822) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:49,917] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:06:49,920] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:06:49,921] {logging_mixin.py:95} INFO - [2018-11-08 15:06:49,921] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:06:50,488] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:50,523] {logging_mixin.py:95} INFO - [2018-11-08 15:06:50,522] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:06:50,525] {logging_mixin.py:95} INFO - [2018-11-08 15:06:50,523] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:01:50.523527+00:00

[2018-11-08 15:06:50,532] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.629 seconds
[2018-11-08 15:06:51,671] {jobs.py:385} INFO - Started process (PID=55825) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:56,681] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:06:56,683] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:06:56,685] {logging_mixin.py:95} INFO - [2018-11-08 15:06:56,684] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:06:57,106] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:06:57,137] {logging_mixin.py:95} INFO - [2018-11-08 15:06:57,137] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:06:57,139] {logging_mixin.py:95} INFO - [2018-11-08 15:06:57,138] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:01:57.138084+00:00

[2018-11-08 15:06:57,147] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.476 seconds
[2018-11-08 15:06:58,272] {jobs.py:385} INFO - Started process (PID=55826) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:03,277] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:07:03,279] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:07:03,281] {logging_mixin.py:95} INFO - [2018-11-08 15:07:03,280] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:07:03,717] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:03,749] {logging_mixin.py:95} INFO - [2018-11-08 15:07:03,749] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:07:03,755] {logging_mixin.py:95} INFO - [2018-11-08 15:07:03,750] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:02:03.750657+00:00

[2018-11-08 15:07:03,761] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.489 seconds
[2018-11-08 15:07:04,903] {jobs.py:385} INFO - Started process (PID=55831) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:09,909] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:07:09,912] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:07:09,917] {logging_mixin.py:95} INFO - [2018-11-08 15:07:09,914] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:07:10,366] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:10,397] {logging_mixin.py:95} INFO - [2018-11-08 15:07:10,397] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:07:10,400] {logging_mixin.py:95} INFO - [2018-11-08 15:07:10,398] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:02:10.397980+00:00

[2018-11-08 15:07:10,407] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.503 seconds
[2018-11-08 15:07:11,509] {jobs.py:385} INFO - Started process (PID=55833) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:16,518] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:07:16,529] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:07:16,531] {logging_mixin.py:95} INFO - [2018-11-08 15:07:16,530] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:07:16,953] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:16,981] {logging_mixin.py:95} INFO - [2018-11-08 15:07:16,981] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:07:16,983] {logging_mixin.py:95} INFO - [2018-11-08 15:07:16,982] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:02:16.982148+00:00

[2018-11-08 15:07:16,990] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.480 seconds
[2018-11-08 15:07:18,136] {jobs.py:385} INFO - Started process (PID=55835) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:23,153] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:07:23,156] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:07:23,157] {logging_mixin.py:95} INFO - [2018-11-08 15:07:23,157] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:07:23,616] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:23,650] {logging_mixin.py:95} INFO - [2018-11-08 15:07:23,649] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:07:23,653] {logging_mixin.py:95} INFO - [2018-11-08 15:07:23,650] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:02:23.650871+00:00

[2018-11-08 15:07:23,659] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.523 seconds
[2018-11-08 15:07:24,745] {jobs.py:385} INFO - Started process (PID=55837) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:29,754] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:07:29,759] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:07:29,762] {logging_mixin.py:95} INFO - [2018-11-08 15:07:29,761] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:07:30,169] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:30,195] {logging_mixin.py:95} INFO - [2018-11-08 15:07:30,195] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:07:30,197] {logging_mixin.py:95} INFO - [2018-11-08 15:07:30,196] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:02:30.196312+00:00

[2018-11-08 15:07:30,204] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.459 seconds
[2018-11-08 15:07:31,274] {jobs.py:385} INFO - Started process (PID=55838) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:36,280] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:07:36,284] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:07:36,287] {logging_mixin.py:95} INFO - [2018-11-08 15:07:36,285] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:07:37,826] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:37,903] {logging_mixin.py:95} INFO - [2018-11-08 15:07:37,902] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:07:37,907] {logging_mixin.py:95} INFO - [2018-11-08 15:07:37,904] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:02:37.904233+00:00

[2018-11-08 15:07:37,918] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.644 seconds
[2018-11-08 15:07:39,055] {jobs.py:385} INFO - Started process (PID=55841) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:44,063] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:07:44,067] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:07:44,069] {logging_mixin.py:95} INFO - [2018-11-08 15:07:44,069] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:07:44,585] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:44,621] {logging_mixin.py:95} INFO - [2018-11-08 15:07:44,621] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:07:44,625] {logging_mixin.py:95} INFO - [2018-11-08 15:07:44,622] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:02:44.622920+00:00

[2018-11-08 15:07:44,633] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.579 seconds
[2018-11-08 15:07:45,736] {jobs.py:385} INFO - Started process (PID=55842) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:50,741] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:07:50,750] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:07:50,752] {logging_mixin.py:95} INFO - [2018-11-08 15:07:50,751] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:07:52,149] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:52,184] {logging_mixin.py:95} INFO - [2018-11-08 15:07:52,183] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:07:52,187] {logging_mixin.py:95} INFO - [2018-11-08 15:07:52,184] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:02:52.184522+00:00

[2018-11-08 15:07:52,197] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.461 seconds
[2018-11-08 15:07:53,293] {jobs.py:385} INFO - Started process (PID=55844) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:58,305] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:07:58,312] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:07:58,316] {logging_mixin.py:95} INFO - [2018-11-08 15:07:58,314] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:07:58,859] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:07:58,891] {logging_mixin.py:95} INFO - [2018-11-08 15:07:58,890] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:07:58,895] {logging_mixin.py:95} INFO - [2018-11-08 15:07:58,893] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:02:58.893206+00:00

[2018-11-08 15:07:58,902] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.609 seconds
[2018-11-08 15:08:00,087] {jobs.py:385} INFO - Started process (PID=55845) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:05,095] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:08:05,098] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:08:05,099] {logging_mixin.py:95} INFO - [2018-11-08 15:08:05,099] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:08:06,105] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:06,153] {logging_mixin.py:95} INFO - [2018-11-08 15:08:06,151] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:08:06,168] {logging_mixin.py:95} INFO - [2018-11-08 15:08:06,153] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:03:06.153850+00:00

[2018-11-08 15:08:06,206] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.119 seconds
[2018-11-08 15:08:07,302] {jobs.py:385} INFO - Started process (PID=55850) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:12,310] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:08:12,316] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:08:12,318] {logging_mixin.py:95} INFO - [2018-11-08 15:08:12,317] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:08:12,837] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:12,878] {logging_mixin.py:95} INFO - [2018-11-08 15:08:12,878] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:08:12,884] {logging_mixin.py:95} INFO - [2018-11-08 15:08:12,879] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:03:12.879457+00:00

[2018-11-08 15:08:12,896] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.595 seconds
[2018-11-08 15:08:14,066] {jobs.py:385} INFO - Started process (PID=55853) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:19,074] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:08:19,087] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:08:19,089] {logging_mixin.py:95} INFO - [2018-11-08 15:08:19,088] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:08:21,524] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:21,622] {logging_mixin.py:95} INFO - [2018-11-08 15:08:21,599] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:08:21,641] {logging_mixin.py:95} INFO - [2018-11-08 15:08:21,629] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:03:21.629217+00:00

[2018-11-08 15:08:21,659] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 7.593 seconds
[2018-11-08 15:08:22,817] {jobs.py:385} INFO - Started process (PID=55854) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:27,831] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:08:27,836] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:08:27,837] {logging_mixin.py:95} INFO - [2018-11-08 15:08:27,837] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:08:28,598] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:28,629] {logging_mixin.py:95} INFO - [2018-11-08 15:08:28,627] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:08:28,632] {logging_mixin.py:95} INFO - [2018-11-08 15:08:28,630] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:03:28.630463+00:00

[2018-11-08 15:08:28,641] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.824 seconds
[2018-11-08 15:08:29,749] {jobs.py:385} INFO - Started process (PID=55856) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:34,760] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:08:34,763] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:08:34,764] {logging_mixin.py:95} INFO - [2018-11-08 15:08:34,764] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:08:35,438] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:35,470] {logging_mixin.py:95} INFO - [2018-11-08 15:08:35,469] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:08:35,475] {logging_mixin.py:95} INFO - [2018-11-08 15:08:35,471] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:03:35.471730+00:00

[2018-11-08 15:08:35,483] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.734 seconds
[2018-11-08 15:08:36,558] {jobs.py:385} INFO - Started process (PID=55858) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:41,567] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:08:41,571] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:08:41,573] {logging_mixin.py:95} INFO - [2018-11-08 15:08:41,572] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:08:42,473] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:42,510] {logging_mixin.py:95} INFO - [2018-11-08 15:08:42,510] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:08:42,516] {logging_mixin.py:95} INFO - [2018-11-08 15:08:42,511] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:03:42.511760+00:00

[2018-11-08 15:08:42,523] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.965 seconds
[2018-11-08 15:08:43,591] {jobs.py:385} INFO - Started process (PID=55860) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:48,598] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:08:48,601] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:08:48,603] {logging_mixin.py:95} INFO - [2018-11-08 15:08:48,602] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:08:50,067] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:50,149] {logging_mixin.py:95} INFO - [2018-11-08 15:08:50,148] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:08:50,159] {logging_mixin.py:95} INFO - [2018-11-08 15:08:50,151] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:03:50.151606+00:00

[2018-11-08 15:08:50,221] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.630 seconds
[2018-11-08 15:08:51,380] {jobs.py:385} INFO - Started process (PID=55862) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:56,390] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:08:56,394] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:08:56,395] {logging_mixin.py:95} INFO - [2018-11-08 15:08:56,395] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:08:57,599] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:08:57,628] {logging_mixin.py:95} INFO - [2018-11-08 15:08:57,627] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:08:57,634] {logging_mixin.py:95} INFO - [2018-11-08 15:08:57,629] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:03:57.629686+00:00

[2018-11-08 15:08:57,646] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.266 seconds
[2018-11-08 15:08:58,716] {jobs.py:385} INFO - Started process (PID=55863) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:03,726] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:09:03,730] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:09:03,734] {logging_mixin.py:95} INFO - [2018-11-08 15:09:03,732] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:09:04,168] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:04,203] {logging_mixin.py:95} INFO - [2018-11-08 15:09:04,203] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:09:04,206] {logging_mixin.py:95} INFO - [2018-11-08 15:09:04,203] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:04:04.203926+00:00

[2018-11-08 15:09:04,212] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.496 seconds
[2018-11-08 15:09:05,363] {jobs.py:385} INFO - Started process (PID=55865) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:10,375] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:09:10,377] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:09:10,379] {logging_mixin.py:95} INFO - [2018-11-08 15:09:10,379] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:09:10,792] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:10,816] {logging_mixin.py:95} INFO - [2018-11-08 15:09:10,815] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:09:10,818] {logging_mixin.py:95} INFO - [2018-11-08 15:09:10,816] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:04:10.816477+00:00

[2018-11-08 15:09:10,825] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.462 seconds
[2018-11-08 15:09:11,873] {jobs.py:385} INFO - Started process (PID=55869) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:16,883] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:09:16,886] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:09:16,887] {logging_mixin.py:95} INFO - [2018-11-08 15:09:16,887] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:09:17,598] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:17,652] {logging_mixin.py:95} INFO - [2018-11-08 15:09:17,651] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:09:17,656] {logging_mixin.py:95} INFO - [2018-11-08 15:09:17,653] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:04:17.653558+00:00

[2018-11-08 15:09:17,669] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.796 seconds
[2018-11-08 15:09:18,762] {jobs.py:385} INFO - Started process (PID=55872) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:23,773] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:09:23,778] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:09:23,779] {logging_mixin.py:95} INFO - [2018-11-08 15:09:23,779] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:09:24,235] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:24,267] {logging_mixin.py:95} INFO - [2018-11-08 15:09:24,266] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:09:24,270] {logging_mixin.py:95} INFO - [2018-11-08 15:09:24,267] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:04:24.267538+00:00

[2018-11-08 15:09:24,279] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.518 seconds
[2018-11-08 15:09:25,343] {jobs.py:385} INFO - Started process (PID=55874) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:30,352] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:09:30,354] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:09:30,355] {logging_mixin.py:95} INFO - [2018-11-08 15:09:30,355] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:09:30,828] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:30,889] {logging_mixin.py:95} INFO - [2018-11-08 15:09:30,889] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:09:30,893] {logging_mixin.py:95} INFO - [2018-11-08 15:09:30,890] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:04:30.890268+00:00

[2018-11-08 15:09:30,902] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.559 seconds
[2018-11-08 15:09:32,011] {jobs.py:385} INFO - Started process (PID=55875) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:37,020] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:09:37,024] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:09:37,027] {logging_mixin.py:95} INFO - [2018-11-08 15:09:37,026] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:09:38,786] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:38,841] {logging_mixin.py:95} INFO - [2018-11-08 15:09:38,840] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:09:38,846] {logging_mixin.py:95} INFO - [2018-11-08 15:09:38,843] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:04:38.842997+00:00

[2018-11-08 15:09:38,861] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.850 seconds
[2018-11-08 15:09:40,053] {jobs.py:385} INFO - Started process (PID=55877) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:45,064] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:09:45,069] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:09:45,071] {logging_mixin.py:95} INFO - [2018-11-08 15:09:45,069] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:09:45,551] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:45,593] {logging_mixin.py:95} INFO - [2018-11-08 15:09:45,592] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:09:45,599] {logging_mixin.py:95} INFO - [2018-11-08 15:09:45,594] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:04:45.594219+00:00

[2018-11-08 15:09:45,609] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.556 seconds
[2018-11-08 15:09:46,688] {jobs.py:385} INFO - Started process (PID=55878) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:51,697] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:09:51,700] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:09:51,701] {logging_mixin.py:95} INFO - [2018-11-08 15:09:51,701] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:09:52,512] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:52,598] {logging_mixin.py:95} INFO - [2018-11-08 15:09:52,597] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:09:52,600] {logging_mixin.py:95} INFO - [2018-11-08 15:09:52,598] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:04:52.598873+00:00

[2018-11-08 15:09:52,611] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.923 seconds
[2018-11-08 15:09:53,669] {jobs.py:385} INFO - Started process (PID=55880) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:58,680] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:09:58,686] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:09:58,687] {logging_mixin.py:95} INFO - [2018-11-08 15:09:58,687] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:09:59,217] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:09:59,266] {logging_mixin.py:95} INFO - [2018-11-08 15:09:59,265] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:09:59,268] {logging_mixin.py:95} INFO - [2018-11-08 15:09:59,266] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:04:59.266700+00:00

[2018-11-08 15:09:59,280] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.611 seconds
[2018-11-08 15:10:00,434] {jobs.py:385} INFO - Started process (PID=55882) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:05,452] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:10:05,455] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:10:05,458] {logging_mixin.py:95} INFO - [2018-11-08 15:10:05,457] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:10:06,874] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:06,935] {logging_mixin.py:95} INFO - [2018-11-08 15:10:06,934] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:10:06,938] {logging_mixin.py:95} INFO - [2018-11-08 15:10:06,936] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:05:06.935997+00:00

[2018-11-08 15:10:06,954] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.520 seconds
[2018-11-08 15:10:08,012] {jobs.py:385} INFO - Started process (PID=55887) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:13,018] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:10:13,021] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:10:13,023] {logging_mixin.py:95} INFO - [2018-11-08 15:10:13,023] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:10:13,461] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:13,491] {logging_mixin.py:95} INFO - [2018-11-08 15:10:13,490] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:10:13,497] {logging_mixin.py:95} INFO - [2018-11-08 15:10:13,492] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:05:13.492033+00:00

[2018-11-08 15:10:13,503] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.491 seconds
[2018-11-08 15:10:14,643] {jobs.py:385} INFO - Started process (PID=55889) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:19,649] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:10:19,654] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:10:19,657] {logging_mixin.py:95} INFO - [2018-11-08 15:10:19,656] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:10:20,211] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:20,241] {logging_mixin.py:95} INFO - [2018-11-08 15:10:20,241] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:10:20,244] {logging_mixin.py:95} INFO - [2018-11-08 15:10:20,242] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:05:20.242412+00:00

[2018-11-08 15:10:20,250] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.607 seconds
[2018-11-08 15:10:21,344] {jobs.py:385} INFO - Started process (PID=55890) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:26,351] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:10:26,360] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:10:26,362] {logging_mixin.py:95} INFO - [2018-11-08 15:10:26,361] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:10:27,381] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:27,461] {logging_mixin.py:95} INFO - [2018-11-08 15:10:27,460] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:10:27,558] {logging_mixin.py:95} INFO - [2018-11-08 15:10:27,462] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:05:27.462863+00:00

[2018-11-08 15:10:27,567] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.223 seconds
[2018-11-08 15:10:28,786] {jobs.py:385} INFO - Started process (PID=55893) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:33,796] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:10:33,799] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:10:33,801] {logging_mixin.py:95} INFO - [2018-11-08 15:10:33,800] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:10:34,304] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:34,335] {logging_mixin.py:95} INFO - [2018-11-08 15:10:34,335] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:10:34,337] {logging_mixin.py:95} INFO - [2018-11-08 15:10:34,335] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:05:34.335842+00:00

[2018-11-08 15:10:34,348] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.562 seconds
[2018-11-08 15:10:35,461] {jobs.py:385} INFO - Started process (PID=55894) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:40,468] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:10:40,470] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:10:40,471] {logging_mixin.py:95} INFO - [2018-11-08 15:10:40,471] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:10:40,878] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:40,909] {logging_mixin.py:95} INFO - [2018-11-08 15:10:40,908] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:10:40,912] {logging_mixin.py:95} INFO - [2018-11-08 15:10:40,910] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:05:40.909933+00:00

[2018-11-08 15:10:40,921] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.460 seconds
[2018-11-08 15:10:42,028] {jobs.py:385} INFO - Started process (PID=55896) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:47,041] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:10:47,045] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:10:47,047] {logging_mixin.py:95} INFO - [2018-11-08 15:10:47,047] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:10:48,467] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:48,515] {logging_mixin.py:95} INFO - [2018-11-08 15:10:48,515] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:10:48,519] {logging_mixin.py:95} INFO - [2018-11-08 15:10:48,516] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:05:48.516448+00:00

[2018-11-08 15:10:48,532] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.504 seconds
[2018-11-08 15:10:49,695] {jobs.py:385} INFO - Started process (PID=55898) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:54,701] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:10:54,712] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:10:54,715] {logging_mixin.py:95} INFO - [2018-11-08 15:10:54,714] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:10:55,401] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:10:55,440] {logging_mixin.py:95} INFO - [2018-11-08 15:10:55,439] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:10:55,443] {logging_mixin.py:95} INFO - [2018-11-08 15:10:55,441] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:05:55.441621+00:00

[2018-11-08 15:10:55,450] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.755 seconds
[2018-11-08 15:10:56,547] {jobs.py:385} INFO - Started process (PID=55899) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:01,552] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:11:01,556] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:11:01,560] {logging_mixin.py:95} INFO - [2018-11-08 15:11:01,558] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:11:03,190] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:03,286] {logging_mixin.py:95} INFO - [2018-11-08 15:11:03,285] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:11:03,289] {logging_mixin.py:95} INFO - [2018-11-08 15:11:03,286] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:06:03.286909+00:00

[2018-11-08 15:11:03,301] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.753 seconds
[2018-11-08 15:11:04,478] {jobs.py:385} INFO - Started process (PID=55902) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:09,487] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:11:09,489] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:11:09,491] {logging_mixin.py:95} INFO - [2018-11-08 15:11:09,490] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:11:10,019] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:10,064] {logging_mixin.py:95} INFO - [2018-11-08 15:11:10,063] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:11:10,068] {logging_mixin.py:95} INFO - [2018-11-08 15:11:10,064] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:06:10.064635+00:00

[2018-11-08 15:11:10,075] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.597 seconds
[2018-11-08 15:11:11,202] {jobs.py:385} INFO - Started process (PID=55906) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:16,212] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:11:16,220] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:11:16,222] {logging_mixin.py:95} INFO - [2018-11-08 15:11:16,221] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:11:16,738] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:16,786] {logging_mixin.py:95} INFO - [2018-11-08 15:11:16,785] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:11:16,788] {logging_mixin.py:95} INFO - [2018-11-08 15:11:16,786] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:06:16.786762+00:00

[2018-11-08 15:11:16,798] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.598 seconds
[2018-11-08 15:11:17,922] {jobs.py:385} INFO - Started process (PID=55908) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:22,931] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:11:22,934] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:11:22,937] {logging_mixin.py:95} INFO - [2018-11-08 15:11:22,935] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:11:23,374] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:23,407] {logging_mixin.py:95} INFO - [2018-11-08 15:11:23,407] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:11:23,409] {logging_mixin.py:95} INFO - [2018-11-08 15:11:23,408] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:06:23.408022+00:00

[2018-11-08 15:11:23,416] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.494 seconds
[2018-11-08 15:11:24,551] {jobs.py:385} INFO - Started process (PID=55909) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:29,559] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:11:29,570] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:11:29,572] {logging_mixin.py:95} INFO - [2018-11-08 15:11:29,571] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:11:31,163] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:31,214] {logging_mixin.py:95} INFO - [2018-11-08 15:11:31,214] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:11:31,225] {logging_mixin.py:95} INFO - [2018-11-08 15:11:31,215] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:06:31.215221+00:00

[2018-11-08 15:11:31,241] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.690 seconds
[2018-11-08 15:11:32,352] {jobs.py:385} INFO - Started process (PID=55911) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:37,360] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:11:37,365] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:11:37,367] {logging_mixin.py:95} INFO - [2018-11-08 15:11:37,366] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:11:38,403] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:38,471] {logging_mixin.py:95} INFO - [2018-11-08 15:11:38,471] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:11:38,474] {logging_mixin.py:95} INFO - [2018-11-08 15:11:38,472] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:06:38.472117+00:00

[2018-11-08 15:11:38,486] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.134 seconds
[2018-11-08 15:11:39,749] {jobs.py:385} INFO - Started process (PID=55914) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:44,764] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:11:44,767] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:11:44,774] {logging_mixin.py:95} INFO - [2018-11-08 15:11:44,772] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:11:45,260] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:45,483] {logging_mixin.py:95} INFO - [2018-11-08 15:11:45,481] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:11:45,506] {logging_mixin.py:95} INFO - [2018-11-08 15:11:45,496] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:06:45.496070+00:00

[2018-11-08 15:11:45,521] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.773 seconds
[2018-11-08 15:11:46,681] {jobs.py:385} INFO - Started process (PID=55915) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:51,689] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:11:51,692] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:11:51,694] {logging_mixin.py:95} INFO - [2018-11-08 15:11:51,693] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:11:52,117] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:52,145] {logging_mixin.py:95} INFO - [2018-11-08 15:11:52,144] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:11:52,147] {logging_mixin.py:95} INFO - [2018-11-08 15:11:52,145] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:06:52.145406+00:00

[2018-11-08 15:11:52,153] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.472 seconds
[2018-11-08 15:11:53,246] {jobs.py:385} INFO - Started process (PID=55917) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:58,255] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:11:58,263] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:11:58,265] {logging_mixin.py:95} INFO - [2018-11-08 15:11:58,265] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:11:58,676] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:11:58,704] {logging_mixin.py:95} INFO - [2018-11-08 15:11:58,704] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:11:58,707] {logging_mixin.py:95} INFO - [2018-11-08 15:11:58,705] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:06:58.705514+00:00

[2018-11-08 15:11:58,713] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.466 seconds
[2018-11-08 15:11:59,783] {jobs.py:385} INFO - Started process (PID=55918) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:04,788] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:12:04,790] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:12:04,792] {logging_mixin.py:95} INFO - [2018-11-08 15:12:04,791] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:12:05,287] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:05,320] {logging_mixin.py:95} INFO - [2018-11-08 15:12:05,320] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:12:05,324] {logging_mixin.py:95} INFO - [2018-11-08 15:12:05,321] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:07:05.321596+00:00

[2018-11-08 15:12:05,331] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.549 seconds
[2018-11-08 15:12:06,406] {jobs.py:385} INFO - Started process (PID=55920) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:11,412] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:12:11,418] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:12:11,420] {logging_mixin.py:95} INFO - [2018-11-08 15:12:11,420] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:12:12,659] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:12,703] {logging_mixin.py:95} INFO - [2018-11-08 15:12:12,703] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:12:12,706] {logging_mixin.py:95} INFO - [2018-11-08 15:12:12,704] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:07:12.704160+00:00

[2018-11-08 15:12:12,713] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.307 seconds
[2018-11-08 15:12:13,862] {jobs.py:385} INFO - Started process (PID=55926) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:18,874] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:12:18,879] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:12:18,880] {logging_mixin.py:95} INFO - [2018-11-08 15:12:18,880] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:12:19,441] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:19,493] {logging_mixin.py:95} INFO - [2018-11-08 15:12:19,492] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:12:19,495] {logging_mixin.py:95} INFO - [2018-11-08 15:12:19,493] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:07:19.493828+00:00

[2018-11-08 15:12:19,502] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.641 seconds
[2018-11-08 15:12:20,614] {jobs.py:385} INFO - Started process (PID=55927) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:25,620] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:12:25,625] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:12:25,627] {logging_mixin.py:95} INFO - [2018-11-08 15:12:25,626] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:12:26,191] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:26,220] {logging_mixin.py:95} INFO - [2018-11-08 15:12:26,220] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:12:26,223] {logging_mixin.py:95} INFO - [2018-11-08 15:12:26,221] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:07:26.221598+00:00

[2018-11-08 15:12:26,230] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.616 seconds
[2018-11-08 15:12:27,358] {jobs.py:385} INFO - Started process (PID=55929) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:32,366] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:12:32,378] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:12:32,381] {logging_mixin.py:95} INFO - [2018-11-08 15:12:32,379] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:12:32,800] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:32,830] {logging_mixin.py:95} INFO - [2018-11-08 15:12:32,829] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:12:32,832] {logging_mixin.py:95} INFO - [2018-11-08 15:12:32,830] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:07:32.830808+00:00

[2018-11-08 15:12:32,841] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.483 seconds
[2018-11-08 15:12:33,909] {jobs.py:385} INFO - Started process (PID=55930) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:38,919] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:12:38,923] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:12:38,927] {logging_mixin.py:95} INFO - [2018-11-08 15:12:38,926] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:12:39,926] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:39,959] {logging_mixin.py:95} INFO - [2018-11-08 15:12:39,959] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:12:39,964] {logging_mixin.py:95} INFO - [2018-11-08 15:12:39,960] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:07:39.960681+00:00

[2018-11-08 15:12:39,984] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.075 seconds
[2018-11-08 15:12:41,060] {jobs.py:385} INFO - Started process (PID=55932) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:46,070] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:12:46,072] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:12:46,075] {logging_mixin.py:95} INFO - [2018-11-08 15:12:46,074] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:12:46,655] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:46,706] {logging_mixin.py:95} INFO - [2018-11-08 15:12:46,705] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:12:46,711] {logging_mixin.py:95} INFO - [2018-11-08 15:12:46,706] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:07:46.706945+00:00

[2018-11-08 15:12:46,722] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.662 seconds
[2018-11-08 15:12:47,800] {jobs.py:385} INFO - Started process (PID=55934) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:52,812] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:12:52,815] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:12:52,817] {logging_mixin.py:95} INFO - [2018-11-08 15:12:52,816] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:12:53,316] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:53,341] {logging_mixin.py:95} INFO - [2018-11-08 15:12:53,341] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:12:53,343] {logging_mixin.py:95} INFO - [2018-11-08 15:12:53,342] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:07:53.342048+00:00

[2018-11-08 15:12:53,349] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.549 seconds
[2018-11-08 15:12:54,414] {jobs.py:385} INFO - Started process (PID=55936) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:59,422] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:12:59,427] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:12:59,429] {logging_mixin.py:95} INFO - [2018-11-08 15:12:59,428] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:12:59,775] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:12:59,800] {logging_mixin.py:95} INFO - [2018-11-08 15:12:59,800] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:12:59,802] {logging_mixin.py:95} INFO - [2018-11-08 15:12:59,800] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:07:59.800839+00:00

[2018-11-08 15:12:59,808] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.394 seconds
[2018-11-08 15:13:00,970] {jobs.py:385} INFO - Started process (PID=55937) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:05,986] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:13:05,990] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:13:05,992] {logging_mixin.py:95} INFO - [2018-11-08 15:13:05,991] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:13:06,422] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:06,451] {logging_mixin.py:95} INFO - [2018-11-08 15:13:06,451] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:13:06,453] {logging_mixin.py:95} INFO - [2018-11-08 15:13:06,452] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:08:06.452023+00:00

[2018-11-08 15:13:06,460] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.490 seconds
[2018-11-08 15:13:07,565] {jobs.py:385} INFO - Started process (PID=55941) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:12,581] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:13:12,584] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:13:12,591] {logging_mixin.py:95} INFO - [2018-11-08 15:13:12,588] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:13:13,044] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:13,078] {logging_mixin.py:95} INFO - [2018-11-08 15:13:13,078] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:13:13,087] {logging_mixin.py:95} INFO - [2018-11-08 15:13:13,079] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:08:13.079413+00:00

[2018-11-08 15:13:13,094] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.530 seconds
[2018-11-08 15:13:14,146] {jobs.py:385} INFO - Started process (PID=55945) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:19,157] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:13:19,159] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:13:19,160] {logging_mixin.py:95} INFO - [2018-11-08 15:13:19,160] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:13:19,708] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:19,743] {logging_mixin.py:95} INFO - [2018-11-08 15:13:19,742] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:13:19,746] {logging_mixin.py:95} INFO - [2018-11-08 15:13:19,744] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:08:19.743979+00:00

[2018-11-08 15:13:19,751] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.605 seconds
[2018-11-08 15:13:20,863] {jobs.py:385} INFO - Started process (PID=55947) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:25,877] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:13:25,879] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:13:25,880] {logging_mixin.py:95} INFO - [2018-11-08 15:13:25,880] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:13:26,355] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:26,400] {logging_mixin.py:95} INFO - [2018-11-08 15:13:26,399] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:13:26,406] {logging_mixin.py:95} INFO - [2018-11-08 15:13:26,400] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:08:26.400563+00:00

[2018-11-08 15:13:26,420] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.557 seconds
[2018-11-08 15:13:27,545] {jobs.py:385} INFO - Started process (PID=55951) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:32,553] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:13:32,557] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:13:32,558] {logging_mixin.py:95} INFO - [2018-11-08 15:13:32,558] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:13:33,422] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:33,488] {logging_mixin.py:95} INFO - [2018-11-08 15:13:33,487] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:13:33,491] {logging_mixin.py:95} INFO - [2018-11-08 15:13:33,488] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:08:33.488911+00:00

[2018-11-08 15:13:33,500] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.956 seconds
[2018-11-08 15:13:34,567] {jobs.py:385} INFO - Started process (PID=55952) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:39,571] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:13:39,574] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:13:39,576] {logging_mixin.py:95} INFO - [2018-11-08 15:13:39,575] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:13:39,915] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:39,941] {logging_mixin.py:95} INFO - [2018-11-08 15:13:39,940] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:13:39,943] {logging_mixin.py:95} INFO - [2018-11-08 15:13:39,941] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:08:39.941572+00:00

[2018-11-08 15:13:39,949] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.381 seconds
[2018-11-08 15:13:41,037] {jobs.py:385} INFO - Started process (PID=55954) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:46,042] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:13:46,047] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:13:46,050] {logging_mixin.py:95} INFO - [2018-11-08 15:13:46,049] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:13:46,742] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:46,773] {logging_mixin.py:95} INFO - [2018-11-08 15:13:46,772] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:13:46,775] {logging_mixin.py:95} INFO - [2018-11-08 15:13:46,773] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:08:46.773680+00:00

[2018-11-08 15:13:46,784] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.746 seconds
[2018-11-08 15:13:47,849] {jobs.py:385} INFO - Started process (PID=55955) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:52,856] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:13:52,860] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:13:52,866] {logging_mixin.py:95} INFO - [2018-11-08 15:13:52,864] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:13:53,596] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:53,642] {logging_mixin.py:95} INFO - [2018-11-08 15:13:53,641] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:13:53,649] {logging_mixin.py:95} INFO - [2018-11-08 15:13:53,643] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:08:53.643423+00:00

[2018-11-08 15:13:53,661] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.812 seconds
[2018-11-08 15:13:54,759] {jobs.py:385} INFO - Started process (PID=55958) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:13:59,769] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:13:59,776] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:13:59,800] {logging_mixin.py:95} INFO - [2018-11-08 15:13:59,796] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:14:00,571] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:00,601] {logging_mixin.py:95} INFO - [2018-11-08 15:14:00,601] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:14:00,604] {logging_mixin.py:95} INFO - [2018-11-08 15:14:00,602] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:09:00.602044+00:00

[2018-11-08 15:14:00,615] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.856 seconds
[2018-11-08 15:14:01,743] {jobs.py:385} INFO - Started process (PID=55960) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:06,748] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:14:06,752] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:14:06,755] {logging_mixin.py:95} INFO - [2018-11-08 15:14:06,754] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:14:08,080] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:08,138] {logging_mixin.py:95} INFO - [2018-11-08 15:14:08,137] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:14:08,146] {logging_mixin.py:95} INFO - [2018-11-08 15:14:08,139] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:09:08.139324+00:00

[2018-11-08 15:14:08,162] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.419 seconds
[2018-11-08 15:14:09,330] {jobs.py:385} INFO - Started process (PID=55964) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:14,341] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:14:14,349] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:14:14,353] {logging_mixin.py:95} INFO - [2018-11-08 15:14:14,352] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:14:15,881] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:15,991] {logging_mixin.py:95} INFO - [2018-11-08 15:14:15,989] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:14:16,029] {logging_mixin.py:95} INFO - [2018-11-08 15:14:15,992] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:09:15.992130+00:00

[2018-11-08 15:14:16,048] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.718 seconds
[2018-11-08 15:14:17,163] {jobs.py:385} INFO - Started process (PID=55966) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:22,173] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:14:22,175] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:14:22,176] {logging_mixin.py:95} INFO - [2018-11-08 15:14:22,175] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:14:22,870] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:22,955] {logging_mixin.py:95} INFO - [2018-11-08 15:14:22,950] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:14:22,961] {logging_mixin.py:95} INFO - [2018-11-08 15:14:22,956] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:09:22.956885+00:00

[2018-11-08 15:14:22,974] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.811 seconds
[2018-11-08 15:14:24,116] {jobs.py:385} INFO - Started process (PID=55967) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:29,125] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:14:29,129] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:14:29,130] {logging_mixin.py:95} INFO - [2018-11-08 15:14:29,129] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:14:30,136] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:30,208] {logging_mixin.py:95} INFO - [2018-11-08 15:14:30,207] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:14:30,212] {logging_mixin.py:95} INFO - [2018-11-08 15:14:30,209] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:09:30.209480+00:00

[2018-11-08 15:14:30,224] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.108 seconds
[2018-11-08 15:14:31,389] {jobs.py:385} INFO - Started process (PID=55970) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:36,421] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:14:36,436] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:14:36,439] {logging_mixin.py:95} INFO - [2018-11-08 15:14:36,439] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:14:39,464] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:39,617] {logging_mixin.py:95} INFO - [2018-11-08 15:14:39,616] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:14:39,634] {logging_mixin.py:95} INFO - [2018-11-08 15:14:39,624] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:09:39.624021+00:00

[2018-11-08 15:14:39,663] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 8.274 seconds
[2018-11-08 15:14:40,869] {jobs.py:385} INFO - Started process (PID=55972) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:45,877] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:14:45,878] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:14:45,879] {logging_mixin.py:95} INFO - [2018-11-08 15:14:45,879] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:14:46,433] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:46,481] {logging_mixin.py:95} INFO - [2018-11-08 15:14:46,481] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:14:46,485] {logging_mixin.py:95} INFO - [2018-11-08 15:14:46,482] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:09:46.482203+00:00

[2018-11-08 15:14:46,494] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.625 seconds
[2018-11-08 15:14:47,565] {jobs.py:385} INFO - Started process (PID=55973) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:52,577] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:14:52,582] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:14:52,589] {logging_mixin.py:95} INFO - [2018-11-08 15:14:52,588] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:14:53,437] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:53,465] {logging_mixin.py:95} INFO - [2018-11-08 15:14:53,465] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:14:53,468] {logging_mixin.py:95} INFO - [2018-11-08 15:14:53,466] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:09:53.466184+00:00

[2018-11-08 15:14:53,476] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.911 seconds
[2018-11-08 15:14:54,543] {jobs.py:385} INFO - Started process (PID=55975) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:14:59,558] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:14:59,567] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:14:59,570] {logging_mixin.py:95} INFO - [2018-11-08 15:14:59,568] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:15:00,149] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:00,184] {logging_mixin.py:95} INFO - [2018-11-08 15:15:00,183] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:15:00,190] {logging_mixin.py:95} INFO - [2018-11-08 15:15:00,187] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:10:00.187063+00:00

[2018-11-08 15:15:00,214] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.670 seconds
[2018-11-08 15:15:01,312] {jobs.py:385} INFO - Started process (PID=55977) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:06,320] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:15:06,327] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:15:06,328] {logging_mixin.py:95} INFO - [2018-11-08 15:15:06,328] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:15:07,033] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:07,064] {logging_mixin.py:95} INFO - [2018-11-08 15:15:07,063] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:15:07,066] {logging_mixin.py:95} INFO - [2018-11-08 15:15:07,064] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:10:07.064596+00:00

[2018-11-08 15:15:07,080] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.769 seconds
[2018-11-08 15:15:08,232] {jobs.py:385} INFO - Started process (PID=55979) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:13,242] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:15:13,248] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:15:13,250] {logging_mixin.py:95} INFO - [2018-11-08 15:15:13,249] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:15:13,799] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:13,850] {logging_mixin.py:95} INFO - [2018-11-08 15:15:13,850] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:15:13,855] {logging_mixin.py:95} INFO - [2018-11-08 15:15:13,851] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:10:13.851010+00:00

[2018-11-08 15:15:13,874] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.642 seconds
[2018-11-08 15:15:14,982] {jobs.py:385} INFO - Started process (PID=55984) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:19,998] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:15:20,000] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:15:20,003] {logging_mixin.py:95} INFO - [2018-11-08 15:15:20,002] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:15:20,746] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:20,801] {logging_mixin.py:95} INFO - [2018-11-08 15:15:20,801] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:15:20,805] {logging_mixin.py:95} INFO - [2018-11-08 15:15:20,802] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:10:20.802439+00:00

[2018-11-08 15:15:20,814] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.832 seconds
[2018-11-08 15:15:21,894] {jobs.py:385} INFO - Started process (PID=55985) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:26,902] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:15:26,906] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:15:26,912] {logging_mixin.py:95} INFO - [2018-11-08 15:15:26,911] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:15:27,896] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:27,931] {logging_mixin.py:95} INFO - [2018-11-08 15:15:27,930] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:15:27,934] {logging_mixin.py:95} INFO - [2018-11-08 15:15:27,931] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:10:27.931797+00:00

[2018-11-08 15:15:27,946] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.052 seconds
[2018-11-08 15:15:29,056] {jobs.py:385} INFO - Started process (PID=55987) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:34,063] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:15:34,067] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:15:34,069] {logging_mixin.py:95} INFO - [2018-11-08 15:15:34,068] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:15:34,673] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:34,707] {logging_mixin.py:95} INFO - [2018-11-08 15:15:34,707] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:15:34,710] {logging_mixin.py:95} INFO - [2018-11-08 15:15:34,708] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:10:34.708744+00:00

[2018-11-08 15:15:34,717] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.662 seconds
[2018-11-08 15:15:35,820] {jobs.py:385} INFO - Started process (PID=55989) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:40,834] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:15:40,842] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:15:40,843] {logging_mixin.py:95} INFO - [2018-11-08 15:15:40,843] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:15:41,351] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:41,401] {logging_mixin.py:95} INFO - [2018-11-08 15:15:41,401] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:15:41,410] {logging_mixin.py:95} INFO - [2018-11-08 15:15:41,402] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:10:41.402021+00:00

[2018-11-08 15:15:41,420] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.600 seconds
[2018-11-08 15:15:42,579] {jobs.py:385} INFO - Started process (PID=55991) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:47,585] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:15:47,587] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:15:47,588] {logging_mixin.py:95} INFO - [2018-11-08 15:15:47,588] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:15:48,105] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:48,140] {logging_mixin.py:95} INFO - [2018-11-08 15:15:48,139] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:15:48,143] {logging_mixin.py:95} INFO - [2018-11-08 15:15:48,140] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:10:48.140560+00:00

[2018-11-08 15:15:48,151] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.572 seconds
[2018-11-08 15:15:49,221] {jobs.py:385} INFO - Started process (PID=55992) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:54,231] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:15:54,233] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:15:54,234] {logging_mixin.py:95} INFO - [2018-11-08 15:15:54,234] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:15:54,698] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:15:54,727] {logging_mixin.py:95} INFO - [2018-11-08 15:15:54,726] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:15:54,730] {logging_mixin.py:95} INFO - [2018-11-08 15:15:54,727] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:10:54.727914+00:00

[2018-11-08 15:15:54,736] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.515 seconds
[2018-11-08 15:15:55,862] {jobs.py:385} INFO - Started process (PID=55994) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:00,870] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:16:00,874] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:16:00,876] {logging_mixin.py:95} INFO - [2018-11-08 15:16:00,875] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:16:01,390] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:01,423] {logging_mixin.py:95} INFO - [2018-11-08 15:16:01,421] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:16:01,426] {logging_mixin.py:95} INFO - [2018-11-08 15:16:01,424] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:11:01.424635+00:00

[2018-11-08 15:16:01,433] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.571 seconds
[2018-11-08 15:16:02,504] {jobs.py:385} INFO - Started process (PID=55996) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:07,511] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:16:07,516] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:16:07,517] {logging_mixin.py:95} INFO - [2018-11-08 15:16:07,516] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:16:07,980] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:08,013] {logging_mixin.py:95} INFO - [2018-11-08 15:16:08,012] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:16:08,016] {logging_mixin.py:95} INFO - [2018-11-08 15:16:08,014] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:11:08.014091+00:00

[2018-11-08 15:16:08,023] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.519 seconds
[2018-11-08 15:16:09,113] {jobs.py:385} INFO - Started process (PID=56001) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:14,120] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:16:14,129] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:16:14,131] {logging_mixin.py:95} INFO - [2018-11-08 15:16:14,130] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:16:15,154] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:15,186] {logging_mixin.py:95} INFO - [2018-11-08 15:16:15,185] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:16:15,189] {logging_mixin.py:95} INFO - [2018-11-08 15:16:15,187] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:11:15.187163+00:00

[2018-11-08 15:16:15,201] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.088 seconds
[2018-11-08 15:16:16,306] {jobs.py:385} INFO - Started process (PID=56003) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:21,319] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:16:21,330] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:16:21,332] {logging_mixin.py:95} INFO - [2018-11-08 15:16:21,331] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:16:21,987] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:22,123] {logging_mixin.py:95} INFO - [2018-11-08 15:16:22,122] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:16:22,137] {logging_mixin.py:95} INFO - [2018-11-08 15:16:22,123] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:11:22.123646+00:00

[2018-11-08 15:16:22,152] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.847 seconds
[2018-11-08 15:16:23,240] {jobs.py:385} INFO - Started process (PID=56004) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:28,248] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:16:28,250] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:16:28,251] {logging_mixin.py:95} INFO - [2018-11-08 15:16:28,251] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:16:28,606] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:28,632] {logging_mixin.py:95} INFO - [2018-11-08 15:16:28,632] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:16:28,635] {logging_mixin.py:95} INFO - [2018-11-08 15:16:28,633] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:11:28.633089+00:00

[2018-11-08 15:16:28,640] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.400 seconds
[2018-11-08 15:16:29,763] {jobs.py:385} INFO - Started process (PID=56006) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:34,772] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:16:34,778] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:16:34,783] {logging_mixin.py:95} INFO - [2018-11-08 15:16:34,781] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:16:35,138] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:35,165] {logging_mixin.py:95} INFO - [2018-11-08 15:16:35,164] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:16:35,168] {logging_mixin.py:95} INFO - [2018-11-08 15:16:35,165] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:11:35.165766+00:00

[2018-11-08 15:16:35,173] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.410 seconds
[2018-11-08 15:16:36,282] {jobs.py:385} INFO - Started process (PID=56007) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:41,290] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:16:41,294] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:16:41,298] {logging_mixin.py:95} INFO - [2018-11-08 15:16:41,297] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:16:42,755] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:42,823] {logging_mixin.py:95} INFO - [2018-11-08 15:16:42,821] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:16:42,838] {logging_mixin.py:95} INFO - [2018-11-08 15:16:42,825] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:11:42.825291+00:00

[2018-11-08 15:16:42,857] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.575 seconds
[2018-11-08 15:16:43,917] {jobs.py:385} INFO - Started process (PID=56010) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:48,925] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:16:48,930] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:16:48,932] {logging_mixin.py:95} INFO - [2018-11-08 15:16:48,931] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:16:49,374] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:49,413] {logging_mixin.py:95} INFO - [2018-11-08 15:16:49,412] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:16:49,425] {logging_mixin.py:95} INFO - [2018-11-08 15:16:49,413] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:11:49.413720+00:00

[2018-11-08 15:16:49,437] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.520 seconds
[2018-11-08 15:16:50,531] {jobs.py:385} INFO - Started process (PID=56012) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:55,539] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:16:55,544] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:16:55,546] {logging_mixin.py:95} INFO - [2018-11-08 15:16:55,545] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:16:56,089] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:16:56,122] {logging_mixin.py:95} INFO - [2018-11-08 15:16:56,121] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:16:56,124] {logging_mixin.py:95} INFO - [2018-11-08 15:16:56,123] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:11:56.123190+00:00

[2018-11-08 15:16:56,134] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.603 seconds
[2018-11-08 15:16:57,191] {jobs.py:385} INFO - Started process (PID=56013) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:02,195] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:17:02,199] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:17:02,201] {logging_mixin.py:95} INFO - [2018-11-08 15:17:02,200] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:17:03,003] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:03,052] {logging_mixin.py:95} INFO - [2018-11-08 15:17:03,052] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:17:03,056] {logging_mixin.py:95} INFO - [2018-11-08 15:17:03,053] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:12:03.053740+00:00

[2018-11-08 15:17:03,067] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.876 seconds
[2018-11-08 15:17:04,127] {jobs.py:385} INFO - Started process (PID=56015) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:09,136] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:17:09,141] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:17:09,143] {logging_mixin.py:95} INFO - [2018-11-08 15:17:09,142] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:17:09,702] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:09,732] {logging_mixin.py:95} INFO - [2018-11-08 15:17:09,732] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:17:09,735] {logging_mixin.py:95} INFO - [2018-11-08 15:17:09,733] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:12:09.733317+00:00

[2018-11-08 15:17:09,746] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.619 seconds
[2018-11-08 15:17:10,873] {jobs.py:385} INFO - Started process (PID=56019) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:15,881] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:17:15,882] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:17:15,884] {logging_mixin.py:95} INFO - [2018-11-08 15:17:15,884] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:17:16,437] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:16,481] {logging_mixin.py:95} INFO - [2018-11-08 15:17:16,480] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:17:16,489] {logging_mixin.py:95} INFO - [2018-11-08 15:17:16,481] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:12:16.481880+00:00

[2018-11-08 15:17:16,496] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.623 seconds
[2018-11-08 15:17:17,632] {jobs.py:385} INFO - Started process (PID=56022) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:22,641] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:17:22,644] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:17:22,646] {logging_mixin.py:95} INFO - [2018-11-08 15:17:22,645] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:17:22,987] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:23,013] {logging_mixin.py:95} INFO - [2018-11-08 15:17:23,012] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:17:23,015] {logging_mixin.py:95} INFO - [2018-11-08 15:17:23,013] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:12:23.013474+00:00

[2018-11-08 15:17:23,021] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.389 seconds
[2018-11-08 15:17:24,133] {jobs.py:385} INFO - Started process (PID=56023) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:29,142] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:17:29,144] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:17:29,146] {logging_mixin.py:95} INFO - [2018-11-08 15:17:29,145] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:17:29,539] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:29,572] {logging_mixin.py:95} INFO - [2018-11-08 15:17:29,572] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:17:29,574] {logging_mixin.py:95} INFO - [2018-11-08 15:17:29,573] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:12:29.573065+00:00

[2018-11-08 15:17:29,582] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.449 seconds
[2018-11-08 15:17:30,639] {jobs.py:385} INFO - Started process (PID=56025) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:35,650] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:17:35,656] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:17:35,658] {logging_mixin.py:95} INFO - [2018-11-08 15:17:35,657] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:17:36,687] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:36,939] {logging_mixin.py:95} INFO - [2018-11-08 15:17:36,939] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:17:36,949] {logging_mixin.py:95} INFO - [2018-11-08 15:17:36,945] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:12:36.945477+00:00

[2018-11-08 15:17:36,966] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.327 seconds
[2018-11-08 15:17:38,040] {jobs.py:385} INFO - Started process (PID=56027) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:43,048] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:17:43,052] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:17:43,054] {logging_mixin.py:95} INFO - [2018-11-08 15:17:43,053] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:17:43,722] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:43,898] {logging_mixin.py:95} INFO - [2018-11-08 15:17:43,897] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:17:43,917] {logging_mixin.py:95} INFO - [2018-11-08 15:17:43,900] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:12:43.900808+00:00

[2018-11-08 15:17:43,929] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.889 seconds
[2018-11-08 15:17:44,990] {jobs.py:385} INFO - Started process (PID=56028) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:49,998] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:17:50,001] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:17:50,003] {logging_mixin.py:95} INFO - [2018-11-08 15:17:50,002] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:17:50,519] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:50,553] {logging_mixin.py:95} INFO - [2018-11-08 15:17:50,553] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:17:50,556] {logging_mixin.py:95} INFO - [2018-11-08 15:17:50,554] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:12:50.554454+00:00

[2018-11-08 15:17:50,569] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.580 seconds
[2018-11-08 15:17:51,636] {jobs.py:385} INFO - Started process (PID=56031) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:56,645] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:17:56,648] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:17:56,650] {logging_mixin.py:95} INFO - [2018-11-08 15:17:56,649] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:17:57,012] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:17:57,037] {logging_mixin.py:95} INFO - [2018-11-08 15:17:57,036] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:17:57,039] {logging_mixin.py:95} INFO - [2018-11-08 15:17:57,037] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:12:57.037551+00:00

[2018-11-08 15:17:57,044] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.409 seconds
[2018-11-08 15:17:58,105] {jobs.py:385} INFO - Started process (PID=56032) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:03,114] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:18:03,116] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:18:03,118] {logging_mixin.py:95} INFO - [2018-11-08 15:18:03,118] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:18:03,480] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:03,504] {logging_mixin.py:95} INFO - [2018-11-08 15:18:03,504] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:18:03,506] {logging_mixin.py:95} INFO - [2018-11-08 15:18:03,505] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:13:03.505171+00:00

[2018-11-08 15:18:03,513] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.408 seconds
[2018-11-08 15:18:04,573] {jobs.py:385} INFO - Started process (PID=56050) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:09,582] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:18:09,587] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:18:09,589] {logging_mixin.py:95} INFO - [2018-11-08 15:18:09,588] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:18:09,923] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:09,949] {logging_mixin.py:95} INFO - [2018-11-08 15:18:09,948] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:18:09,951] {logging_mixin.py:95} INFO - [2018-11-08 15:18:09,949] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:13:09.949614+00:00

[2018-11-08 15:18:09,957] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.384 seconds
[2018-11-08 15:18:11,064] {jobs.py:385} INFO - Started process (PID=56068) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:16,071] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:18:16,075] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:18:16,079] {logging_mixin.py:95} INFO - [2018-11-08 15:18:16,077] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:18:16,406] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:16,442] {logging_mixin.py:95} INFO - [2018-11-08 15:18:16,441] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:18:16,445] {logging_mixin.py:95} INFO - [2018-11-08 15:18:16,443] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:13:16.442961+00:00

[2018-11-08 15:18:16,453] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.388 seconds
[2018-11-08 15:18:17,548] {jobs.py:385} INFO - Started process (PID=56092) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:22,553] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:18:22,555] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:18:22,556] {logging_mixin.py:95} INFO - [2018-11-08 15:18:22,555] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:18:22,933] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:22,957] {logging_mixin.py:95} INFO - [2018-11-08 15:18:22,957] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:18:22,959] {logging_mixin.py:95} INFO - [2018-11-08 15:18:22,957] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:13:22.957964+00:00

[2018-11-08 15:18:22,965] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.416 seconds
[2018-11-08 15:18:24,025] {jobs.py:385} INFO - Started process (PID=56105) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:29,030] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:18:29,033] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:18:29,035] {logging_mixin.py:95} INFO - [2018-11-08 15:18:29,034] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:18:29,438] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:29,465] {logging_mixin.py:95} INFO - [2018-11-08 15:18:29,464] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:18:29,467] {logging_mixin.py:95} INFO - [2018-11-08 15:18:29,465] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:13:29.465495+00:00

[2018-11-08 15:18:29,472] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.448 seconds
[2018-11-08 15:18:30,531] {jobs.py:385} INFO - Started process (PID=56123) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:35,539] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:18:35,546] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:18:35,548] {logging_mixin.py:95} INFO - [2018-11-08 15:18:35,547] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:18:35,878] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:35,902] {logging_mixin.py:95} INFO - [2018-11-08 15:18:35,902] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:18:35,905] {logging_mixin.py:95} INFO - [2018-11-08 15:18:35,903] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:13:35.903531+00:00

[2018-11-08 15:18:35,911] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.380 seconds
[2018-11-08 15:18:37,020] {jobs.py:385} INFO - Started process (PID=56139) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:42,029] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:18:42,033] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:18:42,035] {logging_mixin.py:95} INFO - [2018-11-08 15:18:42,035] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:18:42,427] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:42,455] {logging_mixin.py:95} INFO - [2018-11-08 15:18:42,454] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:18:42,457] {logging_mixin.py:95} INFO - [2018-11-08 15:18:42,455] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:13:42.455452+00:00

[2018-11-08 15:18:42,462] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.442 seconds
[2018-11-08 15:18:43,592] {jobs.py:385} INFO - Started process (PID=56145) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:48,597] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:18:48,600] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:18:48,602] {logging_mixin.py:95} INFO - [2018-11-08 15:18:48,601] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:18:49,048] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:49,080] {logging_mixin.py:95} INFO - [2018-11-08 15:18:49,080] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:18:49,083] {logging_mixin.py:95} INFO - [2018-11-08 15:18:49,081] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:13:49.081153+00:00

[2018-11-08 15:18:49,090] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.498 seconds
[2018-11-08 15:18:50,156] {jobs.py:385} INFO - Started process (PID=56151) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:55,165] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:18:55,169] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:18:55,170] {logging_mixin.py:95} INFO - [2018-11-08 15:18:55,170] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:18:55,620] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:18:55,657] {logging_mixin.py:95} INFO - [2018-11-08 15:18:55,657] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:18:55,659] {logging_mixin.py:95} INFO - [2018-11-08 15:18:55,657] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:13:55.657938+00:00

[2018-11-08 15:18:55,669] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.513 seconds
[2018-11-08 15:18:56,750] {jobs.py:385} INFO - Started process (PID=56173) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:01,754] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:19:01,756] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:19:01,757] {logging_mixin.py:95} INFO - [2018-11-08 15:19:01,757] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:19:02,122] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:02,161] {logging_mixin.py:95} INFO - [2018-11-08 15:19:02,161] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:19:02,169] {logging_mixin.py:95} INFO - [2018-11-08 15:19:02,162] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:14:02.162824+00:00

[2018-11-08 15:19:02,184] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.434 seconds
[2018-11-08 15:19:03,232] {jobs.py:385} INFO - Started process (PID=56182) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:08,236] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:19:08,242] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:19:08,244] {logging_mixin.py:95} INFO - [2018-11-08 15:19:08,243] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:19:08,568] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:08,590] {logging_mixin.py:95} INFO - [2018-11-08 15:19:08,590] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:19:08,592] {logging_mixin.py:95} INFO - [2018-11-08 15:19:08,591] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:14:08.591140+00:00

[2018-11-08 15:19:08,599] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.367 seconds
[2018-11-08 15:19:09,699] {jobs.py:385} INFO - Started process (PID=56186) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:14,706] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:19:14,709] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:19:14,711] {logging_mixin.py:95} INFO - [2018-11-08 15:19:14,711] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:19:15,037] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:15,063] {logging_mixin.py:95} INFO - [2018-11-08 15:19:15,062] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:19:15,065] {logging_mixin.py:95} INFO - [2018-11-08 15:19:15,063] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:14:15.063564+00:00

[2018-11-08 15:19:15,072] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.373 seconds
[2018-11-08 15:19:16,162] {jobs.py:385} INFO - Started process (PID=56188) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:21,169] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:19:21,173] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:19:21,177] {logging_mixin.py:95} INFO - [2018-11-08 15:19:21,175] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:19:21,690] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:21,729] {logging_mixin.py:95} INFO - [2018-11-08 15:19:21,728] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:19:21,731] {logging_mixin.py:95} INFO - [2018-11-08 15:19:21,729] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:14:21.729889+00:00

[2018-11-08 15:19:21,742] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.580 seconds
[2018-11-08 15:19:22,872] {jobs.py:385} INFO - Started process (PID=56189) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:27,878] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:19:27,881] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:19:27,883] {logging_mixin.py:95} INFO - [2018-11-08 15:19:27,882] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:19:28,221] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:28,252] {logging_mixin.py:95} INFO - [2018-11-08 15:19:28,251] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:19:28,258] {logging_mixin.py:95} INFO - [2018-11-08 15:19:28,253] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:14:28.253202+00:00

[2018-11-08 15:19:28,268] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.396 seconds
[2018-11-08 15:19:29,334] {jobs.py:385} INFO - Started process (PID=56192) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:34,345] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:19:34,350] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:19:34,352] {logging_mixin.py:95} INFO - [2018-11-08 15:19:34,352] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:19:34,665] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:34,691] {logging_mixin.py:95} INFO - [2018-11-08 15:19:34,691] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:19:34,693] {logging_mixin.py:95} INFO - [2018-11-08 15:19:34,692] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:14:34.692347+00:00

[2018-11-08 15:19:34,699] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.365 seconds
[2018-11-08 15:19:35,814] {jobs.py:385} INFO - Started process (PID=56193) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:40,819] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:19:40,822] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:19:40,824] {logging_mixin.py:95} INFO - [2018-11-08 15:19:40,823] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:19:41,141] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:41,169] {logging_mixin.py:95} INFO - [2018-11-08 15:19:41,168] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:19:41,172] {logging_mixin.py:95} INFO - [2018-11-08 15:19:41,170] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:14:41.169976+00:00

[2018-11-08 15:19:41,370] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.556 seconds
[2018-11-08 15:19:42,466] {jobs.py:385} INFO - Started process (PID=56195) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:47,475] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:19:47,479] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:19:47,481] {logging_mixin.py:95} INFO - [2018-11-08 15:19:47,480] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:19:47,845] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:47,871] {logging_mixin.py:95} INFO - [2018-11-08 15:19:47,870] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:19:47,873] {logging_mixin.py:95} INFO - [2018-11-08 15:19:47,871] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:14:47.871856+00:00

[2018-11-08 15:19:47,926] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.460 seconds
[2018-11-08 15:19:49,045] {jobs.py:385} INFO - Started process (PID=56196) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:54,050] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:19:54,053] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:19:54,055] {logging_mixin.py:95} INFO - [2018-11-08 15:19:54,055] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:19:54,380] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:19:54,452] {logging_mixin.py:95} INFO - [2018-11-08 15:19:54,452] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:19:54,453] {logging_mixin.py:95} INFO - [2018-11-08 15:19:54,452] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:14:54.452790+00:00

[2018-11-08 15:19:54,458] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.413 seconds
[2018-11-08 15:19:55,516] {jobs.py:385} INFO - Started process (PID=56198) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:00,521] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:20:00,525] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:20:00,526] {logging_mixin.py:95} INFO - [2018-11-08 15:20:00,526] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:20:01,075] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:01,175] {logging_mixin.py:95} INFO - [2018-11-08 15:20:01,175] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:20:01,177] {logging_mixin.py:95} INFO - [2018-11-08 15:20:01,176] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:15:01.176245+00:00

[2018-11-08 15:20:01,184] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.668 seconds
[2018-11-08 15:20:02,270] {jobs.py:385} INFO - Started process (PID=56201) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:07,279] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:20:07,284] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:20:07,286] {logging_mixin.py:95} INFO - [2018-11-08 15:20:07,285] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:20:07,613] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:07,684] {logging_mixin.py:95} INFO - [2018-11-08 15:20:07,683] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:20:07,685] {logging_mixin.py:95} INFO - [2018-11-08 15:20:07,684] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:15:07.684482+00:00

[2018-11-08 15:20:07,690] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.421 seconds
[2018-11-08 15:20:08,819] {jobs.py:385} INFO - Started process (PID=56202) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:13,826] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:20:13,831] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:20:13,834] {logging_mixin.py:95} INFO - [2018-11-08 15:20:13,833] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:20:14,182] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:14,253] {logging_mixin.py:95} INFO - [2018-11-08 15:20:14,253] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:20:14,254] {logging_mixin.py:95} INFO - [2018-11-08 15:20:14,253] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:15:14.253839+00:00

[2018-11-08 15:20:14,260] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.441 seconds
[2018-11-08 15:20:15,391] {jobs.py:385} INFO - Started process (PID=56207) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:20,395] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:20:20,398] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:20:20,400] {logging_mixin.py:95} INFO - [2018-11-08 15:20:20,399] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:20:20,806] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:20,890] {logging_mixin.py:95} INFO - [2018-11-08 15:20:20,890] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:20:20,892] {logging_mixin.py:95} INFO - [2018-11-08 15:20:20,891] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:15:20.891165+00:00

[2018-11-08 15:20:20,898] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.507 seconds
[2018-11-08 15:20:21,974] {jobs.py:385} INFO - Started process (PID=56208) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:26,982] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:20:26,985] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:20:26,987] {logging_mixin.py:95} INFO - [2018-11-08 15:20:26,987] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:20:27,415] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:27,488] {logging_mixin.py:95} INFO - [2018-11-08 15:20:27,488] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:20:27,491] {logging_mixin.py:95} INFO - [2018-11-08 15:20:27,489] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:15:27.489823+00:00

[2018-11-08 15:20:27,497] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.523 seconds
[2018-11-08 15:20:28,662] {jobs.py:385} INFO - Started process (PID=56210) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:33,668] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:20:33,674] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:20:33,676] {logging_mixin.py:95} INFO - [2018-11-08 15:20:33,675] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:20:34,114] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:34,192] {logging_mixin.py:95} INFO - [2018-11-08 15:20:34,191] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:20:34,193] {logging_mixin.py:95} INFO - [2018-11-08 15:20:34,192] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:15:34.192536+00:00

[2018-11-08 15:20:34,198] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.536 seconds
[2018-11-08 15:20:35,299] {jobs.py:385} INFO - Started process (PID=56212) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:40,304] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:20:40,307] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:20:40,309] {logging_mixin.py:95} INFO - [2018-11-08 15:20:40,308] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:20:40,689] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:40,709] {logging_mixin.py:95} INFO - [2018-11-08 15:20:40,709] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:20:40,711] {logging_mixin.py:95} INFO - [2018-11-08 15:20:40,710] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:15:40.710242+00:00

[2018-11-08 15:20:40,716] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.417 seconds
[2018-11-08 15:20:41,861] {jobs.py:385} INFO - Started process (PID=56214) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:46,867] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:20:46,870] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:20:46,872] {logging_mixin.py:95} INFO - [2018-11-08 15:20:46,871] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:20:47,238] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:47,258] {logging_mixin.py:95} INFO - [2018-11-08 15:20:47,257] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:20:47,260] {logging_mixin.py:95} INFO - [2018-11-08 15:20:47,258] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:15:47.258674+00:00

[2018-11-08 15:20:47,264] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.404 seconds
[2018-11-08 15:20:48,362] {jobs.py:385} INFO - Started process (PID=56215) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:53,370] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:20:53,372] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:20:53,375] {logging_mixin.py:95} INFO - [2018-11-08 15:20:53,374] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:20:53,810] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:53,833] {logging_mixin.py:95} INFO - [2018-11-08 15:20:53,833] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:20:53,836] {logging_mixin.py:95} INFO - [2018-11-08 15:20:53,834] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:15:53.834675+00:00

[2018-11-08 15:20:53,842] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.479 seconds
[2018-11-08 15:20:54,960] {jobs.py:385} INFO - Started process (PID=56217) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:20:59,967] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:20:59,974] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:20:59,975] {logging_mixin.py:95} INFO - [2018-11-08 15:20:59,975] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:21:00,464] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:00,484] {logging_mixin.py:95} INFO - [2018-11-08 15:21:00,484] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:21:00,485] {logging_mixin.py:95} INFO - [2018-11-08 15:21:00,484] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:16:00.484774+00:00

[2018-11-08 15:21:00,490] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.530 seconds
[2018-11-08 15:21:01,558] {jobs.py:385} INFO - Started process (PID=56218) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:06,563] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:21:06,566] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:21:06,568] {logging_mixin.py:95} INFO - [2018-11-08 15:21:06,567] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:21:07,105] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:07,127] {logging_mixin.py:95} INFO - [2018-11-08 15:21:07,127] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:21:07,129] {logging_mixin.py:95} INFO - [2018-11-08 15:21:07,128] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:16:07.128153+00:00

[2018-11-08 15:21:07,134] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.575 seconds
[2018-11-08 15:21:08,182] {jobs.py:385} INFO - Started process (PID=56224) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:13,191] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:21:13,195] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:21:13,197] {logging_mixin.py:95} INFO - [2018-11-08 15:21:13,196] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:21:13,658] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:13,680] {logging_mixin.py:95} INFO - [2018-11-08 15:21:13,679] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:21:13,681] {logging_mixin.py:95} INFO - [2018-11-08 15:21:13,680] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:16:13.680619+00:00

[2018-11-08 15:21:13,686] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.504 seconds
[2018-11-08 15:21:14,828] {jobs.py:385} INFO - Started process (PID=56239) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:19,837] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:21:19,842] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:21:19,844] {logging_mixin.py:95} INFO - [2018-11-08 15:21:19,844] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:21:20,426] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:20,452] {logging_mixin.py:95} INFO - [2018-11-08 15:21:20,452] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:21:20,454] {logging_mixin.py:95} INFO - [2018-11-08 15:21:20,452] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:16:20.452879+00:00

[2018-11-08 15:21:20,460] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.632 seconds
[2018-11-08 15:21:21,594] {jobs.py:385} INFO - Started process (PID=56244) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:26,601] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:21:26,605] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:21:26,609] {logging_mixin.py:95} INFO - [2018-11-08 15:21:26,608] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:21:27,066] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:27,092] {logging_mixin.py:95} INFO - [2018-11-08 15:21:27,091] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:21:27,093] {logging_mixin.py:95} INFO - [2018-11-08 15:21:27,092] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:16:27.092622+00:00

[2018-11-08 15:21:27,099] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.506 seconds
[2018-11-08 15:21:28,176] {jobs.py:385} INFO - Started process (PID=56246) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:33,184] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:21:33,190] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:21:33,193] {logging_mixin.py:95} INFO - [2018-11-08 15:21:33,192] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:21:33,693] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:33,728] {logging_mixin.py:95} INFO - [2018-11-08 15:21:33,728] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:21:33,731] {logging_mixin.py:95} INFO - [2018-11-08 15:21:33,729] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:16:33.729198+00:00

[2018-11-08 15:21:33,739] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.563 seconds
[2018-11-08 15:21:34,851] {jobs.py:385} INFO - Started process (PID=56248) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:39,855] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:21:39,857] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:21:39,858] {logging_mixin.py:95} INFO - [2018-11-08 15:21:39,857] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:21:40,324] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:40,346] {logging_mixin.py:95} INFO - [2018-11-08 15:21:40,346] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:21:40,348] {logging_mixin.py:95} INFO - [2018-11-08 15:21:40,347] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:16:40.347378+00:00

[2018-11-08 15:21:40,355] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.504 seconds
[2018-11-08 15:21:41,432] {jobs.py:385} INFO - Started process (PID=56251) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:46,441] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:21:46,443] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:21:46,445] {logging_mixin.py:95} INFO - [2018-11-08 15:21:46,445] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:21:46,815] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:46,835] {logging_mixin.py:95} INFO - [2018-11-08 15:21:46,834] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:21:46,836] {logging_mixin.py:95} INFO - [2018-11-08 15:21:46,835] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:16:46.835555+00:00

[2018-11-08 15:21:46,841] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.409 seconds
[2018-11-08 15:21:47,887] {jobs.py:385} INFO - Started process (PID=56252) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:52,896] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:21:52,899] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:21:52,901] {logging_mixin.py:95} INFO - [2018-11-08 15:21:52,900] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:21:53,270] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:53,294] {logging_mixin.py:95} INFO - [2018-11-08 15:21:53,294] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:21:53,295] {logging_mixin.py:95} INFO - [2018-11-08 15:21:53,294] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:16:53.294851+00:00

[2018-11-08 15:21:53,300] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.413 seconds
[2018-11-08 15:21:54,351] {jobs.py:385} INFO - Started process (PID=56254) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:59,357] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:21:59,362] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:21:59,364] {logging_mixin.py:95} INFO - [2018-11-08 15:21:59,363] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:21:59,726] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:21:59,746] {logging_mixin.py:95} INFO - [2018-11-08 15:21:59,745] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:21:59,747] {logging_mixin.py:95} INFO - [2018-11-08 15:21:59,746] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:16:59.746572+00:00

[2018-11-08 15:21:59,752] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.401 seconds
[2018-11-08 15:22:00,843] {jobs.py:385} INFO - Started process (PID=56255) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:05,851] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:22:05,854] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:22:05,857] {logging_mixin.py:95} INFO - [2018-11-08 15:22:05,855] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:22:06,220] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:06,240] {logging_mixin.py:95} INFO - [2018-11-08 15:22:06,239] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:22:06,241] {logging_mixin.py:95} INFO - [2018-11-08 15:22:06,240] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:17:06.240772+00:00

[2018-11-08 15:22:06,247] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.404 seconds
[2018-11-08 15:22:07,338] {jobs.py:385} INFO - Started process (PID=56257) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:12,344] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:22:12,348] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:22:12,349] {logging_mixin.py:95} INFO - [2018-11-08 15:22:12,348] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:22:12,887] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:12,912] {logging_mixin.py:95} INFO - [2018-11-08 15:22:12,911] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:22:12,913] {logging_mixin.py:95} INFO - [2018-11-08 15:22:12,912] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:17:12.912572+00:00

[2018-11-08 15:22:12,919] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.581 seconds
[2018-11-08 15:22:13,997] {jobs.py:385} INFO - Started process (PID=56263) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:19,004] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:22:19,008] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:22:19,011] {logging_mixin.py:95} INFO - [2018-11-08 15:22:19,010] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:22:19,457] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:19,484] {logging_mixin.py:95} INFO - [2018-11-08 15:22:19,483] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:22:19,486] {logging_mixin.py:95} INFO - [2018-11-08 15:22:19,485] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:17:19.485054+00:00

[2018-11-08 15:22:19,491] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.493 seconds
[2018-11-08 15:22:20,548] {jobs.py:385} INFO - Started process (PID=56264) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:25,556] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:22:25,559] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:22:25,563] {logging_mixin.py:95} INFO - [2018-11-08 15:22:25,562] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:22:26,055] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:26,081] {logging_mixin.py:95} INFO - [2018-11-08 15:22:26,081] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:22:26,083] {logging_mixin.py:95} INFO - [2018-11-08 15:22:26,082] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:17:26.082313+00:00

[2018-11-08 15:22:26,088] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.540 seconds
[2018-11-08 15:22:27,207] {jobs.py:385} INFO - Started process (PID=56266) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:32,219] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:22:32,225] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:22:32,228] {logging_mixin.py:95} INFO - [2018-11-08 15:22:32,228] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:22:32,814] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:32,839] {logging_mixin.py:95} INFO - [2018-11-08 15:22:32,838] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:22:32,840] {logging_mixin.py:95} INFO - [2018-11-08 15:22:32,839] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:17:32.839703+00:00

[2018-11-08 15:22:32,851] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.644 seconds
[2018-11-08 15:22:33,972] {jobs.py:385} INFO - Started process (PID=56267) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:38,982] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:22:38,985] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:22:38,988] {logging_mixin.py:95} INFO - [2018-11-08 15:22:38,987] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:22:39,467] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:39,497] {logging_mixin.py:95} INFO - [2018-11-08 15:22:39,497] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:22:39,498] {logging_mixin.py:95} INFO - [2018-11-08 15:22:39,497] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:17:39.497935+00:00

[2018-11-08 15:22:39,504] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.532 seconds
[2018-11-08 15:22:40,622] {jobs.py:385} INFO - Started process (PID=56269) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:45,631] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:22:45,633] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:22:45,634] {logging_mixin.py:95} INFO - [2018-11-08 15:22:45,634] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:22:46,110] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:46,130] {logging_mixin.py:95} INFO - [2018-11-08 15:22:46,130] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:22:46,132] {logging_mixin.py:95} INFO - [2018-11-08 15:22:46,131] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:17:46.131390+00:00

[2018-11-08 15:22:46,137] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.515 seconds
[2018-11-08 15:22:47,185] {jobs.py:385} INFO - Started process (PID=56271) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:52,194] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:22:52,196] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:22:52,197] {logging_mixin.py:95} INFO - [2018-11-08 15:22:52,197] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:22:52,642] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:52,667] {logging_mixin.py:95} INFO - [2018-11-08 15:22:52,667] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:22:52,668] {logging_mixin.py:95} INFO - [2018-11-08 15:22:52,667] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:17:52.667835+00:00

[2018-11-08 15:22:52,674] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.489 seconds
[2018-11-08 15:22:53,753] {jobs.py:385} INFO - Started process (PID=56273) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:58,766] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:22:58,771] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:22:58,774] {logging_mixin.py:95} INFO - [2018-11-08 15:22:58,773] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:22:59,159] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:22:59,179] {logging_mixin.py:95} INFO - [2018-11-08 15:22:59,179] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:22:59,181] {logging_mixin.py:95} INFO - [2018-11-08 15:22:59,180] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:17:59.180299+00:00

[2018-11-08 15:22:59,188] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.435 seconds
[2018-11-08 15:23:00,303] {jobs.py:385} INFO - Started process (PID=56274) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:05,309] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:23:05,312] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:23:05,314] {logging_mixin.py:95} INFO - [2018-11-08 15:23:05,313] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:23:05,697] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:05,717] {logging_mixin.py:95} INFO - [2018-11-08 15:23:05,716] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:23:05,718] {logging_mixin.py:95} INFO - [2018-11-08 15:23:05,717] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:18:05.717645+00:00

[2018-11-08 15:23:05,723] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.420 seconds
[2018-11-08 15:23:06,774] {jobs.py:385} INFO - Started process (PID=56276) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:11,782] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:23:11,786] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:23:11,788] {logging_mixin.py:95} INFO - [2018-11-08 15:23:11,787] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:23:12,266] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:12,287] {logging_mixin.py:95} INFO - [2018-11-08 15:23:12,286] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:23:12,290] {logging_mixin.py:95} INFO - [2018-11-08 15:23:12,288] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:18:12.288712+00:00

[2018-11-08 15:23:12,295] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.521 seconds
[2018-11-08 15:23:13,352] {jobs.py:385} INFO - Started process (PID=56280) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:18,360] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:23:18,363] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:23:18,365] {logging_mixin.py:95} INFO - [2018-11-08 15:23:18,364] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:23:19,651] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:19,735] {logging_mixin.py:95} INFO - [2018-11-08 15:23:19,735] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:23:19,738] {logging_mixin.py:95} INFO - [2018-11-08 15:23:19,736] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:18:19.736616+00:00

[2018-11-08 15:23:19,753] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.401 seconds
[2018-11-08 15:23:20,849] {jobs.py:385} INFO - Started process (PID=56283) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:25,855] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:23:25,859] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:23:25,861] {logging_mixin.py:95} INFO - [2018-11-08 15:23:25,860] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:23:26,257] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:26,285] {logging_mixin.py:95} INFO - [2018-11-08 15:23:26,284] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:23:26,286] {logging_mixin.py:95} INFO - [2018-11-08 15:23:26,285] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:18:26.285601+00:00

[2018-11-08 15:23:26,292] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.443 seconds
[2018-11-08 15:23:27,356] {jobs.py:385} INFO - Started process (PID=56285) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:32,362] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:23:32,367] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:23:32,369] {logging_mixin.py:95} INFO - [2018-11-08 15:23:32,369] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:23:33,155] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:33,213] {logging_mixin.py:95} INFO - [2018-11-08 15:23:33,212] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:23:33,216] {logging_mixin.py:95} INFO - [2018-11-08 15:23:33,213] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:18:33.213851+00:00

[2018-11-08 15:23:33,225] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.869 seconds
[2018-11-08 15:23:34,331] {jobs.py:385} INFO - Started process (PID=56286) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:39,337] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:23:39,342] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:23:39,344] {logging_mixin.py:95} INFO - [2018-11-08 15:23:39,343] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:23:39,862] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:39,889] {logging_mixin.py:95} INFO - [2018-11-08 15:23:39,889] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:23:39,892] {logging_mixin.py:95} INFO - [2018-11-08 15:23:39,890] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:18:39.890659+00:00

[2018-11-08 15:23:39,898] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.568 seconds
[2018-11-08 15:23:40,979] {jobs.py:385} INFO - Started process (PID=56288) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:45,985] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:23:45,989] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:23:45,993] {logging_mixin.py:95} INFO - [2018-11-08 15:23:45,992] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:23:46,391] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:46,416] {logging_mixin.py:95} INFO - [2018-11-08 15:23:46,416] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:23:46,419] {logging_mixin.py:95} INFO - [2018-11-08 15:23:46,417] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:18:46.417585+00:00

[2018-11-08 15:23:46,426] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.446 seconds
[2018-11-08 15:23:47,539] {jobs.py:385} INFO - Started process (PID=56289) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:52,546] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:23:52,549] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:23:52,553] {logging_mixin.py:95} INFO - [2018-11-08 15:23:52,551] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:23:53,906] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:23:53,966] {logging_mixin.py:95} INFO - [2018-11-08 15:23:53,965] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:23:53,971] {logging_mixin.py:95} INFO - [2018-11-08 15:23:53,969] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:18:53.969124+00:00

[2018-11-08 15:23:53,981] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.441 seconds
[2018-11-08 15:23:55,072] {jobs.py:385} INFO - Started process (PID=56292) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:00,083] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:24:00,089] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:24:00,092] {logging_mixin.py:95} INFO - [2018-11-08 15:24:00,092] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:24:00,768] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:00,796] {logging_mixin.py:95} INFO - [2018-11-08 15:24:00,795] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:24:00,797] {logging_mixin.py:95} INFO - [2018-11-08 15:24:00,796] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:19:00.796458+00:00

[2018-11-08 15:24:00,804] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.732 seconds
[2018-11-08 15:24:01,914] {jobs.py:385} INFO - Started process (PID=56293) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:06,919] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:24:06,921] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:24:06,923] {logging_mixin.py:95} INFO - [2018-11-08 15:24:06,922] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:24:07,302] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:07,324] {logging_mixin.py:95} INFO - [2018-11-08 15:24:07,323] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:24:07,325] {logging_mixin.py:95} INFO - [2018-11-08 15:24:07,324] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:19:07.324859+00:00

[2018-11-08 15:24:07,330] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.417 seconds
[2018-11-08 15:24:08,475] {jobs.py:385} INFO - Started process (PID=56295) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:13,480] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:24:13,481] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:24:13,482] {logging_mixin.py:95} INFO - [2018-11-08 15:24:13,482] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:24:13,917] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:13,940] {logging_mixin.py:95} INFO - [2018-11-08 15:24:13,938] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:24:13,942] {logging_mixin.py:95} INFO - [2018-11-08 15:24:13,941] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:19:13.941354+00:00

[2018-11-08 15:24:13,948] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.474 seconds
[2018-11-08 15:24:15,046] {jobs.py:385} INFO - Started process (PID=56300) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:20,054] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:24:20,057] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:24:20,060] {logging_mixin.py:95} INFO - [2018-11-08 15:24:20,059] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:24:20,778] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:20,798] {logging_mixin.py:95} INFO - [2018-11-08 15:24:20,797] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:24:20,800] {logging_mixin.py:95} INFO - [2018-11-08 15:24:20,798] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:19:20.798574+00:00

[2018-11-08 15:24:20,805] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.759 seconds
[2018-11-08 15:24:21,916] {jobs.py:385} INFO - Started process (PID=56301) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:26,922] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:24:26,924] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:24:26,925] {logging_mixin.py:95} INFO - [2018-11-08 15:24:26,925] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:24:27,562] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:27,614] {logging_mixin.py:95} INFO - [2018-11-08 15:24:27,613] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:24:27,615] {logging_mixin.py:95} INFO - [2018-11-08 15:24:27,614] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:19:27.614648+00:00

[2018-11-08 15:24:27,621] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.705 seconds
[2018-11-08 15:24:28,686] {jobs.py:385} INFO - Started process (PID=56304) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:33,697] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:24:33,700] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:24:33,701] {logging_mixin.py:95} INFO - [2018-11-08 15:24:33,701] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:24:34,111] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:34,131] {logging_mixin.py:95} INFO - [2018-11-08 15:24:34,131] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:24:34,132] {logging_mixin.py:95} INFO - [2018-11-08 15:24:34,131] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:19:34.131965+00:00

[2018-11-08 15:24:34,138] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.451 seconds
[2018-11-08 15:24:35,266] {jobs.py:385} INFO - Started process (PID=56306) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:40,273] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:24:40,275] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:24:40,278] {logging_mixin.py:95} INFO - [2018-11-08 15:24:40,277] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:24:40,651] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:40,675] {logging_mixin.py:95} INFO - [2018-11-08 15:24:40,675] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:24:40,677] {logging_mixin.py:95} INFO - [2018-11-08 15:24:40,676] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:19:40.676199+00:00

[2018-11-08 15:24:40,682] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.417 seconds
[2018-11-08 15:24:41,805] {jobs.py:385} INFO - Started process (PID=56308) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:46,813] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:24:46,816] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:24:46,818] {logging_mixin.py:95} INFO - [2018-11-08 15:24:46,817] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:24:47,199] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:47,219] {logging_mixin.py:95} INFO - [2018-11-08 15:24:47,219] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:24:47,221] {logging_mixin.py:95} INFO - [2018-11-08 15:24:47,220] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:19:47.220157+00:00

[2018-11-08 15:24:47,226] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.421 seconds
[2018-11-08 15:24:48,269] {jobs.py:385} INFO - Started process (PID=56309) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:53,278] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:24:53,283] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:24:53,285] {logging_mixin.py:95} INFO - [2018-11-08 15:24:53,284] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:24:53,759] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:53,790] {logging_mixin.py:95} INFO - [2018-11-08 15:24:53,789] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:24:53,792] {logging_mixin.py:95} INFO - [2018-11-08 15:24:53,791] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:19:53.791413+00:00

[2018-11-08 15:24:53,801] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.532 seconds
[2018-11-08 15:24:54,939] {jobs.py:385} INFO - Started process (PID=56311) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:24:59,945] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:24:59,951] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:24:59,953] {logging_mixin.py:95} INFO - [2018-11-08 15:24:59,953] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:25:00,383] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:00,405] {logging_mixin.py:95} INFO - [2018-11-08 15:25:00,405] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:25:00,408] {logging_mixin.py:95} INFO - [2018-11-08 15:25:00,406] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:20:00.406513+00:00

[2018-11-08 15:25:00,413] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.473 seconds
[2018-11-08 15:25:01,515] {jobs.py:385} INFO - Started process (PID=56312) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:06,525] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:25:06,528] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:25:06,529] {logging_mixin.py:95} INFO - [2018-11-08 15:25:06,529] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:25:06,942] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:06,965] {logging_mixin.py:95} INFO - [2018-11-08 15:25:06,964] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:25:06,966] {logging_mixin.py:95} INFO - [2018-11-08 15:25:06,965] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:20:06.965508+00:00

[2018-11-08 15:25:06,971] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.456 seconds
[2018-11-08 15:25:08,083] {jobs.py:385} INFO - Started process (PID=56316) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:13,091] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:25:13,094] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:25:13,096] {logging_mixin.py:95} INFO - [2018-11-08 15:25:13,096] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:25:13,478] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:13,500] {logging_mixin.py:95} INFO - [2018-11-08 15:25:13,499] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:25:13,503] {logging_mixin.py:95} INFO - [2018-11-08 15:25:13,501] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:20:13.501339+00:00

[2018-11-08 15:25:13,511] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.428 seconds
[2018-11-08 15:25:14,654] {jobs.py:385} INFO - Started process (PID=56321) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:19,659] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:25:19,664] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:25:19,665] {logging_mixin.py:95} INFO - [2018-11-08 15:25:19,665] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:25:20,453] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:20,484] {logging_mixin.py:95} INFO - [2018-11-08 15:25:20,482] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:25:20,486] {logging_mixin.py:95} INFO - [2018-11-08 15:25:20,485] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:20:20.485394+00:00

[2018-11-08 15:25:20,492] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.838 seconds
[2018-11-08 15:25:21,623] {jobs.py:385} INFO - Started process (PID=56322) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:26,633] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:25:26,637] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:25:26,639] {logging_mixin.py:95} INFO - [2018-11-08 15:25:26,638] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:25:27,175] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:27,202] {logging_mixin.py:95} INFO - [2018-11-08 15:25:27,201] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:25:27,204] {logging_mixin.py:95} INFO - [2018-11-08 15:25:27,202] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:20:27.202827+00:00

[2018-11-08 15:25:27,209] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.585 seconds
[2018-11-08 15:25:28,284] {jobs.py:385} INFO - Started process (PID=56324) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:33,295] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:25:33,301] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:25:33,302] {logging_mixin.py:95} INFO - [2018-11-08 15:25:33,302] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:25:33,725] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:33,748] {logging_mixin.py:95} INFO - [2018-11-08 15:25:33,748] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:25:33,750] {logging_mixin.py:95} INFO - [2018-11-08 15:25:33,749] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:20:33.749301+00:00

[2018-11-08 15:25:33,757] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.473 seconds
[2018-11-08 15:25:34,854] {jobs.py:385} INFO - Started process (PID=56325) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:39,861] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:25:39,863] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:25:39,864] {logging_mixin.py:95} INFO - [2018-11-08 15:25:39,863] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:25:40,300] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:40,321] {logging_mixin.py:95} INFO - [2018-11-08 15:25:40,320] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:25:40,323] {logging_mixin.py:95} INFO - [2018-11-08 15:25:40,322] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:20:40.322102+00:00

[2018-11-08 15:25:40,330] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.476 seconds
[2018-11-08 15:25:41,435] {jobs.py:385} INFO - Started process (PID=56328) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:46,439] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:25:46,442] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:25:46,443] {logging_mixin.py:95} INFO - [2018-11-08 15:25:46,443] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:25:46,922] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:46,954] {logging_mixin.py:95} INFO - [2018-11-08 15:25:46,953] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:25:46,955] {logging_mixin.py:95} INFO - [2018-11-08 15:25:46,954] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:20:46.954801+00:00

[2018-11-08 15:25:46,963] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.528 seconds
[2018-11-08 15:25:48,098] {jobs.py:385} INFO - Started process (PID=56329) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:53,105] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:25:53,107] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:25:53,108] {logging_mixin.py:95} INFO - [2018-11-08 15:25:53,108] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:25:53,613] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:53,636] {logging_mixin.py:95} INFO - [2018-11-08 15:25:53,635] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:25:53,637] {logging_mixin.py:95} INFO - [2018-11-08 15:25:53,636] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:20:53.636700+00:00

[2018-11-08 15:25:53,642] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.544 seconds
[2018-11-08 15:25:54,790] {jobs.py:385} INFO - Started process (PID=56331) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:25:59,796] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:25:59,801] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:25:59,803] {logging_mixin.py:95} INFO - [2018-11-08 15:25:59,802] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:26:00,180] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:00,201] {logging_mixin.py:95} INFO - [2018-11-08 15:26:00,201] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:26:00,202] {logging_mixin.py:95} INFO - [2018-11-08 15:26:00,202] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:21:00.201990+00:00

[2018-11-08 15:26:00,208] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.418 seconds
[2018-11-08 15:26:01,330] {jobs.py:385} INFO - Started process (PID=56332) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:06,348] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:26:06,351] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:26:06,353] {logging_mixin.py:95} INFO - [2018-11-08 15:26:06,352] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:26:06,724] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:06,747] {logging_mixin.py:95} INFO - [2018-11-08 15:26:06,747] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:26:06,749] {logging_mixin.py:95} INFO - [2018-11-08 15:26:06,748] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:21:06.748304+00:00

[2018-11-08 15:26:06,754] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.424 seconds
[2018-11-08 15:26:07,813] {jobs.py:385} INFO - Started process (PID=56334) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:12,823] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:26:12,827] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:26:12,829] {logging_mixin.py:95} INFO - [2018-11-08 15:26:12,828] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:26:13,254] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:13,275] {logging_mixin.py:95} INFO - [2018-11-08 15:26:13,274] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:26:13,276] {logging_mixin.py:95} INFO - [2018-11-08 15:26:13,275] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:21:13.275567+00:00

[2018-11-08 15:26:13,281] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.468 seconds
[2018-11-08 15:26:14,390] {jobs.py:385} INFO - Started process (PID=56340) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:19,399] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:26:19,402] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:26:19,404] {logging_mixin.py:95} INFO - [2018-11-08 15:26:19,403] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:26:19,781] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:19,801] {logging_mixin.py:95} INFO - [2018-11-08 15:26:19,801] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:26:19,803] {logging_mixin.py:95} INFO - [2018-11-08 15:26:19,802] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:21:19.802278+00:00

[2018-11-08 15:26:19,809] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.419 seconds
[2018-11-08 15:26:20,856] {jobs.py:385} INFO - Started process (PID=56341) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:25,866] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:26:25,872] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:26:25,874] {logging_mixin.py:95} INFO - [2018-11-08 15:26:25,873] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:26:26,243] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:26,268] {logging_mixin.py:95} INFO - [2018-11-08 15:26:26,267] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:26:26,269] {logging_mixin.py:95} INFO - [2018-11-08 15:26:26,268] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:21:26.268712+00:00

[2018-11-08 15:26:26,274] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.418 seconds
[2018-11-08 15:26:27,347] {jobs.py:385} INFO - Started process (PID=56343) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:32,353] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:26:32,358] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:26:32,359] {logging_mixin.py:95} INFO - [2018-11-08 15:26:32,358] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:26:32,967] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:32,989] {logging_mixin.py:95} INFO - [2018-11-08 15:26:32,988] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:26:32,990] {logging_mixin.py:95} INFO - [2018-11-08 15:26:32,989] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:21:32.989799+00:00

[2018-11-08 15:26:32,996] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.649 seconds
[2018-11-08 15:26:34,145] {jobs.py:385} INFO - Started process (PID=56344) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:39,155] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:26:39,158] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:26:39,161] {logging_mixin.py:95} INFO - [2018-11-08 15:26:39,160] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:26:39,553] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:39,575] {logging_mixin.py:95} INFO - [2018-11-08 15:26:39,574] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:26:39,576] {logging_mixin.py:95} INFO - [2018-11-08 15:26:39,575] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:21:39.575659+00:00

[2018-11-08 15:26:39,581] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.437 seconds
[2018-11-08 15:26:40,662] {jobs.py:385} INFO - Started process (PID=56347) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:45,671] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:26:45,675] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:26:45,678] {logging_mixin.py:95} INFO - [2018-11-08 15:26:45,677] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:26:46,043] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:46,062] {logging_mixin.py:95} INFO - [2018-11-08 15:26:46,062] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:26:46,064] {logging_mixin.py:95} INFO - [2018-11-08 15:26:46,063] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:21:46.063332+00:00

[2018-11-08 15:26:46,069] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.407 seconds
[2018-11-08 15:26:47,116] {jobs.py:385} INFO - Started process (PID=56348) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:52,124] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:26:52,136] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:26:52,138] {logging_mixin.py:95} INFO - [2018-11-08 15:26:52,137] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:26:52,604] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:52,633] {logging_mixin.py:95} INFO - [2018-11-08 15:26:52,633] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:26:52,635] {logging_mixin.py:95} INFO - [2018-11-08 15:26:52,634] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:21:52.634410+00:00

[2018-11-08 15:26:52,641] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.525 seconds
[2018-11-08 15:26:53,740] {jobs.py:385} INFO - Started process (PID=56350) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:58,750] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:26:58,755] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:26:58,757] {logging_mixin.py:95} INFO - [2018-11-08 15:26:58,757] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:26:59,134] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:26:59,153] {logging_mixin.py:95} INFO - [2018-11-08 15:26:59,153] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:26:59,155] {logging_mixin.py:95} INFO - [2018-11-08 15:26:59,154] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:21:59.154331+00:00

[2018-11-08 15:26:59,161] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.421 seconds
[2018-11-08 15:27:00,292] {jobs.py:385} INFO - Started process (PID=56351) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:05,305] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:27:05,307] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:27:05,309] {logging_mixin.py:95} INFO - [2018-11-08 15:27:05,308] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:27:05,826] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:05,866] {logging_mixin.py:95} INFO - [2018-11-08 15:27:05,866] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:27:05,869] {logging_mixin.py:95} INFO - [2018-11-08 15:27:05,867] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:22:05.867521+00:00

[2018-11-08 15:27:05,875] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.582 seconds
[2018-11-08 15:27:07,025] {jobs.py:385} INFO - Started process (PID=56360) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:12,037] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:27:12,039] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:27:12,040] {logging_mixin.py:95} INFO - [2018-11-08 15:27:12,039] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:27:12,555] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:12,580] {logging_mixin.py:95} INFO - [2018-11-08 15:27:12,580] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:27:12,582] {logging_mixin.py:95} INFO - [2018-11-08 15:27:12,581] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:22:12.581671+00:00

[2018-11-08 15:27:12,589] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.564 seconds
[2018-11-08 15:27:13,693] {jobs.py:385} INFO - Started process (PID=56366) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:18,705] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:27:18,707] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:27:18,708] {logging_mixin.py:95} INFO - [2018-11-08 15:27:18,708] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:27:19,176] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:19,200] {logging_mixin.py:95} INFO - [2018-11-08 15:27:19,200] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:27:19,201] {logging_mixin.py:95} INFO - [2018-11-08 15:27:19,200] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:22:19.200896+00:00

[2018-11-08 15:27:19,209] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.515 seconds
[2018-11-08 15:27:20,260] {jobs.py:385} INFO - Started process (PID=56368) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:25,266] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:27:25,270] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:27:25,274] {logging_mixin.py:95} INFO - [2018-11-08 15:27:25,273] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:27:25,681] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:25,701] {logging_mixin.py:95} INFO - [2018-11-08 15:27:25,701] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:27:25,702] {logging_mixin.py:95} INFO - [2018-11-08 15:27:25,701] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:22:25.701865+00:00

[2018-11-08 15:27:25,708] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.447 seconds
[2018-11-08 15:27:26,823] {jobs.py:385} INFO - Started process (PID=56370) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:31,829] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:27:31,831] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:27:31,832] {logging_mixin.py:95} INFO - [2018-11-08 15:27:31,832] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:27:32,724] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:32,747] {logging_mixin.py:95} INFO - [2018-11-08 15:27:32,746] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:27:32,749] {logging_mixin.py:95} INFO - [2018-11-08 15:27:32,748] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:22:32.748219+00:00

[2018-11-08 15:27:32,754] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.931 seconds
[2018-11-08 15:27:33,824] {jobs.py:385} INFO - Started process (PID=56371) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:38,841] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:27:38,848] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:27:38,852] {logging_mixin.py:95} INFO - [2018-11-08 15:27:38,851] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:27:39,361] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:39,381] {logging_mixin.py:95} INFO - [2018-11-08 15:27:39,381] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:27:39,383] {logging_mixin.py:95} INFO - [2018-11-08 15:27:39,382] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:22:39.382081+00:00

[2018-11-08 15:27:39,389] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.566 seconds
[2018-11-08 15:27:40,478] {jobs.py:385} INFO - Started process (PID=56373) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:45,489] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:27:45,495] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:27:45,498] {logging_mixin.py:95} INFO - [2018-11-08 15:27:45,497] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:27:45,868] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:45,887] {logging_mixin.py:95} INFO - [2018-11-08 15:27:45,886] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:27:45,889] {logging_mixin.py:95} INFO - [2018-11-08 15:27:45,887] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:22:45.887901+00:00

[2018-11-08 15:27:45,894] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.416 seconds
[2018-11-08 15:27:46,964] {jobs.py:385} INFO - Started process (PID=56374) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:51,973] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:27:51,975] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:27:51,980] {logging_mixin.py:95} INFO - [2018-11-08 15:27:51,977] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:27:52,864] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:52,984] {logging_mixin.py:95} INFO - [2018-11-08 15:27:52,983] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:27:52,986] {logging_mixin.py:95} INFO - [2018-11-08 15:27:52,985] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:22:52.985004+00:00

[2018-11-08 15:27:52,995] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 6.031 seconds
[2018-11-08 15:27:54,141] {jobs.py:385} INFO - Started process (PID=56377) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:59,153] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:27:59,160] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:27:59,162] {logging_mixin.py:95} INFO - [2018-11-08 15:27:59,161] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:27:59,565] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:27:59,589] {logging_mixin.py:95} INFO - [2018-11-08 15:27:59,589] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:27:59,590] {logging_mixin.py:95} INFO - [2018-11-08 15:27:59,589] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:22:59.589936+00:00

[2018-11-08 15:27:59,595] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.455 seconds
[2018-11-08 15:28:00,722] {jobs.py:385} INFO - Started process (PID=56378) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:28:05,730] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:28:05,733] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:28:05,735] {logging_mixin.py:95} INFO - [2018-11-08 15:28:05,734] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:28:06,188] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:28:06,210] {logging_mixin.py:95} INFO - [2018-11-08 15:28:06,210] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:28:06,212] {logging_mixin.py:95} INFO - [2018-11-08 15:28:06,211] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:23:06.211152+00:00

[2018-11-08 15:28:06,221] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.499 seconds
[2018-11-08 15:28:07,272] {jobs.py:385} INFO - Started process (PID=56380) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:28:12,281] {jobs.py:580} ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-11-08 15:28:12,289] {jobs.py:1782} INFO - Processing file /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py for tasks to queue
[2018-11-08 15:28:12,294] {logging_mixin.py:95} INFO - [2018-11-08 15:28:12,293] {models.py:258} INFO - Filling up the DagBag from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py

[2018-11-08 15:28:12,763] {jobs.py:1794} INFO - DAG(s) dict_keys(['extract_dag']) retrieved from /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:28:12,788] {logging_mixin.py:95} INFO - [2018-11-08 15:28:12,787] {models.py:432} INFO - Finding 'running' jobs without a recent heartbeat

[2018-11-08 15:28:12,790] {logging_mixin.py:95} INFO - [2018-11-08 15:28:12,789] {models.py:436} INFO - Failing jobs without heartbeat after 2018-11-08 17:23:12.789000+00:00

[2018-11-08 15:28:12,795] {jobs.py:392} INFO - Processing /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py took 5.523 seconds
[2018-11-08 15:28:13,953] {jobs.py:385} INFO - Started process (PID=56385) to work on /Users/brow1998/projects/noverde-github/etl/dags/extract_loan_data.py
[2018-11-08 15:28:18,964] {jobs.py:396} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 569, in __init__
    super(SchedulerJob, self).__init__(*args, **kwargs)
  File "<string>", line 6, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 107, in __init__
    self.hostname = get_hostname()
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/utils/net.py", line 45, in get_hostname
    return callable()
  File "/anaconda3/envs/etlv2/lib/python3.6/socket.py", line 673, in getfqdn
    hostname, aliases, ipaddrs = gethostbyaddr(name)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/jobs.py", line 386, in helper
    scheduler_job = SchedulerJob(dag_ids=dag_id_white_list, log=log)
  File "<string>", line 4, in __init__
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/sqlalchemy/orm/state.py", line 411, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/anaconda3/envs/etlv2/lib/python3.6/site-packages/airflow/bin/cli.py", line 85, in sigint_handler
    sys.exit(0)
SystemExit: 0
